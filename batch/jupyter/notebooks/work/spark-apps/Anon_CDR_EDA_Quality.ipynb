{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "500282d3-1943-4eb3-b635-1783cf6827d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/29 04:12:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: CDR Data Quality Assessment - New Year's Analysis, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä CDR DATA QUALITY ASSESSMENT - NEW YEAR'S EVE SPECIAL\n",
      "================================================================================\n",
      "Analysis Period: Dec 31, 2024 - Jan 1, 2025\n",
      "Total Records: 89,911\n",
      "Analysis Started: 2025-06-29 04:12:13.410479\n",
      "================================================================================\n",
      "\n",
      "üîç SCHEMA VALIDATION AND DATA TYPES\n",
      "--------------------------------------------------\n",
      "\n",
      "üìã Data Type Distribution:\n",
      "\n",
      "StringType(): 32 columns\n",
      "  - CDR_ID\n",
      "  - CDR_SUB_ID\n",
      "  - CDR_TYPE\n",
      "  - CDR_BATCH_ID\n",
      "  - SRC_CDR_ID\n",
      "  ... and 27 more\n",
      "\n",
      "DoubleType(): 6 columns\n",
      "  - ACTUAL_USAGE\n",
      "  - RATE_USAGE\n",
      "  - DEBIT_AMOUNT\n",
      "  - UN_DEBIT_AMOUNT\n",
      "  - TOTAL_TAX\n",
      "  ... and 1 more\n",
      "\n",
      "DateType(): 1 columns\n",
      "  - CDR_DAY\n",
      "\n",
      "‚úÖ Schema Validation Results:\n",
      "  - Total Columns: 39\n",
      "  - Numeric Columns: 6\n",
      "  - String Columns: 32\n",
      "  - Date Columns: 1\n",
      "  - Hash Columns: 6\n",
      "\n",
      "üìä DATA COMPLETENESS ANALYSIS\n",
      "--------------------------------------------------\n",
      "\n",
      "üî¥ Columns with Missing Data (>0% nulls):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------+-----------------+----------+-----------------+\n",
      "|column               |completeness     |null_count|null_percentage  |\n",
      "+---------------------+-----------------+----------+-----------------+\n",
      "|IMEI_HASH            |0.0              |89911     |100.0            |\n",
      "|CallingRoamInfo      |0.0              |89911     |100.0            |\n",
      "|CalledRoamInfo       |0.0              |89911     |100.0            |\n",
      "|CalledCellID         |0.0              |89911     |100.0            |\n",
      "|CallingPartyIMSI_HASH|0.424864588315117|89529     |99.57513541168488|\n",
      "|CalledPartyIMSI_HASH |0.489372824237293|89471     |99.5106271757627 |\n",
      "|MSCAddress           |20.5102823903638 |71470     |79.4897176096362 |\n",
      "|CallingCellID        |20.5102823903638 |71470     |79.4897176096362 |\n",
      "+---------------------+-----------------+----------+-----------------+\n",
      "\n",
      "\n",
      "‚úÖ Columns with Complete Data (100% complete):\n",
      "   31 out of 39 columns have no missing values\n",
      "\n",
      "üìà Overall Data Completeness Score: 80.56%\n",
      "\n",
      "üîç Critical Fields Completeness:\n",
      "   CDR_ID: 100.00%\n",
      "   START_DATE: 100.00%\n",
      "   END_DATE: 100.00%\n",
      "   PRI_IDENTITY_HASH: 100.00%\n",
      "   ACTUAL_USAGE: 100.00%\n",
      "   DEBIT_AMOUNT: 100.00%\n",
      "   CDR_DAY: 100.00%\n",
      "\n",
      "üéØ DATA VALIDITY AND RANGE ANALYSIS\n",
      "--------------------------------------------------\n",
      "\n",
      "üìä Numeric Columns Statistics:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ACTUAL_USAGE:\n",
      "  Range: [0.00, 3604.00]\n",
      "  Mean: 161.13, Median: 52.00\n",
      "  Negative values: 0\n",
      "  Zero values: 116\n",
      "  Outliers (IQR method): 12,995 (14.45%)\n",
      "\n",
      "RATE_USAGE:\n",
      "  Range: [0.00, 3660.00]\n",
      "  Mean: 182.35, Median: 60.00\n",
      "  Negative values: 0\n",
      "  Zero values: 116\n",
      "  Outliers (IQR method): 15,975 (17.77%)\n",
      "\n",
      "DEBIT_AMOUNT:\n",
      "  Range: [0.00, 95899.00]\n",
      "  Mean: 512.75, Median: 0.00\n",
      "  Negative values: 0\n",
      "  Zero values: 54,749\n",
      "  Outliers (IQR method): 6,269 (6.97%)\n",
      "\n",
      "UN_DEBIT_AMOUNT:\n",
      "  Range: [0.00, 0.00]\n",
      "  Mean: 0.00, Median: 0.00\n",
      "  Negative values: 0\n",
      "  Zero values: 89,911\n",
      "  Outliers (IQR method): 0 (0.00%)\n",
      "\n",
      "TOTAL_TAX:\n",
      "  Range: [0.00, 0.00]\n",
      "  Mean: 0.00, Median: 0.00\n",
      "  Negative values: 0\n",
      "  Zero values: 89,911\n",
      "  Outliers (IQR method): 0 (0.00%)\n",
      "\n",
      "ChargingTime:\n",
      "  Range: [20241231211909.00, 20250101133522.00]\n",
      "  Mean: 20249392676983.87, Median: 20250101095203.00\n",
      "  Negative values: 0\n",
      "  Zero values: 0\n",
      "  Outliers (IQR method): 11,297 (12.56%)\n",
      "\n",
      "üìÖ Date Range Validation:\n",
      "  Date Range: 2024-12-31 to 2025-01-01\n",
      "  Unique Days: 2\n",
      "\n",
      "‚è±Ô∏è Call Duration Consistency Check:\n",
      "  Records with invalid duration: 0 (0.00%)\n",
      "\n",
      "üîç UNIQUENESS AND DUPLICATE ANALYSIS\n",
      "--------------------------------------------------\n",
      "\n",
      "üìã CDR_ID Uniqueness:\n",
      "  Total Records: 89,911\n",
      "  Unique CDR_IDs: 3,540\n",
      "  Duplicate CDR_IDs: 86,371 (96.06%)\n",
      "\n",
      "  Most Duplicated CDR_IDs:\n",
      "+------------------+-----+\n",
      "|            CDR_ID|count|\n",
      "+------------------+-----+\n",
      "|182100000353521984|  124|\n",
      "|184600000351493184|  120|\n",
      "|184600000351779584|  108|\n",
      "|182100000353257216|  108|\n",
      "|184600000351642816|  108|\n",
      "|180000000351244416|  106|\n",
      "|184600000351810816|  105|\n",
      "|184600000351418816|  105|\n",
      "|184600000351569984|  104|\n",
      "|182100000353601984|  104|\n",
      "+------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "üìã Complete Duplicate Rows Check:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 04:12:28 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ‚úÖ No complete duplicate rows found\n",
      "\n",
      "üîê Hash Columns Uniqueness Analysis:\n",
      "  PRI_IDENTITY_HASH: 40,843 unique values\n",
      "  CallingPartyNumber_HASH: 40,908 unique values\n",
      "  CalledPartyNumber_HASH: 50,708 unique values\n",
      "  CallingPartyIMSI_HASH: 229 unique values\n",
      "  CalledPartyIMSI_HASH: 298 unique values\n",
      "  IMEI_HASH: 1 unique values\n",
      "\n",
      "üìè BUSINESS RULES AND CONSISTENCY CHECKS\n",
      "--------------------------------------------------\n",
      "\n",
      "1Ô∏è‚É£ Usage Consistency (ACTUAL_USAGE <= RATE_USAGE):\n",
      "   Violations: 0 (0.00%)\n",
      "\n",
      "2Ô∏è‚É£ Revenue Consistency (Zero usage = Zero charge):\n",
      "   Violations: 0 (0.00%)\n",
      "\n",
      "3Ô∏è‚É£ Call Success Patterns:\n",
      "+------------------------------------------------------------+-----+\n",
      "|CASE WHEN (ACTUAL_USAGE > 0) THEN Successful ELSE Failed END|count|\n",
      "+------------------------------------------------------------+-----+\n",
      "|                                                  Successful|89795|\n",
      "|                                                      Failed|  116|\n",
      "+------------------------------------------------------------+-----+\n",
      "\n",
      "\n",
      "4Ô∏è‚É£ Service Category Distribution:\n",
      "+----------------+-----+------------------+-----------------+\n",
      "|SERVICE_CATEGORY|count|      avg_duration|      avg_revenue|\n",
      "+----------------+-----+------------------+-----------------+\n",
      "|               1|89911|161.13353204835894|512.7482176819299|\n",
      "+----------------+-----+------------------+-----------------+\n",
      "\n",
      "\n",
      "5Ô∏è‚É£ Call Type Distribution:\n",
      "+--------+-----+\n",
      "|CallType|count|\n",
      "+--------+-----+\n",
      "|       1|47743|\n",
      "|       0|40030|\n",
      "|       3| 2138|\n",
      "+--------+-----+\n",
      "\n",
      "\n",
      "‚è∞ DATA TIMELINESS AND FRESHNESS ANALYSIS\n",
      "--------------------------------------------------\n",
      "\n",
      "üìä Processing Delay Statistics:\n",
      "  Minimum Delay: 41 seconds\n",
      "  Maximum Delay: 7,069 seconds\n",
      "  Average Delay: 2,311 seconds\n",
      "  Median Delay: 2,599 seconds\n",
      "\n",
      "üìÖ Hourly Data Distribution (New Year's Transition):\n",
      "+----------+---------+-----+\n",
      "|   CDR_DAY|call_hour|count|\n",
      "+----------+---------+-----+\n",
      "|2024-12-31|       21|   30|\n",
      "|2024-12-31|       22| 1880|\n",
      "|2024-12-31|       23| 5271|\n",
      "|2025-01-01|        0| 2032|\n",
      "|2025-01-01|        1|  881|\n",
      "|2025-01-01|        2|  643|\n",
      "|2025-01-01|        3|  560|\n",
      "|2025-01-01|        4|  622|\n",
      "|2025-01-01|        5| 1061|\n",
      "|2025-01-01|        6| 2031|\n",
      "|2025-01-01|        7| 5199|\n",
      "|2025-01-01|        8|10827|\n",
      "|2025-01-01|        9|16393|\n",
      "|2025-01-01|       10|18125|\n",
      "|2025-01-01|       11|14625|\n",
      "|2025-01-01|       12| 8912|\n",
      "|2025-01-01|       13|  819|\n",
      "+----------+---------+-----+\n",
      "\n",
      "\n",
      "üéÜ NEW YEAR'S EVE SPECIAL QUALITY ANALYSIS\n",
      "--------------------------------------------------\n",
      "\n",
      "üìä Quality Metrics Comparison:\n",
      "+----------+-------------+------------+------------+------------------+-------------+------------+\n",
      "|   CDR_DAY|total_records|unique_users|failed_calls|      avg_duration|total_revenue|active_cells|\n",
      "+----------+-------------+------------+------------+------------------+-------------+------------+\n",
      "|2024-12-31|         7181|        3924|          13|105.00612728032307|    3734442.0|           1|\n",
      "|2025-01-01|        82730|       38311|         103| 166.0054152060921|  4.2367263E7|           1|\n",
      "+----------+-------------+------------+------------+------------------+-------------+------------+\n",
      "\n",
      "\n",
      "üïê Midnight Analysis (23:00 - 01:00):\n",
      "+----------+----+-----+------------+-----------------+------------+\n",
      "|   CDR_DAY|hour|calls|unique_users|     avg_duration|failed_calls|\n",
      "+----------+----+-----+------------+-----------------+------------+\n",
      "|2024-12-31|  23| 5271|        3116|74.09618668184405|          10|\n",
      "|2025-01-01|   0| 2032|         982|64.12795275590551|           6|\n",
      "|2025-01-01|   1|  881|         440|68.51645856980704|           0|\n",
      "+----------+----+-----+------------+-----------------+------------+\n",
      "\n",
      "\n",
      "‚ö†Ô∏è Failure Rates During New Year's Transition:\n",
      "+----------+----+-----+------------+\n",
      "|   CDR_DAY|hour|calls|failure_rate|\n",
      "+----------+----+-----+------------+\n",
      "|2024-12-31|  23| 5271|        0.19|\n",
      "|2025-01-01|   0| 2032|         0.3|\n",
      "|2025-01-01|   1|  881|         0.0|\n",
      "+----------+----+-----+------------+\n",
      "\n",
      "\n",
      "üìä COMPREHENSIVE DATA QUALITY SCORECARD\n",
      "================================================================================\n",
      "\n",
      "üéØ Data Quality Dimensions:\n",
      "  1. Completeness: 80.56%\n",
      "  2. Validity: 100.00%\n",
      "  3. Uniqueness: 3.94%\n",
      "  4. Consistency: 100.00%\n",
      "\n",
      "üìà OVERALL DATA QUALITY SCORE: 71.12%\n",
      "üìä Quality Grade: D (Poor)\n",
      "\n",
      "üí° RECOMMENDATIONS:\n",
      "  ‚ö†Ô∏è Address missing values in CallingPartyIMSI and CalledPartyIMSI fields\n",
      "  ‚ö†Ô∏è Review duplicate CDR_ID entries\n",
      "\n",
      "‚úÖ Data Quality Assessment Complete!\n",
      "   Analysis completed at: 2025-06-29 04:12:32.419748\n"
     ]
    }
   ],
   "source": [
    "# Notebook 02A: Comprehensive Data Quality Assessment\n",
    "## CDR Telecom Big Data Engineering - Algerie Telecom\n",
    "\n",
    "# ============================================================\n",
    "# NOTEBOOK 02A: COMPREHENSIVE DATA QUALITY ASSESSMENT\n",
    "# Project: CDR Telecom Big Data Engineering Final Year Internship\n",
    "# Focus: New Year's Eve Data Quality Analysis (Dec 31, 2024 - Jan 1, 2025)\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 1: Setup and Initialization\n",
    "# ------------------------------------------------------------\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "spark = init_spark(\"CDR Data Quality Assessment - New Year's Analysis\")\n",
    "\n",
    "# Load anonymized data\n",
    "df = spark.read.parquet(\"/user/hive/warehouse/cdr_anonymized/\")\n",
    "total_rows = df.count()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä CDR DATA QUALITY ASSESSMENT - NEW YEAR'S EVE SPECIAL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Analysis Period: Dec 31, 2024 - Jan 1, 2025\")\n",
    "print(f\"Total Records: {total_rows:,}\")\n",
    "print(f\"Analysis Started: {datetime.now()}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 2: Schema and Data Type Validation\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüîç SCHEMA VALIDATION AND DATA TYPES\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyze data types\n",
    "type_summary = {}\n",
    "for field in df.schema.fields:\n",
    "    dtype = str(field.dataType)\n",
    "    if dtype not in type_summary:\n",
    "        type_summary[dtype] = []\n",
    "    type_summary[dtype].append(field.name)\n",
    "\n",
    "print(\"\\nüìã Data Type Distribution:\")\n",
    "for dtype, columns in type_summary.items():\n",
    "    print(f\"\\n{dtype}: {len(columns)} columns\")\n",
    "    for col in columns[:5]:  # Show first 5 columns\n",
    "        print(f\"  - {col}\")\n",
    "    if len(columns) > 5:\n",
    "        print(f\"  ... and {len(columns) - 5} more\")\n",
    "\n",
    "# Check for schema consistency\n",
    "print(\"\\n‚úÖ Schema Validation Results:\")\n",
    "print(f\"  - Total Columns: {len(df.columns)}\")\n",
    "print(f\"  - Numeric Columns: {len([c for c, t in df.dtypes if t in ['double', 'int', 'bigint']])}\")\n",
    "print(f\"  - String Columns: {len([c for c, t in df.dtypes if t == 'string'])}\")\n",
    "print(f\"  - Date Columns: {len([c for c, t in df.dtypes if 'date' in t])}\")\n",
    "print(f\"  - Hash Columns: {len([c for c in df.columns if c.endswith('_HASH')])}\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 3: Completeness Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìä DATA COMPLETENESS ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Calculate null counts and percentages\n",
    "null_analysis = []\n",
    "for col in df.columns:\n",
    "    null_count = df.filter(F.col(col).isNull()).count()\n",
    "    null_percentage = (null_count / total_rows) * 100\n",
    "    null_analysis.append({\n",
    "        'column': col,\n",
    "        'null_count': null_count,\n",
    "        'null_percentage': null_percentage,\n",
    "        'completeness': 100 - null_percentage\n",
    "    })\n",
    "\n",
    "# Create DataFrame and sort by completeness\n",
    "null_df = spark.createDataFrame(null_analysis)\n",
    "null_df = null_df.orderBy('completeness')\n",
    "\n",
    "print(\"\\nüî¥ Columns with Missing Data (>0% nulls):\")\n",
    "null_df.filter(F.col('null_percentage') > 0).show(20, truncate=False)\n",
    "\n",
    "print(\"\\n‚úÖ Columns with Complete Data (100% complete):\")\n",
    "complete_columns = null_df.filter(F.col('completeness') == 100).count()\n",
    "print(f\"   {complete_columns} out of {len(df.columns)} columns have no missing values\")\n",
    "\n",
    "# Overall completeness score\n",
    "avg_completeness = null_df.agg(F.avg('completeness')).collect()[0][0]\n",
    "print(f\"\\nüìà Overall Data Completeness Score: {avg_completeness:.2f}%\")\n",
    "\n",
    "# Critical fields completeness check\n",
    "critical_fields = ['CDR_ID', 'START_DATE', 'END_DATE', 'PRI_IDENTITY_HASH', \n",
    "                  'ACTUAL_USAGE', 'DEBIT_AMOUNT', 'CDR_DAY']\n",
    "print(\"\\nüîç Critical Fields Completeness:\")\n",
    "for field in critical_fields:\n",
    "    if field in df.columns:\n",
    "        completeness = null_df.filter(F.col('column') == field).select('completeness').collect()[0][0]\n",
    "        print(f\"   {field}: {completeness:.2f}%\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 4: Validity and Range Checks\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüéØ DATA VALIDITY AND RANGE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Numeric columns validation\n",
    "numeric_cols = ['ACTUAL_USAGE', 'RATE_USAGE', 'DEBIT_AMOUNT', \n",
    "                'UN_DEBIT_AMOUNT', 'TOTAL_TAX', 'ChargingTime']\n",
    "\n",
    "print(\"\\nüìä Numeric Columns Statistics:\")\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        stats = df.select(\n",
    "            F.min(col).alias('min'),\n",
    "            F.max(col).alias('max'),\n",
    "            F.avg(col).alias('avg'),\n",
    "            F.stddev(col).alias('stddev'),\n",
    "            F.expr(f\"percentile_approx({col}, 0.5)\").alias('median'),\n",
    "            F.sum(F.when(F.col(col) < 0, 1).otherwise(0)).alias('negative_values'),\n",
    "            F.sum(F.when(F.col(col) == 0, 1).otherwise(0)).alias('zero_values')\n",
    "        ).collect()[0]\n",
    "        \n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Range: [{stats['min']:.2f}, {stats['max']:.2f}]\")\n",
    "        print(f\"  Mean: {stats['avg']:.2f}, Median: {stats['median']:.2f}\")\n",
    "        print(f\"  Negative values: {stats['negative_values']:,}\")\n",
    "        print(f\"  Zero values: {stats['zero_values']:,}\")\n",
    "        \n",
    "        # Outlier detection using IQR method\n",
    "        q1 = df.select(F.expr(f\"percentile_approx({col}, 0.25)\")).collect()[0][0]\n",
    "        q3 = df.select(F.expr(f\"percentile_approx({col}, 0.75)\")).collect()[0][0]\n",
    "        iqr = q3 - q1\n",
    "        lower_bound = q1 - 1.5 * iqr\n",
    "        upper_bound = q3 + 1.5 * iqr\n",
    "        \n",
    "        outliers = df.filter(\n",
    "            (F.col(col) < lower_bound) | (F.col(col) > upper_bound)\n",
    "        ).count()\n",
    "        \n",
    "        print(f\"  Outliers (IQR method): {outliers:,} ({outliers/total_rows*100:.2f}%)\")\n",
    "\n",
    "# Date validation\n",
    "print(\"\\nüìÖ Date Range Validation:\")\n",
    "date_stats = df.select(\n",
    "    F.min('CDR_DAY').alias('min_date'),\n",
    "    F.max('CDR_DAY').alias('max_date'),\n",
    "    F.countDistinct('CDR_DAY').alias('unique_days')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Date Range: {date_stats['min_date']} to {date_stats['max_date']}\")\n",
    "print(f\"  Unique Days: {date_stats['unique_days']}\")\n",
    "\n",
    "# Validate START_DATE and END_DATE consistency\n",
    "print(\"\\n‚è±Ô∏è Call Duration Consistency Check:\")\n",
    "df_with_duration = df.withColumn(\n",
    "    'calculated_duration',\n",
    "    (F.unix_timestamp(F.col('END_DATE'), 'yyyyMMddHHmmss') - \n",
    "     F.unix_timestamp(F.col('START_DATE'), 'yyyyMMddHHmmss'))\n",
    ")\n",
    "\n",
    "duration_issues = df_with_duration.filter(\n",
    "    (F.col('calculated_duration') < 0) | \n",
    "    (F.col('calculated_duration') > 86400)  # More than 24 hours\n",
    ").count()\n",
    "\n",
    "print(f\"  Records with invalid duration: {duration_issues:,} ({duration_issues/total_rows*100:.2f}%)\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 5: Uniqueness and Duplicate Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüîç UNIQUENESS AND DUPLICATE ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Check CDR_ID uniqueness\n",
    "unique_cdr_ids = df.select('CDR_ID').distinct().count()\n",
    "duplicate_cdr_ids = total_rows - unique_cdr_ids\n",
    "\n",
    "print(f\"\\nüìã CDR_ID Uniqueness:\")\n",
    "print(f\"  Total Records: {total_rows:,}\")\n",
    "print(f\"  Unique CDR_IDs: {unique_cdr_ids:,}\")\n",
    "print(f\"  Duplicate CDR_IDs: {duplicate_cdr_ids:,} ({duplicate_cdr_ids/total_rows*100:.2f}%)\")\n",
    "\n",
    "if duplicate_cdr_ids > 0:\n",
    "    # Find most duplicated CDR_IDs\n",
    "    print(\"\\n  Most Duplicated CDR_IDs:\")\n",
    "    df.groupBy('CDR_ID').count() \\\n",
    "        .filter(F.col('count') > 1) \\\n",
    "        .orderBy(F.desc('count')) \\\n",
    "        .show(10)\n",
    "\n",
    "# Check for complete duplicate rows\n",
    "print(\"\\nüìã Complete Duplicate Rows Check:\")\n",
    "duplicate_rows = df.groupBy(df.columns).count() \\\n",
    "    .filter(F.col('count') > 1) \\\n",
    "    .agg(F.sum(F.col('count') - 1)).collect()\n",
    "\n",
    "if duplicate_rows[0][0]:\n",
    "    print(f\"  Found {duplicate_rows[0][0]:,} complete duplicate rows\")\n",
    "else:\n",
    "    print(\"  ‚úÖ No complete duplicate rows found\")\n",
    "\n",
    "# Analyze hash columns for uniqueness\n",
    "print(\"\\nüîê Hash Columns Uniqueness Analysis:\")\n",
    "hash_cols = [c for c in df.columns if c.endswith('_HASH')]\n",
    "for col in hash_cols:\n",
    "    unique_values = df.select(col).distinct().count()\n",
    "    print(f\"  {col}: {unique_values:,} unique values\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 6: Consistency and Business Rules Validation\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìè BUSINESS RULES AND CONSISTENCY CHECKS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Rule 1: ACTUAL_USAGE should be <= RATE_USAGE\n",
    "print(\"\\n1Ô∏è‚É£ Usage Consistency (ACTUAL_USAGE <= RATE_USAGE):\")\n",
    "usage_violations = df.filter(\n",
    "    (F.col('ACTUAL_USAGE') > F.col('RATE_USAGE')) & \n",
    "    F.col('ACTUAL_USAGE').isNotNull() & \n",
    "    F.col('RATE_USAGE').isNotNull()\n",
    ").count()\n",
    "print(f\"   Violations: {usage_violations:,} ({usage_violations/total_rows*100:.2f}%)\")\n",
    "\n",
    "# Rule 2: DEBIT_AMOUNT should be 0 when ACTUAL_USAGE is 0\n",
    "print(\"\\n2Ô∏è‚É£ Revenue Consistency (Zero usage = Zero charge):\")\n",
    "revenue_violations = df.filter(\n",
    "    (F.col('ACTUAL_USAGE') == 0) & (F.col('DEBIT_AMOUNT') > 0)\n",
    ").count()\n",
    "print(f\"   Violations: {revenue_violations:,} ({revenue_violations/total_rows*100:.2f}%)\")\n",
    "\n",
    "# Rule 3: Call success validation\n",
    "print(\"\\n3Ô∏è‚É£ Call Success Patterns:\")\n",
    "call_patterns = df.groupBy(\n",
    "    F.when(F.col('ACTUAL_USAGE') > 0, 'Successful').otherwise('Failed')\n",
    ").count().orderBy('count', ascending=False)\n",
    "call_patterns.show()\n",
    "\n",
    "# Rule 4: Service category distribution\n",
    "print(\"\\n4Ô∏è‚É£ Service Category Distribution:\")\n",
    "df.groupBy('SERVICE_CATEGORY').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    F.avg('ACTUAL_USAGE').alias('avg_duration'),\n",
    "    F.avg('DEBIT_AMOUNT').alias('avg_revenue')\n",
    ").orderBy('count', ascending=False).show()\n",
    "\n",
    "# Rule 5: Call type distribution\n",
    "print(\"\\n5Ô∏è‚É£ Call Type Distribution:\")\n",
    "df.groupBy('CallType').count().orderBy('count', ascending=False).show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 7: Timeliness and Data Freshness\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n‚è∞ DATA TIMELINESS AND FRESHNESS ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Analyze processing delays\n",
    "df_time = df.withColumn(\n",
    "    'processing_delay_seconds',\n",
    "    F.unix_timestamp(F.col('CREATE_DATE'), 'yyyyMMddHHmmss') - \n",
    "    F.unix_timestamp(F.col('END_DATE'), 'yyyyMMddHHmmss')\n",
    ")\n",
    "\n",
    "delay_stats = df_time.select(\n",
    "    F.min('processing_delay_seconds').alias('min_delay'),\n",
    "    F.max('processing_delay_seconds').alias('max_delay'),\n",
    "    F.avg('processing_delay_seconds').alias('avg_delay'),\n",
    "    F.expr(\"percentile_approx(processing_delay_seconds, 0.5)\").alias('median_delay')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nüìä Processing Delay Statistics:\")\n",
    "print(f\"  Minimum Delay: {delay_stats['min_delay']:,.0f} seconds\")\n",
    "print(f\"  Maximum Delay: {delay_stats['max_delay']:,.0f} seconds\")\n",
    "print(f\"  Average Delay: {delay_stats['avg_delay']:,.0f} seconds\")\n",
    "print(f\"  Median Delay: {delay_stats['median_delay']:,.0f} seconds\")\n",
    "\n",
    "# Data distribution by hour for both days\n",
    "print(\"\\nüìÖ Hourly Data Distribution (New Year's Transition):\")\n",
    "hourly_dist = df.withColumn(\n",
    "    'call_hour', F.hour(F.to_timestamp(F.col('START_DATE'), 'yyyyMMddHHmmss'))\n",
    ").groupBy('CDR_DAY', 'call_hour').count().orderBy('CDR_DAY', 'call_hour')\n",
    "\n",
    "hourly_dist.show(48)  # Show all hours for both days\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 8: New Year's Eve Special Quality Checks\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüéÜ NEW YEAR'S EVE SPECIAL QUALITY ANALYSIS\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Compare data quality between Dec 31 and Jan 1\n",
    "print(\"\\nüìä Quality Metrics Comparison:\")\n",
    "quality_by_day = df.groupBy('CDR_DAY').agg(\n",
    "    F.count('*').alias('total_records'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed_calls'),\n",
    "    F.avg('ACTUAL_USAGE').alias('avg_duration'),\n",
    "    F.sum('DEBIT_AMOUNT').alias('total_revenue'),\n",
    "    F.countDistinct('CallingCellID').alias('active_cells')\n",
    ").orderBy('CDR_DAY')\n",
    "\n",
    "quality_by_day.show()\n",
    "\n",
    "# Analyze midnight spike\n",
    "print(\"\\nüïê Midnight Analysis (23:00 - 01:00):\")\n",
    "midnight_df = df.withColumn(\n",
    "    'hour', F.hour(F.to_timestamp(F.col('START_DATE'), 'yyyyMMddHHmmss'))\n",
    ").filter(\n",
    "    ((F.col('CDR_DAY') == '2024-12-31') & (F.col('hour') >= 23)) |\n",
    "    ((F.col('CDR_DAY') == '2025-01-01') & (F.col('hour') <= 1))\n",
    ")\n",
    "\n",
    "midnight_stats = midnight_df.groupBy('CDR_DAY', 'hour').agg(\n",
    "    F.count('*').alias('calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users'),\n",
    "    F.avg('ACTUAL_USAGE').alias('avg_duration'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed_calls')\n",
    ").orderBy('CDR_DAY', 'hour')\n",
    "\n",
    "midnight_stats.show()\n",
    "\n",
    "# Calculate failure rate during peak hours\n",
    "failure_rate = midnight_stats.withColumn(\n",
    "    'failure_rate', F.round(F.col('failed_calls') / F.col('calls') * 100, 2)\n",
    ")\n",
    "print(\"\\n‚ö†Ô∏è Failure Rates During New Year's Transition:\")\n",
    "failure_rate.select('CDR_DAY', 'hour', 'calls', 'failure_rate').show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 9: Data Quality Score Card\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìä COMPREHENSIVE DATA QUALITY SCORECARD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Calculate individual quality dimensions\n",
    "completeness_score = avg_completeness\n",
    "\n",
    "validity_score = 100 - (duration_issues / total_rows * 100)\n",
    "\n",
    "uniqueness_score = (unique_cdr_ids / total_rows) * 100\n",
    "\n",
    "consistency_score = 100 - ((usage_violations + revenue_violations) / (total_rows * 2) * 100)\n",
    "\n",
    "# Overall Data Quality Score\n",
    "overall_score = (completeness_score + validity_score + uniqueness_score + consistency_score) / 4\n",
    "\n",
    "print(f\"\\nüéØ Data Quality Dimensions:\")\n",
    "print(f\"  1. Completeness: {completeness_score:.2f}%\")\n",
    "print(f\"  2. Validity: {validity_score:.2f}%\")\n",
    "print(f\"  3. Uniqueness: {uniqueness_score:.2f}%\")\n",
    "print(f\"  4. Consistency: {consistency_score:.2f}%\")\n",
    "print(f\"\\nüìà OVERALL DATA QUALITY SCORE: {overall_score:.2f}%\")\n",
    "\n",
    "# Quality Grade\n",
    "if overall_score >= 95:\n",
    "    grade = \"A+ (Excellent)\"\n",
    "elif overall_score >= 90:\n",
    "    grade = \"A (Very Good)\"\n",
    "elif overall_score >= 85:\n",
    "    grade = \"B (Good)\"\n",
    "elif overall_score >= 80:\n",
    "    grade = \"C (Fair)\"\n",
    "else:\n",
    "    grade = \"D (Poor)\"\n",
    "\n",
    "print(f\"üìä Quality Grade: {grade}\")\n",
    "\n",
    "# Recommendations\n",
    "print(\"\\nüí° RECOMMENDATIONS:\")\n",
    "if completeness_score < 95:\n",
    "    print(\"  ‚ö†Ô∏è Address missing values in CallingPartyIMSI and CalledPartyIMSI fields\")\n",
    "if validity_score < 95:\n",
    "    print(\"  ‚ö†Ô∏è Investigate records with invalid duration calculations\")\n",
    "if uniqueness_score < 100:\n",
    "    print(\"  ‚ö†Ô∏è Review duplicate CDR_ID entries\")\n",
    "if consistency_score < 95:\n",
    "    print(\"  ‚ö†Ô∏è Fix business rule violations in usage and revenue fields\")\n",
    "\n",
    "print(\"\\n‚úÖ Data Quality Assessment Complete!\")\n",
    "print(f\"   Analysis completed at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e25a9323-fc91-43a6-8cfd-69e781bd9352",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 04:14:08 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: CDR Exploratory Data Analysis - New Year's Special, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "================================================================================\n",
      "üéä CDR EXPLORATORY DATA ANALYSIS - NEW YEAR'S EVE SPECIAL\n",
      "================================================================================\n",
      "Analysis Period: December 31, 2024 - January 1, 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Records: 89,911\n",
      "Unique Subscribers: 40,843\n",
      "================================================================================\n",
      "\n",
      "üìÖ TEMPORAL ANALYSIS - NEW YEAR'S EVE CELEBRATION\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä Hourly Call Volume Distribution:\n",
      "+----------+---------+-----------+------------+----------------+------------+------------------+-------------+\n",
      "|   CDR_DAY|call_hour|total_calls|unique_users|successful_calls|failed_calls|      avg_duration|total_revenue|\n",
      "+----------+---------+-----------+------------+----------------+------------+------------------+-------------+\n",
      "|2024-12-31|       21|         30|          30|              30|           0|            2907.0|     216000.0|\n",
      "|2024-12-31|       22|       1880|        1076|            1877|           3| 146.9563829787234|    1928527.0|\n",
      "|2024-12-31|       23|       5271|        3116|            5261|          10| 74.09618668184405|    1589915.0|\n",
      "|2025-01-01|        0|       2032|         982|            2026|           6| 64.12795275590551|     810174.0|\n",
      "|2025-01-01|        1|        881|         440|             881|           0| 68.51645856980704|     612753.0|\n",
      "|2025-01-01|        2|        643|         281|             643|           0| 67.65629860031105|     520752.0|\n",
      "|2025-01-01|        3|        560|         258|             557|           3| 42.46785714285714|     309090.0|\n",
      "|2025-01-01|        4|        622|         283|             622|           0|38.033762057877816|     291576.0|\n",
      "|2025-01-01|        5|       1061|         512|            1061|           0| 48.29123468426013|     633294.0|\n",
      "|2025-01-01|        6|       2031|        1130|            2031|           0| 67.81831610044313|    1072041.0|\n",
      "|2025-01-01|        7|       5199|        3244|            5192|           7|121.89440276976342|    2820382.0|\n",
      "|2025-01-01|        8|      10827|        6975|           10815|          12|174.40168098272835|    5871458.0|\n",
      "|2025-01-01|        9|      16393|       10888|           16380|          13|213.84853291038857|    8206586.0|\n",
      "|2025-01-01|       10|      18125|       11951|           18108|          17|200.44375172413794|    9718766.0|\n",
      "|2025-01-01|       11|      14625|       10071|           14604|          21| 166.4119658119658|    6803847.0|\n",
      "|2025-01-01|       12|       8912|        6116|            8895|          17|123.12264362657092|    4419455.0|\n",
      "|2025-01-01|       13|        819|         618|             812|           7| 87.12698412698413|     277089.0|\n",
      "+----------+---------+-----------+------------+----------------+------------+------------------+-------------+\n",
      "\n",
      "\n",
      "üîù Top 5 Peak Hours:\n",
      "+----------+---------+-----------+------------+----------------+------------+------------------+-------------+\n",
      "|   CDR_DAY|call_hour|total_calls|unique_users|successful_calls|failed_calls|      avg_duration|total_revenue|\n",
      "+----------+---------+-----------+------------+----------------+------------+------------------+-------------+\n",
      "|2025-01-01|       10|      18125|       11951|           18108|          17|200.44375172413794|    9718766.0|\n",
      "|2025-01-01|        9|      16393|       10888|           16380|          13|213.84853291038857|    8206586.0|\n",
      "|2025-01-01|       11|      14625|       10071|           14604|          21| 166.4119658119658|    6803847.0|\n",
      "|2025-01-01|        8|      10827|        6975|           10815|          12|174.40168098272835|    5871458.0|\n",
      "|2025-01-01|       12|       8912|        6116|            8895|          17|123.12264362657092|    4419455.0|\n",
      "+----------+---------+-----------+------------+----------------+------------+------------------+-------------+\n",
      "\n",
      "\n",
      "üïê Midnight Celebration Window (22:00 Dec 31 - 02:00 Jan 1):\n",
      "+----------+---------+-----+------------+----------------+---------+\n",
      "|   CDR_DAY|call_hour|calls|active_users|avg_duration_sec|  revenue|\n",
      "+----------+---------+-----+------------+----------------+---------+\n",
      "|2024-12-31|       22| 1880|        1076|          146.96|1928527.0|\n",
      "|2024-12-31|       23| 5271|        3116|            74.1|1589915.0|\n",
      "|2025-01-01|        0| 2032|         982|           64.13| 810174.0|\n",
      "|2025-01-01|        1|  881|         440|           68.52| 612753.0|\n",
      "|2025-01-01|        2|  643|         281|           67.66| 520752.0|\n",
      "+----------+---------+-----+------------+----------------+---------+\n",
      "\n",
      "\n",
      "üìû CALL PATTERN ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚è±Ô∏è Call Duration Distribution:\n",
      "\n",
      "Call Duration Categories:\n",
      "  0-10s: 6,452 calls (7.18%)\n",
      "  10-30s: 21,296 calls (23.69%)\n",
      "  30-60s: 21,994 calls (24.46%)\n",
      "  1-2min: 18,478 calls (20.55%)\n",
      "  2-5min: 10,348 calls (11.51%)\n",
      "  5-10min: 5,394 calls (6.00%)\n",
      "  10-30min: 5,031 calls (5.60%)\n",
      "  30-60min: 796 calls (0.89%)\n",
      "  >1hour: 122 calls (0.14%)\n",
      "\n",
      "üìä Call Success Patterns by Day:\n",
      "+----------+-----------+----------+------+--------------------+------------+------------+\n",
      "|   CDR_DAY|total_calls|successful|failed|avg_success_duration|success_rate|failure_rate|\n",
      "+----------+-----------+----------+------+--------------------+------------+------------+\n",
      "|2025-01-01|      82730|     82627|   103|              166.21|       99.88|        0.12|\n",
      "|2024-12-31|       7181|      7168|    13|               105.2|       99.82|        0.18|\n",
      "+----------+-----------+----------+------+--------------------+------------+------------+\n",
      "\n",
      "\n",
      "üì± Service Category Distribution:\n",
      "+----------------+-----+------------+-------------+------------+\n",
      "|SERVICE_CATEGORY|count|avg_duration|total_revenue|unique_users|\n",
      "+----------------+-----+------------+-------------+------------+\n",
      "|               1|89911|      161.13|  4.6101705E7|       40843|\n",
      "+----------------+-----+------------+-------------+------------+\n",
      "\n",
      "\n",
      "üë• USER BEHAVIOR ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä User Segmentation:\n",
      "+------------+----------+---------+------------+---------+\n",
      "|user_segment|user_count|avg_calls|avg_duration|avg_spend|\n",
      "+------------+----------+---------+------------+---------+\n",
      "|Minimal User|     37842|     1.53|      313.12|   567.94|\n",
      "|  Light User|      2787|     7.54|      741.06|  5109.77|\n",
      "| Medium User|       180|    26.81|     1652.39| 25433.26|\n",
      "|  Heavy User|        34|   181.62|     8112.59|170312.44|\n",
      "+------------+----------+---------+------------+---------+\n",
      "\n",
      "\n",
      "üèÜ Top 10 Most Active Users:\n",
      "+-----------+-------------+-----------+-----------+\n",
      "|total_calls|total_minutes|total_spend|active_days|\n",
      "+-----------+-------------+-----------+-----------+\n",
      "|       1429|       473.47|  1500450.0|          2|\n",
      "|       1036|       315.62|        0.0|          2|\n",
      "|        594|       936.57|  1210994.0|          2|\n",
      "|        464|       393.87|   558000.0|          2|\n",
      "|        392|       275.23|   429300.0|          1|\n",
      "|        209|       112.85|   208800.0|          2|\n",
      "|        124|       188.33|   225000.0|          2|\n",
      "|        124|        53.95|        0.0|          1|\n",
      "|        111|       103.28|   134100.0|          1|\n",
      "|         99|        54.73|    43679.0|          1|\n",
      "+-----------+-------------+-----------+-----------+\n",
      "\n",
      "\n",
      "üéä Users active at midnight transition: 258\n",
      "\n",
      "üí∞ REVENUE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "üíµ Total Revenue: 46,101,705.00 DZD\n",
      "üìä Average Revenue per Call: 512.75 DZD\n",
      "üí≥ Paid Calls: 35,162 (39.11%)\n",
      "üÜì Free Calls: 54,749 (60.89%)\n",
      "\n",
      "üìÖ Daily Revenue Comparison:\n",
      "+----------+-------------+-----------+--------------------+------------+-------+\n",
      "|   CDR_DAY|total_revenue|total_calls|avg_revenue_per_call|unique_users|   ARPU|\n",
      "+----------+-------------+-----------+--------------------+------------+-------+\n",
      "|2024-12-31|    3734442.0|       7181|   520.0448405514552|        3924| 951.69|\n",
      "|2025-01-01|  4.2367263E7|      82730|    512.114867641726|       38311|1105.88|\n",
      "+----------+-------------+-----------+--------------------+------------+-------+\n",
      "\n",
      "\n",
      "üí∞ Revenue Distribution Analysis:\n",
      "  Q1 (25th percentile): 400.00 DZD\n",
      "  Median: 900.00 DZD\n",
      "  Q3 (75th percentile): 1050.00 DZD\n",
      "  95th percentile: 3600.00 DZD\n",
      "  99th percentile: 11370.00 DZD\n",
      "\n",
      "‚è∞ Hourly Revenue Pattern:\n",
      "\n",
      "üîù Top 5 Revenue Hours:\n",
      "+----------+---------+--------------+-----+\n",
      "|   CDR_DAY|call_hour|hourly_revenue|calls|\n",
      "+----------+---------+--------------+-----+\n",
      "|2025-01-01|       10|     9718766.0|18125|\n",
      "|2025-01-01|        9|     8206586.0|16393|\n",
      "|2025-01-01|       11|     6803847.0|14625|\n",
      "|2025-01-01|        8|     5871458.0|10827|\n",
      "|2025-01-01|       12|     4419455.0| 8912|\n",
      "+----------+---------+--------------+-----+\n",
      "\n",
      "\n",
      "üì° NETWORK CELL ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìä Cell Distribution Summary:\n",
      "Total Active Cells: 2\n",
      "Cells with >1000 calls: 2\n",
      "Cells with >50% failure rate: 0\n",
      "\n",
      "üèÜ Top 10 Busiest Cells:\n",
      "+----------------+-----------+------------+------------+------------------+-------------+------------+--------------+\n",
      "|   CallingCellID|total_calls|unique_users|failed_calls|      avg_duration|total_revenue|failure_rate|calls_per_user|\n",
      "+----------------+-----------+------------+------------+------------------+-------------+------------+--------------+\n",
      "|            NULL|      71470|       31179|           4|154.47755701693018|  4.0061217E7|        0.01|          2.29|\n",
      "|603093E819668914|      18441|        9664|         112|186.92945068054877|    6040488.0|        0.61|          1.91|\n",
      "+----------------+-----------+------------+------------+------------------+-------------+------------+--------------+\n",
      "\n",
      "\n",
      "‚ö†Ô∏è Cells with High Failure Rates (>30%):\n",
      "+-------------+-----------+------------+------------+\n",
      "|CallingCellID|total_calls|failure_rate|unique_users|\n",
      "+-------------+-----------+------------+------------+\n",
      "+-------------+-----------+------------+------------+\n",
      "\n",
      "\n",
      "üéä Cells with Highest New Year's Eve Activity:\n",
      "+----------------+---------+---------+\n",
      "|   CallingCellID|nye_calls|nye_users|\n",
      "+----------------+---------+---------+\n",
      "|            NULL|     8470|     3940|\n",
      "|603093E819668914|     2237|     1048|\n",
      "+----------------+---------+---------+\n",
      "\n",
      "\n",
      "üì± CALL TYPE AND SERVICE ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìû Call Type Distribution:\n",
      "+--------+-----+------------+------------+-------------+------------+\n",
      "|CallType|count|avg_duration|failed_calls|total_revenue|failure_rate|\n",
      "+--------+-----+------------+------------+-------------+------------+\n",
      "|       1|47743|        73.5|          73|  3.0206567E7|        0.15|\n",
      "|       0|40030|      270.83|          41|    9542339.0|         0.1|\n",
      "|       3| 2138|        64.2|           2|    6352799.0|        0.09|\n",
      "+--------+-----+------------+------------+-------------+------------+\n",
      "\n",
      "\n",
      "üîÑ Service Flow Distribution:\n",
      "+-----------+-----+------------+------------+\n",
      "|ServiceFlow|count|unique_users|avg_duration|\n",
      "+-----------+-----+------------+------------+\n",
      "|          1|89345|       40632|       161.4|\n",
      "|          2|  440|         241|      120.13|\n",
      "|          3|  126|          16|      112.15|\n",
      "+-----------+-----+------------+------------+\n",
      "\n",
      "\n",
      "üåç Roaming Analysis:\n",
      "+---------+-----+------------+-------------+\n",
      "|RoamState|count|avg_duration|total_revenue|\n",
      "+---------+-----+------------+-------------+\n",
      "|        0|89911|      161.13|  4.6101705E7|\n",
      "+---------+-----+------------+-------------+\n",
      "\n",
      "\n",
      "‚Ü™Ô∏è Call Forwarding Patterns:\n",
      "+--------------------+-----+----------+\n",
      "|CallForwardIndicator|count|percentage|\n",
      "+--------------------+-----+----------+\n",
      "|                   0|89785|     99.86|\n",
      "|                  11|  126|      0.14|\n",
      "+--------------------+-----+----------+\n",
      "\n",
      "\n",
      "üéÜ NEW YEAR'S EVE SPECIAL INSIGHTS\n",
      "============================================================\n",
      "\n",
      "üìä Key Metrics Comparison:\n",
      "+----------+-----------+------------+------------+-------------+------------+--------------+------------+\n",
      "|   CDR_DAY|total_calls|unique_users|avg_duration|total_revenue|failed_calls|calls_per_user|failure_rate|\n",
      "+----------+-----------+------------+------------+-------------+------------+--------------+------------+\n",
      "|2024-12-31|       7181|        3924|      105.01|    3734442.0|          13|          1.83|        0.18|\n",
      "|2025-01-01|      82730|       38311|      166.01|  4.2367263E7|         103|          2.16|        0.12|\n",
      "+----------+-----------+------------+------------+-------------+------------+--------------+------------+\n",
      "\n",
      "\n",
      "üìà New Year's Day Growth Rates:\n",
      "  Call Volume: +1052.1%\n",
      "  Active Users: +876.3%\n",
      "  Revenue: +1034.5%\n",
      "\n",
      "üïê Midnight Hour Deep Dive (23:00-00:59):\n",
      "+----------+---------+-----+--------------+-----------+------------+--------------------+----------------+\n",
      "|   CDR_DAY|call_hour|calls|unique_callers|short_calls|failed_calls|avg_success_duration|short_call_ratio|\n",
      "+----------+---------+-----+--------------+-----------+------------+--------------------+----------------+\n",
      "|2024-12-31|       23| 5271|          3116|       1263|          10|               74.24|           23.96|\n",
      "|2025-01-01|        0| 2032|           982|        765|           6|               64.32|           37.65|\n",
      "+----------+---------+-----+--------------+-----------+------------+--------------------+----------------+\n",
      "\n",
      "\n",
      "üéâ Service Usage During Celebration Hours (22:00-02:00):\n",
      "+----------------+-----------------+------------+\n",
      "|SERVICE_CATEGORY|celebration_calls|unique_users|\n",
      "+----------------+-----------------+------------+\n",
      "|               1|            10707|        4988|\n",
      "+----------------+-----------------+------------+\n",
      "\n",
      "\n",
      "üîç KEY FINDINGS AND BUSINESS INSIGHTS\n",
      "============================================================\n",
      "\n",
      "1Ô∏è‚É£ NEW YEAR'S TRAFFIC SURGE:\n",
      "   ‚Ä¢ Dec 31: 7,181 calls\n",
      "   ‚Ä¢ Jan 1: 82,730 calls\n",
      "   ‚Ä¢ Surge Factor: 11.5x\n",
      "   ‚Ä¢ This represents a 1052% increase in traffic\n",
      "\n",
      "2Ô∏è‚É£ NETWORK RESILIENCE:\n",
      "   ‚Ä¢ Dec 31 Failure Rate: 0.18%\n",
      "   ‚Ä¢ Jan 1 Failure Rate: 0.12%\n",
      "   ‚Ä¢ Despite 11.5x traffic surge, failure rate only increased by -0.06 percentage points\n",
      "\n",
      "3Ô∏è‚É£ MIDNIGHT CELEBRATION PATTERN:\n",
      "   ‚Ä¢ Calls in first hour of 2025: 2,032\n",
      "   ‚Ä¢ This represents 2.5% of all New Year's Day calls\n",
      "   ‚Ä¢ Clear spike indicating mass New Year greetings\n",
      "\n",
      "4Ô∏è‚É£ REVENUE OPPORTUNITY:\n",
      "   ‚Ä¢ Revenue increased by 1034.5% on New Year's Day\n",
      "   ‚Ä¢ ARPU on Dec 31: 951.69 DZD\n",
      "   ‚Ä¢ ARPU on Jan 1: 1105.88 DZD\n",
      "\n",
      "5Ô∏è‚É£ SERVICE PREFERENCES:\n",
      "   ‚Ä¢ Voice Calls: 89,911 (100.0%)\n",
      "   ‚Ä¢ SMS Messages: 0 (0.0%)\n",
      "   ‚Ä¢ Voice remains dominant for New Year greetings\n",
      "\n",
      "============================================================\n",
      "üí° BUSINESS RECOMMENDATIONS:\n",
      "============================================================\n",
      "1. üì° Network Capacity: Plan for 12x normal capacity for future New Year events\n",
      "2. üí∞ Revenue Optimization: Create special New Year packages to monetize surge\n",
      "3. üéØ Marketing: Target heavy users (50+ calls) with premium offerings\n",
      "4. üîß Infrastructure: Focus on cells with >30% failure rates before next holiday\n",
      "5. üì± Service Innovation: Consider special New Year SMS bundles\n",
      "\n",
      "‚úÖ EDA Complete! Analysis finished at: 2025-06-29 04:14:16.181117\n",
      "\n",
      "üìä EXPORTING KEY METRICS FOR VISUALIZATION\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 04:14:16 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n",
      "25/06/29 04:14:19 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Saved: eda_hourly_metrics\n",
      "‚úÖ Saved: eda_user_segments\n",
      "‚úÖ Saved: eda_cell_performance\n",
      "\n",
      "üìà Ready for visualization in Superset/PowerBI!\n",
      "   - Use eda_hourly_metrics for time series charts\n",
      "   - Use eda_user_segments for user distribution\n",
      "   - Use eda_cell_performance for network maps\n",
      "\n",
      "‚úÖ Analysis complete! Spark session closed.\n"
     ]
    }
   ],
   "source": [
    "# Notebook 02B: Comprehensive Exploratory Data Analysis (EDA)\n",
    "## CDR Telecom Big Data Engineering - New Year's Eve Focus\n",
    "\n",
    "# ============================================================\n",
    "# NOTEBOOK 02B: COMPREHENSIVE EXPLORATORY DATA ANALYSIS\n",
    "# Project: CDR Telecom Big Data Engineering Final Year Internship  \n",
    "# Focus: New Year's Eve Insights (Dec 31, 2024 - Jan 1, 2025)\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 1: Setup and Data Loading\n",
    "# ------------------------------------------------------------\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "spark = init_spark(\"CDR Exploratory Data Analysis - New Year's Special\")\n",
    "\n",
    "# Load data\n",
    "df = spark.read.parquet(\"/user/hive/warehouse/cdr_anonymized/\")\n",
    "df.cache()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üéä CDR EXPLORATORY DATA ANALYSIS - NEW YEAR'S EVE SPECIAL\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Analysis Period: December 31, 2024 - January 1, 2025\")\n",
    "print(f\"Total Records: {df.count():,}\")\n",
    "print(f\"Unique Subscribers: {df.select('PRI_IDENTITY_HASH').distinct().count():,}\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 2: Temporal Analysis - New Year's Transition\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìÖ TEMPORAL ANALYSIS - NEW YEAR'S EVE CELEBRATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Add temporal features\n",
    "df_temporal = df.withColumn(\n",
    "    'call_hour', F.hour(F.to_timestamp(F.col('START_DATE'), 'yyyyMMddHHmmss'))\n",
    ").withColumn(\n",
    "    'call_minute', F.minute(F.to_timestamp(F.col('START_DATE'), 'yyyyMMddHHmmss'))\n",
    ").withColumn(\n",
    "    'day_name', \n",
    "    F.when(F.col('CDR_DAY') == '2024-12-31', 'New Years Eve')\n",
    "    .otherwise('New Years Day')\n",
    ")\n",
    "\n",
    "# Hourly distribution\n",
    "hourly_stats = df_temporal.groupBy('CDR_DAY', 'call_hour').agg(\n",
    "    F.count('*').alias('total_calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') > 0, 1).otherwise(0)).alias('successful_calls'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed_calls'),\n",
    "    F.avg('ACTUAL_USAGE').alias('avg_duration'),\n",
    "    F.sum('DEBIT_AMOUNT').alias('total_revenue')\n",
    ").orderBy('CDR_DAY', 'call_hour')\n",
    "\n",
    "print(\"\\nüìä Hourly Call Volume Distribution:\")\n",
    "hourly_stats.show(48)\n",
    "\n",
    "# Identify peak hours\n",
    "peak_hours = hourly_stats.orderBy(F.desc('total_calls')).limit(5)\n",
    "print(\"\\nüîù Top 5 Peak Hours:\")\n",
    "peak_hours.show()\n",
    "\n",
    "# Midnight spike analysis\n",
    "midnight_window = df_temporal.filter(\n",
    "    ((F.col('CDR_DAY') == '2024-12-31') & (F.col('call_hour').between(22, 23))) |\n",
    "    ((F.col('CDR_DAY') == '2025-01-01') & (F.col('call_hour').between(0, 2)))\n",
    ")\n",
    "\n",
    "print(\"\\nüïê Midnight Celebration Window (22:00 Dec 31 - 02:00 Jan 1):\")\n",
    "midnight_stats = midnight_window.groupBy('CDR_DAY', 'call_hour').agg(\n",
    "    F.count('*').alias('calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('active_users'),\n",
    "    F.round(F.avg('ACTUAL_USAGE'), 2).alias('avg_duration_sec'),\n",
    "    F.round(F.sum('DEBIT_AMOUNT'), 2).alias('revenue')\n",
    ").orderBy('CDR_DAY', 'call_hour')\n",
    "midnight_stats.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 3: Call Pattern Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìû CALL PATTERN ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Call duration distribution\n",
    "print(\"\\n‚è±Ô∏è Call Duration Distribution:\")\n",
    "duration_buckets = [0, 10, 30, 60, 120, 300, 600, 1800, 3600, 86400]\n",
    "duration_labels = ['0-10s', '10-30s', '30-60s', '1-2min', '2-5min', \n",
    "                  '5-10min', '10-30min', '30-60min', '>1hour']\n",
    "\n",
    "df_duration = df\n",
    "for i in range(len(duration_buckets)-1):\n",
    "    df_duration = df_duration.withColumn(\n",
    "        f'duration_{duration_labels[i]}',\n",
    "        F.when((F.col('ACTUAL_USAGE') >= duration_buckets[i]) & \n",
    "               (F.col('ACTUAL_USAGE') < duration_buckets[i+1]), 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "duration_dist = df_duration.select(\n",
    "    *[F.sum(f'duration_{label}').alias(label) for label in duration_labels]\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\nCall Duration Categories:\")\n",
    "for label in duration_labels:\n",
    "    count = duration_dist[label]\n",
    "    percentage = (count / df.count()) * 100\n",
    "    print(f\"  {label}: {count:,} calls ({percentage:.2f}%)\")\n",
    "\n",
    "# Call types and patterns\n",
    "print(\"\\nüìä Call Success Patterns by Day:\")\n",
    "success_patterns = df.groupBy('CDR_DAY').agg(\n",
    "    F.count('*').alias('total_calls'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') > 0, 1).otherwise(0)).alias('successful'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed'),\n",
    "    F.round(F.avg(F.when(F.col('ACTUAL_USAGE') > 0, F.col('ACTUAL_USAGE'))), 2).alias('avg_success_duration')\n",
    ").withColumn(\n",
    "    'success_rate', F.round(F.col('successful') / F.col('total_calls') * 100, 2)\n",
    ").withColumn(\n",
    "    'failure_rate', F.round(F.col('failed') / F.col('total_calls') * 100, 2)\n",
    ")\n",
    "success_patterns.show()\n",
    "\n",
    "# Service category analysis\n",
    "print(\"\\nüì± Service Category Distribution:\")\n",
    "service_dist = df.groupBy('SERVICE_CATEGORY').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    F.round(F.avg('ACTUAL_USAGE'), 2).alias('avg_duration'),\n",
    "    F.round(F.sum('DEBIT_AMOUNT'), 2).alias('total_revenue'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users')\n",
    ").orderBy(F.desc('count'))\n",
    "service_dist.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 4: User Behavior Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüë• USER BEHAVIOR ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# User activity distribution\n",
    "user_stats = df.groupBy('PRI_IDENTITY_HASH').agg(\n",
    "    F.count('*').alias('total_calls'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') > 0, 1).otherwise(0)).alias('successful_calls'),\n",
    "    F.sum('ACTUAL_USAGE').alias('total_duration'),\n",
    "    F.sum('DEBIT_AMOUNT').alias('total_spend'),\n",
    "    F.countDistinct('CDR_DAY').alias('active_days')\n",
    ")\n",
    "\n",
    "# User segments\n",
    "user_segments = user_stats.withColumn(\n",
    "    'user_segment',\n",
    "    F.when(F.col('total_calls') >= 50, 'Heavy User')\n",
    "    .when(F.col('total_calls') >= 20, 'Medium User')\n",
    "    .when(F.col('total_calls') >= 5, 'Light User')\n",
    "    .otherwise('Minimal User')\n",
    ")\n",
    "\n",
    "print(\"\\nüìä User Segmentation:\")\n",
    "segment_summary = user_segments.groupBy('user_segment').agg(\n",
    "    F.count('*').alias('user_count'),\n",
    "    F.round(F.avg('total_calls'), 2).alias('avg_calls'),\n",
    "    F.round(F.avg('total_duration'), 2).alias('avg_duration'),\n",
    "    F.round(F.avg('total_spend'), 2).alias('avg_spend')\n",
    ").orderBy(F.desc('user_count'))\n",
    "segment_summary.show()\n",
    "\n",
    "# Top users analysis\n",
    "print(\"\\nüèÜ Top 10 Most Active Users:\")\n",
    "top_users = user_stats.orderBy(F.desc('total_calls')).limit(10)\n",
    "top_users.select(\n",
    "    F.col('total_calls'),\n",
    "    F.round(F.col('total_duration')/60, 2).alias('total_minutes'),\n",
    "    F.round(F.col('total_spend'), 2).alias('total_spend'),\n",
    "    F.col('active_days')\n",
    ").show()\n",
    "\n",
    "# New Year's Eve special users\n",
    "nye_active = df_temporal.filter(\n",
    "    (F.col('CDR_DAY') == '2024-12-31') & (F.col('call_hour').between(23, 23))\n",
    ").select('PRI_IDENTITY_HASH').distinct()\n",
    "\n",
    "ny_active = df_temporal.filter(\n",
    "    (F.col('CDR_DAY') == '2025-01-01') & (F.col('call_hour').between(0, 0))\n",
    ").select('PRI_IDENTITY_HASH').distinct()\n",
    "\n",
    "midnight_users = nye_active.intersect(ny_active).count()\n",
    "print(f\"\\nüéä Users active at midnight transition: {midnight_users:,}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 5: Revenue Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüí∞ REVENUE ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Overall revenue metrics\n",
    "revenue_stats = df.agg(\n",
    "    F.sum('DEBIT_AMOUNT').alias('total_revenue'),\n",
    "    F.avg('DEBIT_AMOUNT').alias('avg_revenue_per_call'),\n",
    "    F.sum(F.when(F.col('DEBIT_AMOUNT') > 0, 1).otherwise(0)).alias('paid_calls'),\n",
    "    F.sum(F.when(F.col('DEBIT_AMOUNT') == 0, 1).otherwise(0)).alias('free_calls')\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nüíµ Total Revenue: {revenue_stats['total_revenue']:,.2f} DZD\")\n",
    "print(f\"üìä Average Revenue per Call: {revenue_stats['avg_revenue_per_call']:.2f} DZD\")\n",
    "print(f\"üí≥ Paid Calls: {revenue_stats['paid_calls']:,} ({revenue_stats['paid_calls']/df.count()*100:.2f}%)\")\n",
    "print(f\"üÜì Free Calls: {revenue_stats['free_calls']:,} ({revenue_stats['free_calls']/df.count()*100:.2f}%)\")\n",
    "\n",
    "# Revenue by day\n",
    "print(\"\\nüìÖ Daily Revenue Comparison:\")\n",
    "daily_revenue = df.groupBy('CDR_DAY').agg(\n",
    "    F.sum('DEBIT_AMOUNT').alias('total_revenue'),\n",
    "    F.count('*').alias('total_calls'),\n",
    "    F.avg('DEBIT_AMOUNT').alias('avg_revenue_per_call'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users')\n",
    ").withColumn(\n",
    "    'ARPU', F.round(F.col('total_revenue') / F.col('unique_users'), 2)\n",
    ").orderBy('CDR_DAY')\n",
    "daily_revenue.show()\n",
    "\n",
    "# Revenue distribution\n",
    "print(\"\\nüí∞ Revenue Distribution Analysis:\")\n",
    "revenue_percentiles = df.filter(F.col('DEBIT_AMOUNT') > 0).select(\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT, 0.25)\").alias(\"Q1\"),\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT, 0.50)\").alias(\"Median\"),\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT, 0.75)\").alias(\"Q3\"),\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT, 0.95)\").alias(\"P95\"),\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT, 0.99)\").alias(\"P99\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"  Q1 (25th percentile): {revenue_percentiles['Q1']:.2f} DZD\")\n",
    "print(f\"  Median: {revenue_percentiles['Median']:.2f} DZD\")\n",
    "print(f\"  Q3 (75th percentile): {revenue_percentiles['Q3']:.2f} DZD\")\n",
    "print(f\"  95th percentile: {revenue_percentiles['P95']:.2f} DZD\")\n",
    "print(f\"  99th percentile: {revenue_percentiles['P99']:.2f} DZD\")\n",
    "\n",
    "# Hourly revenue pattern\n",
    "print(\"\\n‚è∞ Hourly Revenue Pattern:\")\n",
    "hourly_revenue = df_temporal.groupBy('CDR_DAY', 'call_hour').agg(\n",
    "    F.sum('DEBIT_AMOUNT').alias('hourly_revenue'),\n",
    "    F.count('*').alias('calls')\n",
    ").orderBy('CDR_DAY', 'call_hour')\n",
    "\n",
    "# Find peak revenue hours\n",
    "peak_revenue_hours = hourly_revenue.orderBy(F.desc('hourly_revenue')).limit(5)\n",
    "print(\"\\nüîù Top 5 Revenue Hours:\")\n",
    "peak_revenue_hours.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 6: Network Cell Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüì° NETWORK CELL ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Cell activity distribution\n",
    "cell_stats = df.groupBy('CallingCellID').agg(\n",
    "    F.count('*').alias('total_calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed_calls'),\n",
    "    F.avg('ACTUAL_USAGE').alias('avg_duration'),\n",
    "    F.sum('DEBIT_AMOUNT').alias('total_revenue')\n",
    ").withColumn(\n",
    "    'failure_rate', F.round(F.col('failed_calls') / F.col('total_calls') * 100, 2)\n",
    ").withColumn(\n",
    "    'calls_per_user', F.round(F.col('total_calls') / F.col('unique_users'), 2)\n",
    ")\n",
    "\n",
    "print(\"\\nüìä Cell Distribution Summary:\")\n",
    "print(f\"Total Active Cells: {cell_stats.count()}\")\n",
    "print(f\"Cells with >1000 calls: {cell_stats.filter(F.col('total_calls') > 1000).count()}\")\n",
    "print(f\"Cells with >50% failure rate: {cell_stats.filter(F.col('failure_rate') > 50).count()}\")\n",
    "\n",
    "# Top performing cells\n",
    "print(\"\\nüèÜ Top 10 Busiest Cells:\")\n",
    "top_cells = cell_stats.orderBy(F.desc('total_calls')).limit(10)\n",
    "top_cells.show()\n",
    "\n",
    "# Cells with issues\n",
    "print(\"\\n‚ö†Ô∏è Cells with High Failure Rates (>30%):\")\n",
    "problem_cells = cell_stats.filter(F.col('failure_rate') > 30).orderBy(F.desc('failure_rate'))\n",
    "problem_cells.select('CallingCellID', 'total_calls', 'failure_rate', 'unique_users').show(10)\n",
    "\n",
    "# New Year's Eve cell load\n",
    "nye_cell_load = df_temporal.filter(\n",
    "    ((F.col('CDR_DAY') == '2024-12-31') & (F.col('call_hour') >= 22)) |\n",
    "    ((F.col('CDR_DAY') == '2025-01-01') & (F.col('call_hour') <= 2))\n",
    ").groupBy('CallingCellID').agg(\n",
    "    F.count('*').alias('nye_calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('nye_users')\n",
    ")\n",
    "\n",
    "print(\"\\nüéä Cells with Highest New Year's Eve Activity:\")\n",
    "nye_cell_load.orderBy(F.desc('nye_calls')).show(10)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 7: Call Type and Service Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüì± CALL TYPE AND SERVICE ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Call type distribution\n",
    "print(\"\\nüìû Call Type Distribution:\")\n",
    "call_type_stats = df.groupBy('CallType').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    F.round(F.avg('ACTUAL_USAGE'), 2).alias('avg_duration'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed_calls'),\n",
    "    F.round(F.sum('DEBIT_AMOUNT'), 2).alias('total_revenue')\n",
    ").withColumn(\n",
    "    'failure_rate', F.round(F.col('failed_calls') / F.col('count') * 100, 2)\n",
    ").orderBy(F.desc('count'))\n",
    "call_type_stats.show()\n",
    "\n",
    "# Service flow analysis\n",
    "print(\"\\nüîÑ Service Flow Distribution:\")\n",
    "service_flow_stats = df.groupBy('ServiceFlow').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users'),\n",
    "    F.round(F.avg('ACTUAL_USAGE'), 2).alias('avg_duration')\n",
    ").orderBy(F.desc('count'))\n",
    "service_flow_stats.show()\n",
    "\n",
    "# Roaming analysis\n",
    "print(\"\\nüåç Roaming Analysis:\")\n",
    "roaming_stats = df.groupBy('RoamState').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    F.round(F.avg('ACTUAL_USAGE'), 2).alias('avg_duration'),\n",
    "    F.round(F.sum('DEBIT_AMOUNT'), 2).alias('total_revenue')\n",
    ").orderBy('RoamState')\n",
    "roaming_stats.show()\n",
    "\n",
    "# Call forwarding patterns\n",
    "print(\"\\n‚Ü™Ô∏è Call Forwarding Patterns:\")\n",
    "forward_stats = df.groupBy('CallForwardIndicator').agg(\n",
    "    F.count('*').alias('count'),\n",
    "    F.round(F.count('*') / df.count() * 100, 2).alias('percentage')\n",
    ").orderBy('CallForwardIndicator')\n",
    "forward_stats.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 8: New Year's Eve Special Insights\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüéÜ NEW YEAR'S EVE SPECIAL INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Compare Dec 31 vs Jan 1\n",
    "print(\"\\nüìä Key Metrics Comparison:\")\n",
    "comparison = df.groupBy('CDR_DAY').agg(\n",
    "    F.count('*').alias('total_calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users'),\n",
    "    F.round(F.avg('ACTUAL_USAGE'), 2).alias('avg_duration'),\n",
    "    F.round(F.sum('DEBIT_AMOUNT'), 2).alias('total_revenue'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed_calls')\n",
    ").withColumn(\n",
    "    'calls_per_user', F.round(F.col('total_calls') / F.col('unique_users'), 2)\n",
    ").withColumn(\n",
    "    'failure_rate', F.round(F.col('failed_calls') / F.col('total_calls') * 100, 2)\n",
    ")\n",
    "comparison.show()\n",
    "\n",
    "# Calculate growth rates\n",
    "dec31_stats = comparison.filter(F.col('CDR_DAY') == '2024-12-31').collect()[0]\n",
    "jan01_stats = comparison.filter(F.col('CDR_DAY') == '2025-01-01').collect()[0]\n",
    "\n",
    "print(\"\\nüìà New Year's Day Growth Rates:\")\n",
    "print(f\"  Call Volume: +{((jan01_stats['total_calls'] - dec31_stats['total_calls']) / dec31_stats['total_calls'] * 100):.1f}%\")\n",
    "print(f\"  Active Users: +{((jan01_stats['unique_users'] - dec31_stats['unique_users']) / dec31_stats['unique_users'] * 100):.1f}%\")\n",
    "print(f\"  Revenue: +{((jan01_stats['total_revenue'] - dec31_stats['total_revenue']) / dec31_stats['total_revenue'] * 100):.1f}%\")\n",
    "\n",
    "# Midnight hour analysis\n",
    "print(\"\\nüïê Midnight Hour Deep Dive (23:00-00:59):\")\n",
    "midnight_deep = df_temporal.filter(\n",
    "    ((F.col('CDR_DAY') == '2024-12-31') & (F.col('call_hour') == 23)) |\n",
    "    ((F.col('CDR_DAY') == '2025-01-01') & (F.col('call_hour') == 0))\n",
    ").groupBy('CDR_DAY', 'call_hour').agg(\n",
    "    F.count('*').alias('calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_callers'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') <= 30, 1).otherwise(0)).alias('short_calls'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed_calls'),\n",
    "    F.round(F.avg(F.when(F.col('ACTUAL_USAGE') > 0, F.col('ACTUAL_USAGE'))), 2).alias('avg_success_duration')\n",
    ").withColumn(\n",
    "    'short_call_ratio', F.round(F.col('short_calls') / F.col('calls') * 100, 2)\n",
    ")\n",
    "midnight_deep.show()\n",
    "\n",
    "# Service types during celebration\n",
    "print(\"\\nüéâ Service Usage During Celebration Hours (22:00-02:00):\")\n",
    "celebration_services = df_temporal.filter(\n",
    "    ((F.col('CDR_DAY') == '2024-12-31') & (F.col('call_hour') >= 22)) |\n",
    "    ((F.col('CDR_DAY') == '2025-01-01') & (F.col('call_hour') <= 2))\n",
    ").groupBy('SERVICE_CATEGORY').agg(\n",
    "    F.count('*').alias('celebration_calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users')\n",
    ").orderBy(F.desc('celebration_calls'))\n",
    "celebration_services.show()\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 9: Key Findings and Business Insights\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüîç KEY FINDINGS AND BUSINESS INSIGHTS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Traffic Surge Analysis\n",
    "jan1_calls = df.filter(F.col('CDR_DAY') == '2025-01-01').count()\n",
    "dec31_calls = df.filter(F.col('CDR_DAY') == '2024-12-31').count()\n",
    "surge_factor = jan1_calls / dec31_calls\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ NEW YEAR'S TRAFFIC SURGE:\")\n",
    "print(f\"   ‚Ä¢ Dec 31: {dec31_calls:,} calls\")\n",
    "print(f\"   ‚Ä¢ Jan 1: {jan1_calls:,} calls\")\n",
    "print(f\"   ‚Ä¢ Surge Factor: {surge_factor:.1f}x\")\n",
    "print(f\"   ‚Ä¢ This represents a {(surge_factor-1)*100:.0f}% increase in traffic\")\n",
    "\n",
    "# 2. Network Performance Under Load\n",
    "nye_failure = df.filter(F.col('CDR_DAY') == '2024-12-31').agg(\n",
    "    F.avg(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0))\n",
    ").collect()[0][0] * 100\n",
    "\n",
    "ny_failure = df.filter(F.col('CDR_DAY') == '2025-01-01').agg(\n",
    "    F.avg(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0))\n",
    ").collect()[0][0] * 100\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ NETWORK RESILIENCE:\")\n",
    "print(f\"   ‚Ä¢ Dec 31 Failure Rate: {nye_failure:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Jan 1 Failure Rate: {ny_failure:.2f}%\")\n",
    "print(f\"   ‚Ä¢ Despite 11.5x traffic surge, failure rate only increased by {ny_failure-nye_failure:.2f} percentage points\")\n",
    "\n",
    "# 3. User Behavior Patterns\n",
    "midnight_spike = df_temporal.filter(\n",
    "    (F.col('CDR_DAY') == '2025-01-01') & (F.col('call_hour') == 0)\n",
    ").count()\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ MIDNIGHT CELEBRATION PATTERN:\")\n",
    "print(f\"   ‚Ä¢ Calls in first hour of 2025: {midnight_spike:,}\")\n",
    "print(f\"   ‚Ä¢ This represents {midnight_spike/jan1_calls*100:.1f}% of all New Year's Day calls\")\n",
    "print(f\"   ‚Ä¢ Clear spike indicating mass New Year greetings\")\n",
    "\n",
    "# 4. Revenue Impact\n",
    "revenue_increase = ((jan01_stats['total_revenue'] - dec31_stats['total_revenue']) / dec31_stats['total_revenue'] * 100)\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ REVENUE OPPORTUNITY:\")\n",
    "print(f\"   ‚Ä¢ Revenue increased by {revenue_increase:.1f}% on New Year's Day\")\n",
    "print(f\"   ‚Ä¢ ARPU on Dec 31: {dec31_stats['total_revenue']/dec31_stats['unique_users']:.2f} DZD\")\n",
    "print(f\"   ‚Ä¢ ARPU on Jan 1: {jan01_stats['total_revenue']/jan01_stats['unique_users']:.2f} DZD\")\n",
    "\n",
    "# 5. Service Category Insights\n",
    "voice_calls = df.filter(F.col('SERVICE_CATEGORY') == '1').count()\n",
    "sms_messages = df.filter(F.col('SERVICE_CATEGORY') == '2').count()\n",
    "\n",
    "print(\"\\n5Ô∏è‚É£ SERVICE PREFERENCES:\")\n",
    "print(f\"   ‚Ä¢ Voice Calls: {voice_calls:,} ({voice_calls/df.count()*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ SMS Messages: {sms_messages:,} ({sms_messages/df.count()*100:.1f}%)\")\n",
    "print(f\"   ‚Ä¢ Voice remains dominant for New Year greetings\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"üí° BUSINESS RECOMMENDATIONS:\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. üì° Network Capacity: Plan for 12x normal capacity for future New Year events\")\n",
    "print(\"2. üí∞ Revenue Optimization: Create special New Year packages to monetize surge\")\n",
    "print(\"3. üéØ Marketing: Target heavy users (50+ calls) with premium offerings\")\n",
    "print(\"4. üîß Infrastructure: Focus on cells with >30% failure rates before next holiday\")\n",
    "print(\"5. üì± Service Innovation: Consider special New Year SMS bundles\")\n",
    "\n",
    "print(f\"\\n‚úÖ EDA Complete! Analysis finished at: {datetime.now()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 10: Export Key Metrics for Visualization\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nüìä EXPORTING KEY METRICS FOR VISUALIZATION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create summary tables for BI tools\n",
    "\n",
    "# 1. Hourly metrics for time series visualization\n",
    "hourly_export = df_temporal.groupBy('CDR_DAY', 'call_hour').agg(\n",
    "    F.count('*').alias('calls'),\n",
    "    F.countDistinct('PRI_IDENTITY_HASH').alias('unique_users'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') > 0, 1).otherwise(0)).alias('successful_calls'),\n",
    "    F.sum(F.when(F.col('ACTUAL_USAGE') == 0, 1).otherwise(0)).alias('failed_calls'),\n",
    "    F.round(F.avg('ACTUAL_USAGE'), 2).alias('avg_duration'),\n",
    "    F.round(F.sum('DEBIT_AMOUNT'), 2).alias('revenue')\n",
    ").withColumn('hour_timestamp', \n",
    "    F.concat(F.col('CDR_DAY'), F.lit(' '), \n",
    "             F.lpad(F.col('call_hour'), 2, '0'), F.lit(':00:00'))\n",
    ").orderBy('CDR_DAY', 'call_hour')\n",
    "\n",
    "hourly_export.write.mode('overwrite').saveAsTable('eda_hourly_metrics')\n",
    "print(\"‚úÖ Saved: eda_hourly_metrics\")\n",
    "\n",
    "# 2. User segments for pie charts\n",
    "user_segment_export = user_segments.groupBy('user_segment').agg(\n",
    "    F.count('*').alias('user_count'),\n",
    "    F.sum('total_calls').alias('total_calls'),\n",
    "    F.sum('total_spend').alias('total_revenue')\n",
    ").withColumn('pct_users', F.round(F.col('user_count') / user_segments.count() * 100, 2))\n",
    "\n",
    "user_segment_export.write.mode('overwrite').saveAsTable('eda_user_segments')\n",
    "print(\"‚úÖ Saved: eda_user_segments\")\n",
    "\n",
    "# 3. Cell performance for geographic visualization\n",
    "cell_export = cell_stats.select(\n",
    "    'CallingCellID',\n",
    "    'total_calls',\n",
    "    'unique_users',\n",
    "    'failure_rate',\n",
    "    F.round('total_revenue', 2).alias('revenue')\n",
    ").filter(F.col('CallingCellID').isNotNull())\n",
    "\n",
    "cell_export.write.mode('overwrite').saveAsTable('eda_cell_performance')\n",
    "print(\"‚úÖ Saved: eda_cell_performance\")\n",
    "\n",
    "print(\"\\nüìà Ready for visualization in Superset/PowerBI!\")\n",
    "print(\"   - Use eda_hourly_metrics for time series charts\")\n",
    "print(\"   - Use eda_user_segments for user distribution\")\n",
    "print(\"   - Use eda_cell_performance for network maps\")\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n",
    "print(\"\\n‚úÖ Analysis complete! Spark session closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8afb11-2eed-47e8-bd19-bd5973f2451a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
