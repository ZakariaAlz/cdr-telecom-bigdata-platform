{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "318c87a7-04f5-4e85-931a-cdddf48c1720",
   "metadata": {},
   "source": [
    "- 06_Anomaly_Detection_Engineering.ipynb\n",
    "- Algerie Telecom - Anomaly Detection Engineering\n",
    "- Author: Data Engineering Team\n",
    "- Date: July 2025\n",
    "\n",
    "\n",
    "üö® Anomaly Detection Engineering\n",
    "\n",
    "Implement statistical, pattern-based, and time-series anomaly detection for CDR data.\n",
    "\n",
    "**Objectives**\n",
    "- Develop statistical anomaly detection methods\n",
    "- Implement pattern-based and isolation forest algorithms\n",
    "- Build time-series detectors using moving windows\n",
    "- Apply real-time scoring and alerting\n",
    "- Validate detection accuracy and visualize anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eb9b4cf3-81ff-4167-97f0-7974247efa48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: Anomaly Detection- AT CDR, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "‚úÖ SparkSession initialized\n",
      "Spark Version: 3.5.1\n",
      "Warehouse Location: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Environment ready for feature engineering\n",
      "üìä Total CDR records: 768,359,379\n",
      "+--------+---------+-------+\n",
      "|col_name|data_type|comment|\n",
      "+--------+---------+-------+\n",
      "|  cdr_id|   string|   NULL|\n",
      "+--------+---------+-------+\n",
      "only showing top 1 row\n",
      "\n",
      "‚úÖ ‚úî fact_cdr_raw table found!\n",
      "‚úÖ Environment ready.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Standard imports and setup\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/batch/jupyter/notebooks/work/scripts')\n",
    "from spark_init import init_spark\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime, timedelta\n",
    "import json\n",
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "\n",
    "# Initialize Spark\n",
    "spark = init_spark(\"Anomaly Detection- AT CDR\")\n",
    "print(\"‚úÖ SparkSession initialized\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Warehouse Location: {spark.conf.get('spark.sql.warehouse.dir')}\")\n",
    "\n",
    "# Use the database\n",
    "spark.sql(\"USE at_cdr_analysis\")\n",
    "\n",
    "# Set visualization style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Create temp views\n",
    "cdr_raw = spark.read.parquet(\"/user/hive/warehouse/Raw/raw_cdr_enhanced/\")\n",
    "cdr_raw.createOrReplaceTempView(\"fact_cdr_raw\")\n",
    "\n",
    "customers = spark.read.parquet(\"/user/hive/warehouse/Raw/customer_dim_enhanced/\")\n",
    "customers.createOrReplaceTempView(\"dim_customers\")\n",
    "\n",
    "print(\"‚úÖ Environment ready for feature engineering\")\n",
    "print(f\"üìä Total CDR records: {cdr_raw.count():,}\")\n",
    "\n",
    "try:\n",
    "    spark.sql(\"DESCRIBE fact_cdr_raw\").show(1)\n",
    "    print(\"‚úÖ ‚úî fact_cdr_raw table found!\")\n",
    "except AnalysisException:\n",
    "    raise RuntimeError(\"‚ùå fact_cdr_raw table not found! Please ingest CDR raw data first.\")\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"‚úÖ Environment ready.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a822cd1-80fb-433b-aff5-2c3719e6b461",
   "metadata": {},
   "source": [
    "## 2. Build Statistical Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e76da2d0-2e30-4a4a-8906-b201f7b333db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Computing baseline statistics...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 05:02:53 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Baseline ready for 519912 customers.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ñ∂ Computing baseline statistics...\")\n",
    "baseline_stats = spark.sql(\"\"\"\n",
    "WITH daily_stats AS (\n",
    "  SELECT\n",
    "    customer_id,\n",
    "    DATE(timestamp) AS dt,\n",
    "    COUNT(*) FILTER(WHERE cdr_type='DATA')       AS sessions,\n",
    "    SUM(data_volume_mb)                           AS total_mb,\n",
    "    MAX(data_volume_mb)                           AS max_mb,\n",
    "    COUNT(DISTINCT HOUR(timestamp))               AS hours\n",
    "  FROM fact_cdr_raw\n",
    "  WHERE cdr_type='DATA'\n",
    "    AND timestamp < date_sub(current_date, 7)\n",
    "  GROUP BY customer_id, DATE(timestamp)\n",
    ")\n",
    "SELECT\n",
    "  customer_id,\n",
    "  COUNT(*)                       AS days,\n",
    "  AVG(sessions)                  AS mu_sess,\n",
    "  STDDEV(sessions)               AS sigma_sess,\n",
    "  AVG(total_mb)                  AS mu_mb,\n",
    "  STDDEV(total_mb)               AS sigma_mb,\n",
    "  AVG(max_mb)                    AS mu_max,\n",
    "  STDDEV(max_mb)                 AS sigma_max,\n",
    "  AVG(hours)                     AS mu_hrs,\n",
    "  STDDEV(hours)                  AS sigma_hrs\n",
    "FROM daily_stats\n",
    "GROUP BY customer_id\n",
    "HAVING COUNT(*) >= 14\n",
    "\"\"\")\n",
    "\n",
    "cnt = baseline_stats.count()\n",
    "if cnt == 0:\n",
    "    raise RuntimeError(\"‚ùå No baseline statistics computed ‚Äî check your historical data.\")\n",
    "baseline_stats.write.mode(\"overwrite\").saveAsTable(\"anomaly_baseline\")\n",
    "print(f\"‚úÖ Baseline ready for {cnt} customers.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62f86a6-566b-49c7-87f5-2d96d391b779",
   "metadata": {},
   "source": [
    "## 3. Z-Score Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cdf05960-982c-4485-a3ac-fe00835aa0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Gathering recent 7-day activity...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Computing Z-scores & flagging anomalies...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 65:=================================================>      (21 + 3) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Z-score anomalies written (110357 flagged)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"‚ñ∂ Gathering recent 7-day activity...\")\n",
    "recent = spark.sql(\"\"\"\n",
    "WITH recent AS (\n",
    "    SELECT\n",
    "      customer_id,\n",
    "      DATE(timestamp) AS dt,\n",
    "      COUNT(*) FILTER(WHERE cdr_type='DATA')   AS sessions,\n",
    "      SUM(data_volume_mb)                     AS total_mb,\n",
    "      MAX(data_volume_mb)                     AS max_mb\n",
    "    FROM fact_cdr_raw\n",
    "    WHERE cdr_type='DATA'\n",
    "      AND DATE(timestamp) >= date_sub(current_date, 7)\n",
    "    GROUP BY customer_id, DATE(timestamp)\n",
    ")\n",
    "SELECT r.*, b.mu_sess, b.sigma_sess, b.mu_mb, b.sigma_mb, b.mu_max, b.sigma_max\n",
    "FROM recent r\n",
    "JOIN anomaly_baseline b\n",
    "  ON r.customer_id = b.customer_id\n",
    "\"\"\")\n",
    "\n",
    "if recent.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è  No recent DATA activity‚Äîskipping Z-score detection.\\n\")\n",
    "else:\n",
    "    # bring in the missing functions\n",
    "    from pyspark.sql.functions import when, col, lit, udf\n",
    "    from pyspark.sql.types import DoubleType\n",
    "\n",
    "    # UDF for Z-score\n",
    "    def zscore(v, m, s):\n",
    "        return float(abs(v - m) / s) if s not in (0, None) else 0.0\n",
    "    z_udf = udf(zscore, DoubleType())\n",
    "\n",
    "    print(\"‚ñ∂ Computing Z-scores & flagging anomalies...\")\n",
    "    zres = recent \\\n",
    "      .withColumn(\"z_sess\", z_udf(\"sessions\",\"mu_sess\",\"sigma_sess\")) \\\n",
    "      .withColumn(\"z_mb\",   z_udf(\"total_mb\",\"mu_mb\",\"sigma_mb\"))     \\\n",
    "      .withColumn(\"z_max\",  z_udf(\"max_mb\",\"mu_max\",\"sigma_max\"))      \\\n",
    "      .withColumn(\"is_anom\",\n",
    "          when(\n",
    "            (col(\"z_sess\") > 3) |\n",
    "            (col(\"z_mb\")   > 3) |\n",
    "            (col(\"z_max\")  > 3),\n",
    "            lit(1)\n",
    "          )\n",
    "          .otherwise(lit(0))\n",
    "      )\n",
    "\n",
    "    zres.write.mode(\"overwrite\").partitionBy(\"dt\").saveAsTable(\"anomaly_zscore\")\n",
    "    flagged = zres.filter(col(\"is_anom\") == 1).count()\n",
    "    print(f\"‚úÖ Z-score anomalies written ({flagged} flagged)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dfb7b-2066-4d9c-9c62-8bc4bf58022a",
   "metadata": {},
   "source": [
    "## 4. Isolation Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a5222760-1226-492f-a314-4406a934b03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Preparing features for Isolation Forest...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fitting IsolationForest on sample...\n",
      "‚úÖ IsolationForest flagged 500 of 10000 samples.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ñ∂ Preparing features for Isolation Forest...\")\n",
    "\n",
    "features_df = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  customer_id,\n",
    "  DATE(timestamp) AS dt,\n",
    "  COUNT(*) FILTER(WHERE cdr_type='DATA')   AS sessions,\n",
    "  SUM(data_volume_mb)                     AS total_mb,\n",
    "  MAX(data_volume_mb)                     AS max_mb\n",
    "FROM fact_cdr_raw\n",
    "WHERE cdr_type='DATA'\n",
    "  AND DATE(timestamp) >= date_sub(current_date, 30)\n",
    "GROUP BY customer_id, DATE(timestamp)\n",
    "\"\"\")\n",
    "\n",
    "if features_df.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è  No data for Isolation Forest ‚Äì skipping.\\n\")\n",
    "else:\n",
    "    from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "    import numpy as np\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "\n",
    "    # Assemble & scale\n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=[\"sessions\",\"total_mb\",\"max_mb\"], outputCol=\"features\")\n",
    "    feat = assembler.transform(features_df)\n",
    "    scaler = StandardScaler(\n",
    "        inputCol=\"features\", outputCol=\"scaled\", withMean=True, withStd=True\n",
    "    )\n",
    "    feat = scaler.fit(feat).transform(feat)\n",
    "\n",
    "    # Sample 5% in Spark, then limit to 10k rows for .toPandas()\n",
    "    sample_spark = feat.sample(fraction=0.05, seed=42).select(\"customer_id\",\"dt\",\"scaled\")\n",
    "    sample_spark_ltd = sample_spark.limit(10000)       # <-- cap to avoid NPE\n",
    "    sample_pdf = sample_spark_ltd.toPandas()\n",
    "\n",
    "    X = np.vstack(sample_pdf[\"scaled\"].values)\n",
    "\n",
    "    print(\"‚ñ∂ Fitting IsolationForest on sample...\")\n",
    "    iso = IsolationForest(random_state=42, contamination=0.05)\n",
    "    sample_pdf[\"outlier\"] = iso.fit_predict(X)\n",
    "    sample_pdf[\"score\"]   = iso.score_samples(X)\n",
    "\n",
    "    n_anom = (sample_pdf[\"outlier\"] == -1).sum()\n",
    "    print(f\"‚úÖ IsolationForest flagged {n_anom} of {len(sample_pdf)} samples.\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5feac78f-d8b0-4716-976d-43f7e137de41",
   "metadata": {},
   "source": [
    "## 5. Time-Series Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2028515-e8fd-46f7-ba60-0e09b42af4cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Computing hourly aggregates for time-series analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 112:===================================================>   (45 + 3) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Time-series anomalies written (1700 flagged)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, when, lit\n",
    "\n",
    "print(\"‚ñ∂ Computing hourly aggregates for time-series analysis...\")\n",
    "ts_df = spark.sql(\"\"\"\n",
    "WITH hr AS (\n",
    "  SELECT\n",
    "    wilaya_code,\n",
    "    DATE(timestamp) AS dt,\n",
    "    HOUR(timestamp)    AS hr,\n",
    "    SUM(data_volume_mb)            AS mb_agg\n",
    "  FROM fact_cdr_raw\n",
    "  GROUP BY wilaya_code, DATE(timestamp), HOUR(timestamp)\n",
    ")\n",
    "SELECT \n",
    "  *,\n",
    "  AVG(mb_agg) OVER (\n",
    "    PARTITION BY wilaya_code, hr\n",
    "    ORDER BY dt\n",
    "    ROWS BETWEEN 27 PRECEDING AND 1 PRECEDING\n",
    "  ) AS mu_m,\n",
    "  STDDEV(mb_agg) OVER (\n",
    "    PARTITION BY wilaya_code, hr\n",
    "    ORDER BY dt\n",
    "    ROWS BETWEEN 27 PRECEDING AND 1 PRECEDING\n",
    "  ) AS sd_m\n",
    "FROM hr\n",
    "WHERE dt >= date_sub(current_date, 30)\n",
    "\"\"\")\n",
    "\n",
    "if ts_df.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è  No aggregates for time-series detection ‚Äì skipping.\\n\")\n",
    "else:\n",
    "    ts_anom = ts_df.withColumn(\"anom_type\",\n",
    "        when(col(\"mb_agg\") > col(\"mu_m\") + 3 * col(\"sd_m\"), lit(\"UPPER\"))\n",
    "       .when(col(\"mb_agg\") < col(\"mu_m\") - 3 * col(\"sd_m\"), lit(\"LOWER\"))\n",
    "       .otherwise(lit(\"NORMAL\"))\n",
    "    )\n",
    "    ts_anom.write.mode(\"overwrite\") \\\n",
    "         .partitionBy(\"wilaya_code\", \"hr\") \\\n",
    "         .saveAsTable(\"anomaly_timeseries\")\n",
    "\n",
    "    # Corrected f-string / .format() usage\n",
    "    count_flagged = ts_anom.filter(\"anom_type != 'NORMAL'\").count()\n",
    "    print(f\"‚úÖ Time-series anomalies written ({count_flagged} flagged)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68ae922-71eb-4644-9313-69bef231ca50",
   "metadata": {},
   "source": [
    "## 6. Pattern-Based Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a7ac7b3d-a702-472f-919c-ffd242d13dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Running pattern-based rules...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 137:==================================================>    (22 + 2) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pattern-anomalies written (4861974 flagged)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"‚ñ∂ Running pattern-based rules...\")\n",
    "pb = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  customer_id,\n",
    "  DATE(timestamp) AS dt,\n",
    "  SUM(CASE WHEN HOUR(timestamp) BETWEEN 2 AND 5  THEN 1 ELSE 0 END) AS nocturnal_sessions,\n",
    "  SUM(CASE WHEN HOUR(timestamp) BETWEEN 20 AND 23 THEN 1 ELSE 0 END) AS peak_sessions,\n",
    "  MAX(data_volume_mb)/NULLIF(AVG(data_volume_mb),0)     AS max_avg_ratio\n",
    "FROM fact_cdr_raw\n",
    "GROUP BY customer_id, DATE(timestamp)\n",
    "\"\"\")\n",
    "\n",
    "if pb.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è  No data for pattern-based detection ‚Äì skipping.\\n\")\n",
    "else:\n",
    "    pb_anom = pb.withColumn(\n",
    "        \"pattern_anom\",\n",
    "        when(col(\"nocturnal_sessions\") > 10, lit(\"NIGHT_OWLS\"))\n",
    "       .when(col(\"max_avg_ratio\") > 5,    lit(\"SPIKE_RATIO\"))\n",
    "       .otherwise(lit(\"NORMAL\"))\n",
    "    )\n",
    "    pb_anom.write.mode(\"overwrite\") \\\n",
    "          .partitionBy(\"dt\") \\\n",
    "          .saveAsTable(\"anomaly_pattern\")\n",
    "\n",
    "    flagged = pb_anom.filter(\"pattern_anom <> 'NORMAL'\").count()\n",
    "    print(f\"‚úÖ Pattern-anomalies written ({flagged} flagged)\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198a7f99-eec5-47e5-83b9-0042bb7d5c51",
   "metadata": {},
   "source": [
    "## 7. Alerting & Dashboard\n",
    "\n",
    "- Aggregate anomaly counts\n",
    "- Store alerts in Hive table\n",
    "- Build Spark Streaming job (omitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f1c63f07-3764-4a63-8142-308c68206d7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Consolidating anomaly flags...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üö® Total customers to alert: 861611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 163:================================================>      (21 + 3) / 24]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Alerts table updated.\n",
      "\n",
      "üéâ Anomaly Detection Engineering pipeline complete.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/08 05:56:15 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/07/08 05:56:15 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "print(\"‚ñ∂ Consolidating anomaly flags...\")\n",
    "consolidated = spark.sql(\"\"\"\n",
    "SELECT \n",
    "  z.customer_id, z.dt,\n",
    "  z.is_anom        AS zscore_flag,\n",
    "  CASE WHEN p.pattern_anom!='NORMAL' THEN 1 ELSE 0 END AS pattern_flag,\n",
    "  CASE WHEN t.anom_type!='NORMAL'  THEN 1 ELSE 0 END AS ts_flag\n",
    "FROM anomaly_zscore z\n",
    "LEFT JOIN anomaly_pattern p ON z.customer_id=p.customer_id AND z.dt=p.dt\n",
    "LEFT JOIN anomaly_timeseries t ON z.dt = t.dt AND t.wilaya_code='16' AND t.hr=20\n",
    "\"\"\")\n",
    "if consolidated.rdd.isEmpty():\n",
    "    print(\"‚ö†Ô∏è  No consolidated anomalies ‚Äì nothing to alert.\\n\")\n",
    "else:\n",
    "    alerts = consolidated.filter(\"zscore_flag+pattern_flag+ts_flag >= 1\")\n",
    "    print(f\"üö® Total customers to alert: {alerts.count()}\")\n",
    "    alerts.write.mode(\"overwrite\").saveAsTable(\"anomaly_alerts\")\n",
    "    print(\"‚úÖ Alerts table updated.\\n\")\n",
    "\n",
    "print(\"üéâ Anomaly Detection Engineering pipeline complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db62f8f-3b90-4551-a731-9791cf697118",
   "metadata": {},
   "source": [
    "## 8. Validation & Metrics\n",
    "\n",
    "- Precision/Recall against labeled incidents\n",
    "- AUC-ROC for iso forest\n",
    "- Performance testing: throughput and latency\n",
    "\n",
    "üöÄ End of Notebook 06"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b6ca11-4eef-4aad-8ae7-4f39880afef4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
