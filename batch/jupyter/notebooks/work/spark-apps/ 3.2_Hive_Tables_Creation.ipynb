{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37c506a3-fa4c-418f-8989-dac62c242155",
   "metadata": {},
   "source": [
    "- 03_Hive_Tables_Creation.ipynb\n",
    "- Algerie Telecom - Hive Tables Creation and Optimization\n",
    "- Author: Data Engineering Team\n",
    "- Date: July 2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7014acb5-4242-4d54-b007-3b4de6f3be64",
   "metadata": {},
   "source": [
    "# 🗄️ Hive Tables Creation and Optimization\n",
    "\n",
    "Create optimized Hive tables for efficient querying and analytics.\n",
    "\n",
    "## Objectives:\n",
    "- Create external tables for raw data\n",
    "- Implement partitioning strategy\n",
    "- Create optimized views for analytics\n",
    "- Set up aggregated tables for performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1de43fb9-4de7-434b-a8aa-c92bd788f9d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/07/07 19:57:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession initialized (App: Hive Tables Creation - Generated AT CDR, Spark: 3.5.1)\n",
      "✅ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "✅ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "✅ SparkSession initialized\n",
      "Spark Version: 3.5.1\n",
      "Warehouse Location: hdfs://namenode:9000/user/hive/warehouse\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work/batch/jupyter/notebooks/work/scripts')\n",
    "from spark_init import init_spark\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import hashlib\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Initialize Spark with proper configuration\n",
    "spark = init_spark(\"Hive Tables Creation - Generated AT CDR\")\n",
    "print(\"✅ SparkSession initialized\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Warehouse Location: {spark.conf.get('spark.sql.warehouse.dir')}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f9ff31-628c-4ad4-b19f-4f991fcb7399",
   "metadata": {},
   "source": [
    "## 2. Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b0d44306-7007-491a-b81f-1e0aef961812",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 19:57:32 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database created/selected\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS at_cdr_analysis\")\n",
    "spark.sql(\"USE at_cdr_analysis\")\n",
    "print(\"✅ Database created/selected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d2ed5f-9ed6-42af-97fe-65727b542c81",
   "metadata": {},
   "source": [
    "## 3. Create Customer Dimension Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2a81045e-7432-473b-a4a2-2d442d85a967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 16:41:04 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Customer dimension table created successfully\n",
      "+---------------------+---------+-------+\n",
      "|col_name             |data_type|comment|\n",
      "+---------------------+---------+-------+\n",
      "|customer_id          |string   |NULL   |\n",
      "|connection_id        |string   |NULL   |\n",
      "|wilaya_code          |string   |NULL   |\n",
      "|wilaya_name          |string   |NULL   |\n",
      "|customer_type        |string   |NULL   |\n",
      "|service_type         |string   |NULL   |\n",
      "|offer_name           |string   |NULL   |\n",
      "|offer_price          |double   |NULL   |\n",
      "|activation_date      |timestamp|NULL   |\n",
      "|is_active            |boolean  |NULL   |\n",
      "|tech_adoption_score  |double   |NULL   |\n",
      "|business_score       |double   |NULL   |\n",
      "|network_quality_score|double   |NULL   |\n",
      "|usage_profile        |string   |NULL   |\n",
      "|bandwidth_mbps       |double   |NULL   |\n",
      "|data_cap_gb          |double   |NULL   |\n",
      "|upload_mbps          |double   |NULL   |\n",
      "|sla_percentage       |double   |NULL   |\n",
      "|boost_hours          |string   |NULL   |\n",
      "|night_boost          |boolean  |NULL   |\n",
      "+---------------------+---------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop existing table if exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS dim_customers\")\n",
    "\n",
    "# Create external table for customers\n",
    "create_customer_table_sql = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS dim_customers (\n",
    "    customer_id STRING,\n",
    "    connection_id STRING,\n",
    "    wilaya_code STRING,\n",
    "    wilaya_name STRING,\n",
    "    customer_type STRING,\n",
    "    service_type STRING,\n",
    "    offer_name STRING,\n",
    "    offer_price DOUBLE,\n",
    "    activation_date TIMESTAMP,\n",
    "    is_active BOOLEAN,\n",
    "    tech_adoption_score DOUBLE,\n",
    "    business_score DOUBLE,\n",
    "    network_quality_score DOUBLE,\n",
    "    usage_profile STRING,\n",
    "    bandwidth_mbps DOUBLE,\n",
    "    data_cap_gb DOUBLE,\n",
    "    upload_mbps DOUBLE,\n",
    "    sla_percentage DOUBLE,\n",
    "    boost_hours STRING,\n",
    "    night_boost BOOLEAN\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '/user/hive/warehouse/Raw/customer_dim_enhanced/'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_customer_table_sql)\n",
    "print(\"✅ Customer dimension table created successfully\")\n",
    "\n",
    "# Verify table\n",
    "spark.sql(\"DESCRIBE FORMATTED dim_customers\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb75bf-29f7-4904-9138-8d747839a570",
   "metadata": {},
   "source": [
    "## 4. Create CDR Fact Table (Partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f699d619-a95e-4c20-bbee-ba1df30387f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 16:47:07 WARN HiveExternalCatalog: Hive incompatible types found: timestamp_ntz. Persisting data source table `spark_catalog`.`at_generated_cdr`.`fact_cdr` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created at_generated_cdr.fact_cdr – first few partitions:\n"
     ]
    },
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near 'LIMIT'.(line 1, pos 42)\n\n== SQL ==\nSHOW PARTITIONS at_generated_cdr.fact_cdr LIMIT 10\n------------------------------------------^^^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# 6) Quick sanity-check\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Created at_generated_cdr.fact_cdr – first few partitions:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW PARTITIONS at_generated_cdr.fact_cdr LIMIT 10\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     35\u001b[0m total \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mtable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mat_generated_cdr.fact_cdr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Total rows in fact_cdr = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near 'LIMIT'.(line 1, pos 42)\n\n== SQL ==\nSHOW PARTITIONS at_generated_cdr.fact_cdr LIMIT 10\n------------------------------------------^^^\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 16:49:14 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/07/07 16:49:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "spark.conf.set(\"spark.sql.parquet.int64AsTimestamp\", \"TIMESTAMP_NANOS\")\n",
    "\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "# 1) Read your enhanced raw CDRs (this will infer the schema from the underlying parquet)\n",
    "raw = spark.read.parquet(\"/user/hive/warehouse/Raw/raw_cdr_enhanced\")\n",
    "\n",
    "# 2) Add year/month/day partition columns\n",
    "fact = (\n",
    "    raw\n",
    "    .withColumn(\"yr\",  year(\"timestamp\"))\n",
    "    .withColumn(\"mo\",  month(\"timestamp\"))\n",
    "    .withColumn(\"day\", dayofmonth(\"timestamp\"))\n",
    ")\n",
    "\n",
    "# 3) Drop any existing managed table\n",
    "spark.sql(\"DROP TABLE IF EXISTS at_generated_cdr.fact_cdr\")\n",
    "\n",
    "# 4) Write out as a managed, partitioned parquet table\n",
    "(\n",
    "    fact.write\n",
    "        .mode(\"overwrite\")           # blow away old data & metadata\n",
    "        .format(\"parquet\")           # store as parquet\n",
    "        .partitionBy(\"yr\",\"mo\",\"day\")\n",
    "        .saveAsTable(\"at_generated_cdr.fact_cdr\")\n",
    ")\n",
    "\n",
    "# 5) Ensure Hive metastore sees all of your new daily partitions\n",
    "spark.sql(\"MSCK REPAIR TABLE at_generated_cdr.fact_cdr\")\n",
    "\n",
    "# 6) Quick sanity-check\n",
    "print(\"✅ Created at_generated_cdr.fact_cdr – first few partitions:\")\n",
    "spark.sql(\"SHOW PARTITIONS at_generated_cdr.fact_cdr LIMIT 10\").show(truncate=False)\n",
    "\n",
    "total = spark.table(\"at_generated_cdr.fact_cdr\").count()\n",
    "print(f\"✅ Total rows in fact_cdr = {total:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5b156d75-dc46-4473-abe7-b24c718535da",
   "metadata": {},
   "outputs": [
    {
     "ename": "ParseException",
     "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near ')'.(line 5, pos 2)\n\n== SQL ==\n\n  CREATE EXTERNAL TABLE IF NOT EXISTS Raw.raw_cdr_enhanced (\n    -- (you could list the full schema here, \n    -- but since it's parquet you can let Spark infer it)\n  )\n--^^^\n  STORED AS PARQUET\n  LOCATION '/user/hive/warehouse/Raw/raw_cdr_enhanced'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParseException\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 8\u001b[0m\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124m  CREATE DATABASE IF NOT EXISTS Raw\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;124m  LOCATION \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/user/hive/warehouse/Raw\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 2) Now create an EXTERNAL table over the parquet files in that directory\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43m  CREATE EXTERNAL TABLE IF NOT EXISTS Raw.raw_cdr_enhanced (\u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m    -- (you could list the full schema here, \u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m    -- but since it\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms parquet you can let Spark infer it)\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m  )\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m  STORED AS PARQUET\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m  LOCATION \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/user/hive/warehouse/Raw/raw_cdr_enhanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mParseException\u001b[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near ')'.(line 5, pos 2)\n\n== SQL ==\n\n  CREATE EXTERNAL TABLE IF NOT EXISTS Raw.raw_cdr_enhanced (\n    -- (you could list the full schema here, \n    -- but since it's parquet you can let Spark infer it)\n  )\n--^^^\n  STORED AS PARQUET\n  LOCATION '/user/hive/warehouse/Raw/raw_cdr_enhanced'\n"
     ]
    }
   ],
   "source": [
    "# 1) Create the Hive database (if not already there)\n",
    "spark.sql(\"\"\"\n",
    "  CREATE DATABASE IF NOT EXISTS Raw\n",
    "  LOCATION '/user/hive/warehouse/Raw'\n",
    "\"\"\")\n",
    "\n",
    "# 2) Now create an EXTERNAL table over the parquet files in that directory\n",
    "spark.sql(\"\"\"\n",
    "  CREATE EXTERNAL TABLE IF NOT EXISTS Raw.raw_cdr_enhanced (\n",
    "    -- (you could list the full schema here, \n",
    "    -- but since it's parquet you can let Spark infer it)\n",
    "  )\n",
    "  STORED AS PARQUET\n",
    "  LOCATION '/user/hive/warehouse/Raw/raw_cdr_enhanced'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2baa0324-621c-4d70-82fe-e394a47dee03",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[SCHEMA_NOT_FOUND] The schema `raw` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSHOW TABLES IN raw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshow(truncate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [SCHEMA_NOT_FOUND] The schema `raw` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a catalog, verify the current_schema() output, or qualify the name with the correct catalog.\nTo tolerate the error on drop use DROP SCHEMA IF EXISTS."
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN raw\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d17dde8-5df4-42a3-9c3d-21a236ea0e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|namespace          |\n",
      "+-------------------+\n",
      "|algerie_telecom_cdr|\n",
      "|algerie_telecom_gen|\n",
      "|at_generated_cdr   |\n",
      "|default            |\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW DATABASES\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56753c34-1e7d-4d80-994d-6d7cc19395c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `Raw`.`raw_cdr_enhanced` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [Raw, raw_cdr_enhanced], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m year, month, dayofmonth\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1) Read the raw data and add your partition columns\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m (\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRaw.raw_cdr_enhanced\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m            \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myear\u001b[39m\u001b[38;5;124m\"\u001b[39m,  year(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m            \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmonth\u001b[39m\u001b[38;5;124m\"\u001b[39m, month(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      7\u001b[0m            \u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mday\u001b[39m\u001b[38;5;124m\"\u001b[39m,   dayofmonth(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m)))\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# 2) Drop any old version of the managed table\u001b[39;00m\n\u001b[1;32m     10\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE IF EXISTS fact_cdr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1667\u001b[0m, in \u001b[0;36mSparkSession.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m \n\u001b[1;32m   1639\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `Raw`.`raw_cdr_enhanced` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [Raw, raw_cdr_enhanced], [], false\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "\n",
    "# 1) Read the raw data and add your partition columns\n",
    "df = (spark.table(\"Raw.raw_cdr_enhanced\")\n",
    "           .withColumn(\"year\",  year(\"timestamp\"))\n",
    "           .withColumn(\"month\", month(\"timestamp\"))\n",
    "           .withColumn(\"day\",   dayofmonth(\"timestamp\")))\n",
    "\n",
    "# 2) Drop any old version of the managed table\n",
    "spark.sql(\"DROP TABLE IF EXISTS fact_cdr\")\n",
    "\n",
    "# 3) Write out as a new MANAGED partitioned table in one shot\n",
    "(df.write\n",
    "   .mode(\"overwrite\")                # overwrite any old data & metadata\n",
    "   .partitionBy(\"year\",\"month\",\"day\")# partition columns\n",
    "   .format(\"parquet\")\n",
    "   .saveAsTable(\"fact_cdr\")          # <=== managed table in your Hive warehouse\n",
    ")\n",
    "\n",
    "# 4) Quick sanity-check: show a few partitions\n",
    "print(\"👉 Partitions:\")\n",
    "spark.sql(\"SHOW PARTITIONS fact_cdr LIMIT 10\").show(truncate=False)\n",
    "\n",
    "# 5) Count total rows\n",
    "cnt = spark.table(\"fact_cdr\").count()\n",
    "print(f\"✅ Loaded fact_cdr – total rows = {cnt:,}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "67c7c941-b151-48df-810d-bb1784f7a3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/06 15:10:10 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "[Stage 1:=================================================>       (42 + 6) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date Range: 2025-03-20 to 2025-07-19 (121 days)\n",
      "✅ CDR fact table created with daily partitions\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# First, let's analyze the data to determine optimal partitioning\n",
    "sample_df = spark.read.parquet(\"/user/hive/warehouse/Raw/raw_cdr_enhanced/\")\n",
    "sample_df.createOrReplaceTempView(\"sample_cdr\")\n",
    "\n",
    "# Check date range for partitioning\n",
    "date_range = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE(MIN(timestamp)) as min_date,\n",
    "        DATE(MAX(timestamp)) as max_date,\n",
    "        DATEDIFF(DATE(MAX(timestamp)), DATE(MIN(timestamp))) as total_days\n",
    "    FROM sample_cdr\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"Date Range: {date_range['min_date']} to {date_range['max_date']} ({date_range['total_days']} days)\")\n",
    "\n",
    "# Create partitioned CDR table\n",
    "spark.sql(\"DROP TABLE IF EXISTS fact_cdr\")\n",
    "\n",
    "create_cdr_table_sql = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS fact_cdr (\n",
    "    cdr_id STRING,\n",
    "    timestamp TIMESTAMP,\n",
    "    customer_id STRING,\n",
    "    connection_id STRING,\n",
    "    wilaya_code STRING,\n",
    "    wilaya_name STRING,\n",
    "    cdr_type STRING,\n",
    "    service_type STRING,\n",
    "    data_volume_mb DOUBLE,\n",
    "    duration_minutes DOUBLE,\n",
    "    session_quality STRING,\n",
    "    usage_type STRING,\n",
    "    offer_name STRING,\n",
    "    customer_type STRING,\n",
    "    bandwidth_mbps DOUBLE,\n",
    "    data_cap_gb DOUBLE,\n",
    "    anomaly_type STRING,\n",
    "    severity STRING,\n",
    "    outage_duration_hours DOUBLE,\n",
    "    outage_type STRING,\n",
    "    impact_level STRING,\n",
    "    estimated_loss_da DOUBLE,\n",
    "    affected_services STRING,\n",
    "    old_offer STRING,\n",
    "    new_offer STRING,\n",
    "    old_price DOUBLE,\n",
    "    new_price DOUBLE,\n",
    "    change_reason STRING,\n",
    "    upgrade_type STRING,\n",
    "    upgrade_price DOUBLE,\n",
    "    speed_multiplier DOUBLE,\n",
    "    duration_days DOUBLE,\n",
    "    original_bandwidth DOUBLE,\n",
    "    boosted_bandwidth DOUBLE\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT, day INT)\n",
    "STORED AS PARQUET\n",
    "LOCATION '/user/hive/warehouse/at_generated_cdr.db/fact_cdr/'\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(create_cdr_table_sql)\n",
    "print(\"✅ CDR fact table created with daily partitions\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0baf35-b1dc-42a8-8952-53005e3d1fcc",
   "metadata": {},
   "source": [
    "## 5. Load CDR Data into Partitioned Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b09a431-9db8-4d41-af8a-247402917718",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 12:08:34 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 18\u001b[0m\n\u001b[1;32m     12\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP TABLE IF EXISTS \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtable_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 2) Read recursively so Spark can infer schema from any file in that folder:\u001b[39;00m\n\u001b[1;32m     15\u001b[0m sample_df \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     16\u001b[0m     \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecursiveFileLookup\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m---> 18\u001b[0m \u001b[43m         \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcdr_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m          \u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     20\u001b[0m )\n\u001b[1;32m     21\u001b[0m schema \u001b[38;5;241m=\u001b[39m sample_df\u001b[38;5;241m.\u001b[39mschema\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 3) Build a DDL column list\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:544\u001b[0m, in \u001b[0;36mDataFrameReader.parquet\u001b[0;34m(self, *paths, **options)\u001b[0m\n\u001b[1;32m    533\u001b[0m int96RebaseMode \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mint96RebaseMode\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    534\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m    535\u001b[0m     mergeSchema\u001b[38;5;241m=\u001b[39mmergeSchema,\n\u001b[1;32m    536\u001b[0m     pathGlobFilter\u001b[38;5;241m=\u001b[39mpathGlobFilter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m     int96RebaseMode\u001b[38;5;241m=\u001b[39mint96RebaseMode,\n\u001b[1;32m    542\u001b[0m )\n\u001b[0;32m--> 544\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_to_seq\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNABLE_TO_INFER_SCHEMA] Unable to infer schema for Parquet. It must be specified manually."
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Register fact_cdr External Table\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "cdr_path   = \"/user/hive/warehouse/Raw/raw_cdr_enhanced\"\n",
    "table_name = \"fact_cdr\"\n",
    "\n",
    "# 1) Drop any old definition\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# 2) Read recursively so Spark can infer schema from any file in that folder:\n",
    "sample_df = (\n",
    "    spark.read\n",
    "         .option(\"recursiveFileLookup\", \"true\")\n",
    "         .parquet(cdr_path)\n",
    "         .limit(1)\n",
    ")\n",
    "schema = sample_df.schema\n",
    "\n",
    "# 3) Build a DDL column list\n",
    "cols_ddl = \",\\n  \".join(f\"{f.name} {f.dataType.simpleString()}\" for f in schema.fields)\n",
    "\n",
    "# 4) Create the external table\n",
    "ddl = f\"\"\"\n",
    "CREATE EXTERNAL TABLE {table_name} (\n",
    "  {cols_ddl}\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION '{cdr_path}'\n",
    "\"\"\"\n",
    "print(\"Running DDL:\\n\", ddl)\n",
    "spark.sql(ddl)\n",
    "\n",
    "# 5) Repair partitions\n",
    "spark.sql(f\"MSCK REPAIR TABLE {table_name}\")\n",
    "\n",
    "# 6) Verify\n",
    "print(\"Total rows:\", spark.table(table_name).count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8b3d9af8-b960-4b8e-8197-29d8f51acf2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧹 Dropping any pre‐existing `fact_cdr`…\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "No parquet files found in /user/hive/warehouse/Raw/raw_cdr_enhanced",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m local_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(cdr_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*.parquet\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m local_files:\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo parquet files found in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcdr_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m sample_file \u001b[38;5;241m=\u001b[39m local_files[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m📂 Reading sample Parquet to infer schema:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m   \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msample_file\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No parquet files found in /user/hive/warehouse/Raw/raw_cdr_enhanced"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import os, glob\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Create fact_cdr External Table\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "cdr_path   = \"/user/hive/warehouse/Raw/raw_cdr_enhanced\"\n",
    "table_name = \"fact_cdr\"\n",
    "\n",
    "print(f\"🧹 Dropping any pre‐existing `{table_name}`…\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "# Glob your parquet files (assumes you're running on a node with local HDFS mount)\n",
    "# If that doesn't work, point to one known file, e.g. \".../cdr_20250718_to_20250719.parquet\"\n",
    "local_files = glob.glob(os.path.join(cdr_path, \"*.parquet\"))\n",
    "if not local_files:\n",
    "    raise RuntimeError(f\"No parquet files found in {cdr_path}\")\n",
    "\n",
    "sample_file = local_files[0]\n",
    "print(f\"📂 Reading sample Parquet to infer schema:\\n   {sample_file}\")\n",
    "sample_df = spark.read.parquet(sample_file)\n",
    "schema    = sample_df.schema\n",
    "print(f\"📐 Inferred schema with {len(schema.fields)} fields:\")\n",
    "sample_df.printSchema()\n",
    "\n",
    "print(f\"\\n🚀 Creating empty DataFrame with that schema…\")\n",
    "empty_df = spark.createDataFrame([], schema)\n",
    "\n",
    "print(f\"🚀 Writing empty DataFrame as external Hive table `{table_name}`…\")\n",
    "(\n",
    "    empty_df.write\n",
    "        .mode(\"overwrite\")\n",
    "        .format(\"parquet\")\n",
    "        .option(\"path\", cdr_path)    # point at existing parquet dir\n",
    "        .saveAsTable(table_name)     # register in the Hive metastore\n",
    ")\n",
    "\n",
    "print(\"✅ Table created! Repairing partitions…\")\n",
    "spark.sql(f\"MSCK REPAIR TABLE {table_name}\")\n",
    "\n",
    "# -- Verification --\n",
    "print(\"\\n📊 Total record count in `fact_cdr`:\")\n",
    "total = spark.table(table_name).count()\n",
    "print(f\"➡️  {total:,} rows\")\n",
    "\n",
    "print(\"\\n📅 Date summary:\")\n",
    "spark.table(table_name) \\\n",
    "     .selectExpr(\n",
    "         \"MIN(timestamp) AS first_ts\",\n",
    "         \"MAX(timestamp) AS last_ts\",\n",
    "         \"COUNT(DISTINCT TO_DATE(timestamp)) AS unique_days\"\n",
    "     ) \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "print(\"\\n📈 Monthly distribution (year/month → rows):\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT\n",
    "      YEAR(timestamp) AS year,\n",
    "      MONTH(timestamp) AS month,\n",
    "      COUNT(*)          AS records\n",
    "    FROM {table_name}\n",
    "    GROUP BY YEAR(timestamp), MONTH(timestamp)\n",
    "    ORDER BY year, month\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d177ee-c20c-4259-861c-3d012cf5ffca",
   "metadata": {},
   "source": [
    " ## 6. Create Optimized Views for Common Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946201e9-0e56-4bb7-aa07-07c12fd0ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View for daily aggregated metrics\n",
    "spark.sql(\"DROP VIEW IF EXISTS v_daily_metrics\")\n",
    "\n",
    "daily_metrics_view = \"\"\"\n",
    "CREATE VIEW IF NOT EXISTS v_daily_metrics AS\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    wilaya_code,\n",
    "    wilaya_name,\n",
    "    service_type,\n",
    "    customer_type,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(*) as total_cdrs,\n",
    "    SUM(CASE WHEN cdr_type = 'DATA' THEN 1 ELSE 0 END) as data_sessions,\n",
    "    SUM(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) as total_data_mb,\n",
    "    SUM(CASE WHEN cdr_type = 'DATA' THEN duration_minutes ELSE 0 END) as total_duration_minutes,\n",
    "    AVG(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE NULL END) as avg_session_data_mb,\n",
    "    COUNT(DISTINCT CASE WHEN cdr_type = 'ANOMALY' THEN customer_id END) as customers_with_anomalies,\n",
    "    COUNT(CASE WHEN cdr_type = 'OUTAGE' THEN 1 END) as outage_events,\n",
    "    SUM(CASE WHEN cdr_type = 'OUTAGE' THEN outage_duration_hours ELSE 0 END) as total_outage_hours\n",
    "FROM fact_cdr\n",
    "GROUP BY DATE(timestamp), wilaya_code, wilaya_name, service_type, customer_type\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(daily_metrics_view)\n",
    "print(\"✅ Daily metrics view created\")\n",
    "\n",
    "# View for customer usage summary\n",
    "spark.sql(\"DROP VIEW IF EXISTS v_customer_usage_summary\")\n",
    "\n",
    "customer_usage_view = \"\"\"\n",
    "CREATE VIEW IF NOT EXISTS v_customer_usage_summary AS\n",
    "SELECT \n",
    "    c.customer_id,\n",
    "    c.customer_type,\n",
    "    c.service_type,\n",
    "    c.offer_name,\n",
    "    c.bandwidth_mbps,\n",
    "    c.wilaya_name,\n",
    "    COUNT(f.cdr_id) as total_sessions,\n",
    "    SUM(CASE WHEN f.cdr_type = 'DATA' THEN f.data_volume_mb ELSE 0 END) as total_data_mb,\n",
    "    AVG(CASE WHEN f.cdr_type = 'DATA' THEN f.data_volume_mb ELSE NULL END) as avg_session_mb,\n",
    "    SUM(CASE WHEN f.cdr_type = 'DATA' THEN f.duration_minutes ELSE 0 END) as total_minutes,\n",
    "    COUNT(DISTINCT DATE(f.timestamp)) as active_days,\n",
    "    COUNT(CASE WHEN f.cdr_type = 'ANOMALY' THEN 1 END) as anomaly_count,\n",
    "    MAX(f.timestamp) as last_activity\n",
    "FROM dim_customers c\n",
    "LEFT JOIN fact_cdr f ON c.customer_id = f.customer_id\n",
    "WHERE c.is_active = true\n",
    "GROUP BY c.customer_id, c.customer_type, c.service_type, c.offer_name, \n",
    "         c.bandwidth_mbps, c.wilaya_name\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(customer_usage_view)\n",
    "print(\"✅ Customer usage summary view created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfab944-32da-415b-9232-db1d6aae4db8",
   "metadata": {},
   "source": [
    "## 7. Create Aggregated Tables for Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5592f3-fbc2-49de-abbb-bcc67c95a050",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# %%\n",
    "# Hourly aggregated table for real-time analytics\n",
    "spark.sql(\"DROP TABLE IF EXISTS agg_hourly_metrics\")\n",
    "\n",
    "hourly_agg_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS agg_hourly_metrics AS\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    HOUR(timestamp) as hour,\n",
    "    wilaya_code,\n",
    "    service_type,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(CASE WHEN cdr_type = 'DATA' THEN 1 END) as data_sessions,\n",
    "    SUM(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) as total_data_gb,\n",
    "    AVG(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE NULL END) as avg_session_mb,\n",
    "    PERCENTILE_APPROX(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE NULL END, 0.5) as median_session_mb,\n",
    "    MAX(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) as max_session_mb,\n",
    "    COUNT(CASE WHEN cdr_type = 'ANOMALY' THEN 1 END) as anomaly_count,\n",
    "    COUNT(CASE WHEN cdr_type = 'OUTAGE' THEN 1 END) as outage_count,\n",
    "    year,\n",
    "    month\n",
    "FROM fact_cdr\n",
    "GROUP BY DATE(timestamp), HOUR(timestamp), wilaya_code, service_type, year, month\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(hourly_agg_sql)\n",
    "print(\"✅ Hourly aggregated metrics table created\")\n",
    "\n",
    "# Network quality impact table\n",
    "spark.sql(\"DROP TABLE IF EXISTS agg_network_quality_impact\")\n",
    "\n",
    "network_quality_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS agg_network_quality_impact AS\n",
    "SELECT \n",
    "    c.wilaya_code,\n",
    "    c.wilaya_name,\n",
    "    c.network_quality_score,\n",
    "    CASE \n",
    "        WHEN c.network_quality_score >= 0.9 THEN 'Excellent'\n",
    "        WHEN c.network_quality_score >= 0.8 THEN 'Good'\n",
    "        WHEN c.network_quality_score >= 0.6 THEN 'Fair'\n",
    "        ELSE 'Poor'\n",
    "    END as quality_category,\n",
    "    COUNT(DISTINCT c.customer_id) as customer_count,\n",
    "    AVG(c.offer_price) as avg_revenue,\n",
    "    COUNT(f.cdr_id) as total_cdrs,\n",
    "    SUM(CASE WHEN f.cdr_type = 'DATA' THEN f.data_volume_mb ELSE 0 END) as total_data_mb,\n",
    "    COUNT(CASE WHEN f.cdr_type = 'ANOMALY' THEN 1 END) as anomaly_count,\n",
    "    COUNT(CASE WHEN f.cdr_type = 'OUTAGE' THEN 1 END) as outage_impact\n",
    "FROM dim_customers c\n",
    "LEFT JOIN fact_cdr f ON c.customer_id = f.customer_id\n",
    "GROUP BY c.wilaya_code, c.wilaya_name, c.network_quality_score\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(network_quality_sql)\n",
    "print(\"✅ Network quality impact table created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bd735c-303b-4ea5-8e1b-7d081f5fa294",
   "metadata": {},
   "source": [
    "## 8. Create Indexes and Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139af384-3362-4005-8bca-a44184a8df36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# %%\n",
    "# Compute statistics for query optimization\n",
    "print(\"Computing table statistics...\")\n",
    "\n",
    "# Analyze tables\n",
    "spark.sql(\"ANALYZE TABLE dim_customers COMPUTE STATISTICS\")\n",
    "spark.sql(\"ANALYZE TABLE fact_cdr PARTITION(year, month, day) COMPUTE STATISTICS\")\n",
    "spark.sql(\"ANALYZE TABLE agg_hourly_metrics COMPUTE STATISTICS\")\n",
    "spark.sql(\"ANALYZE TABLE agg_network_quality_impact COMPUTE STATISTICS\")\n",
    "\n",
    "print(\"✅ Table statistics computed\")\n",
    "\n",
    "# Create bloom filter indexes for frequently filtered columns (if supported)\n",
    "try:\n",
    "    spark.sql(\"\"\"\n",
    "        CREATE BLOOMFILTER INDEX idx_customer_id \n",
    "        ON TABLE fact_cdr (customer_id) \n",
    "        OPTIONS (numBits=1000000, numHashFunctions=5)\n",
    "    \"\"\")\n",
    "    print(\"✅ Bloom filter index created\")\n",
    "except:\n",
    "    print(\"ℹ️ Bloom filter indexes not supported in this Hive version\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2475db-9c79-4015-8689-9e5212223eb4",
   "metadata": {},
   "source": [
    "## 9. Create Materialized Views for Complex Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee25a48c-6109-4b3e-92f3-b89a316d94e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "\n",
    "# Customer behavior patterns view\n",
    "spark.sql(\"DROP VIEW IF EXISTS v_customer_behavior_patterns\")\n",
    "\n",
    "behavior_patterns_sql = \"\"\"\n",
    "CREATE VIEW IF NOT EXISTS v_customer_behavior_patterns AS\n",
    "WITH hourly_usage AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        HOUR(timestamp) as hour,\n",
    "        AVG(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) as avg_data_mb\n",
    "    FROM fact_cdr\n",
    "    WHERE cdr_type = 'DATA'\n",
    "    GROUP BY customer_id, HOUR(timestamp)\n",
    "),\n",
    "usage_patterns AS (\n",
    "    SELECT \n",
    "        customer_id,\n",
    "        CASE \n",
    "            WHEN MAX(CASE WHEN hour BETWEEN 20 AND 23 THEN avg_data_mb ELSE 0 END) > \n",
    "                 AVG(avg_data_mb) * 2 THEN 'Evening Heavy'\n",
    "            WHEN MAX(CASE WHEN hour BETWEEN 9 AND 17 THEN avg_data_mb ELSE 0 END) > \n",
    "                 AVG(avg_data_mb) * 2 THEN 'Business Hours'\n",
    "            WHEN MAX(CASE WHEN hour BETWEEN 0 AND 5 THEN avg_data_mb ELSE 0 END) > \n",
    "                 AVG(avg_data_mb) * 2 THEN 'Night Owl'\n",
    "            ELSE 'Regular'\n",
    "        END as usage_pattern\n",
    "    FROM hourly_usage\n",
    "    GROUP BY customer_id\n",
    ")\n",
    "SELECT \n",
    "    c.*,\n",
    "    up.usage_pattern,\n",
    "    cs.total_sessions,\n",
    "    cs.total_data_mb,\n",
    "    cs.anomaly_count\n",
    "FROM dim_customers c\n",
    "LEFT JOIN usage_patterns up ON c.customer_id = up.customer_id\n",
    "LEFT JOIN v_customer_usage_summary cs ON c.customer_id = cs.customer_id\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(behavior_patterns_sql)\n",
    "print(\"✅ Customer behavior patterns view created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a337b71-3be4-4ad1-b74f-b244800622bc",
   "metadata": {},
   "source": [
    "# ## 10. Create Special Analytics Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8723dcf-3cb2-4489-9ab2-b830ed59efd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Anomaly detection summary table\n",
    "spark.sql(\"DROP TABLE IF EXISTS analytics_anomaly_summary\")\n",
    "\n",
    "anomaly_summary_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS analytics_anomaly_summary AS\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    wilaya_code,\n",
    "    anomaly_type,\n",
    "    severity,\n",
    "    COUNT(*) as anomaly_count,\n",
    "    COUNT(DISTINCT customer_id) as affected_customers,\n",
    "    AVG(data_volume_mb) as avg_anomaly_volume,\n",
    "    COLLECT_LIST(customer_id) as sample_customers\n",
    "FROM fact_cdr\n",
    "WHERE cdr_type = 'ANOMALY'\n",
    "GROUP BY DATE(timestamp), wilaya_code, anomaly_type, severity\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(anomaly_summary_sql)\n",
    "print(\"✅ Anomaly summary table created\")\n",
    "\n",
    "# Revenue impact analysis table\n",
    "spark.sql(\"DROP TABLE IF EXISTS analytics_revenue_impact\")\n",
    "\n",
    "revenue_impact_sql = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS analytics_revenue_impact AS\n",
    "SELECT \n",
    "    c.customer_type,\n",
    "    c.service_type,\n",
    "    c.value_segment,\n",
    "    COUNT(DISTINCT c.customer_id) as customer_count,\n",
    "    SUM(c.offer_price) as total_monthly_revenue,\n",
    "    AVG(usage.total_data_mb) as avg_monthly_data_mb,\n",
    "    COUNT(DISTINCT CASE WHEN usage.anomaly_count > 0 THEN c.customer_id END) as customers_with_issues,\n",
    "    AVG(CASE WHEN NOT c.is_active THEN c.offer_price ELSE 0 END) as churned_revenue\n",
    "FROM ("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df614bb0-2817-4796-9b8a-27e92ef2ece0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Database created/selected\n",
      "📋 Registering customer dimension table...\n",
      "✅ Customer dimension table registered\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Records: 530,719\n",
      "\n",
      "📋 Registering CDR table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 18:10:52 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[INVALID_TEMP_OBJ_REFERENCE] Cannot create the persistent object `spark_catalog`.`at_cdr_analysis`.`fact_cdr` of the type VIEW because it references to the temporary object `fact_cdr_raw` of the type VIEW. Please make the temporary object `fact_cdr_raw` persistent, or make the persistent object `spark_catalog`.`at_cdr_analysis`.`fact_cdr` temporary.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Create a view with computed columns\u001b[39;00m\n\u001b[1;32m     66\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDROP VIEW IF EXISTS fact_cdr\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 67\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;43mCREATE VIEW IF NOT EXISTS fact_cdr AS\u001b[39;49m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;43mSELECT \u001b[39;49m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;124;43m    *,\u001b[39;49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;124;43m    YEAR(timestamp) as year,\u001b[39;49m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;124;43m    MONTH(timestamp) as month,\u001b[39;49m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;124;43m    DAY(timestamp) as day,\u001b[39;49m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;124;43m    HOUR(timestamp) as hour\u001b[39;49m\n\u001b[1;32m     75\u001b[0m \u001b[38;5;124;43mFROM fact_cdr_raw\u001b[39;49m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m cdr_count \u001b[38;5;241m=\u001b[39m cdr_df\u001b[38;5;241m.\u001b[39mcount()\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ CDR tables registered\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INVALID_TEMP_OBJ_REFERENCE] Cannot create the persistent object `spark_catalog`.`at_cdr_analysis`.`fact_cdr` of the type VIEW because it references to the temporary object `fact_cdr_raw` of the type VIEW. Please make the temporary object `fact_cdr_raw` persistent, or make the persistent object `spark_catalog`.`at_cdr_analysis`.`fact_cdr` temporary."
     ]
    }
   ],
   "source": [
    "# # %% [markdown]\n",
    "# # # 🆘 SURVIVAL MODE: Hive Tables Creation (Fixed Version)\n",
    "# # \n",
    "# # **Strategy**: Use DataFrames to register tables, avoid schema issues\n",
    "# # **Time Budget**: 20 minutes max\n",
    "\n",
    "# # %%\n",
    "# import sys\n",
    "# sys.path.append('/home/jovyan/work/batch/jupyter/notebooks/work/scripts')\n",
    "# from spark_init import init_spark\n",
    "# from pyspark.sql import functions as F\n",
    "# import time\n",
    "\n",
    "# # Initialize Spark\n",
    "# spark = init_spark(\"Hive Tables SURVIVAL MODE - Fixed\")\n",
    "# print(\"✅ SparkSession initialized\")\n",
    "# print(f\"⚠️ C: Drive Space Critical - Using minimal approach!\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Create Database\n",
    "\n",
    "# %%\n",
    "# Create database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS at_cdr_analysis\")\n",
    "spark.sql(\"USE at_cdr_analysis\")\n",
    "print(\"✅ Database created/selected\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 2. Register Tables Using DataFrames (Avoids Schema Issues)\n",
    "\n",
    "# %%\n",
    "print(\"📋 Registering customer dimension table...\")\n",
    "\n",
    "# Read the parquet files directly\n",
    "customer_df = spark.read.parquet(\"/user/hive/warehouse/Raw/customer_dim_enhanced/\")\n",
    "\n",
    "# Register as a table (this handles schema automatically)\n",
    "customer_df.createOrReplaceTempView(\"dim_customers_temp\")\n",
    "\n",
    "# Create permanent table from the temp view\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dim_customers\n",
    "    USING PARQUET\n",
    "    LOCATION '/user/hive/warehouse/Raw/customer_dim_enhanced/'\n",
    "    AS SELECT * FROM dim_customers_temp WHERE 1=0\n",
    "\"\"\")\n",
    "\n",
    "# Drop temp view\n",
    "spark.sql(\"DROP VIEW IF EXISTS dim_customers_temp\")\n",
    "print(\"✅ Customer dimension table registered\")\n",
    "print(f\"   Records: {customer_df.count():,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Register CDR Table\n",
    "\n",
    "# %%\n",
    "print(\"\\n📋 Registering CDR table...\")\n",
    "\n",
    "# Read CDR parquet files\n",
    "cdr_df = spark.read.parquet(\"/user/hive/warehouse/Raw/raw_cdr_enhanced/\")\n",
    "\n",
    "# Register as a view first (fastest approach)\n",
    "cdr_df.createOrReplaceTempView(\"fact_cdr_raw\")\n",
    "\n",
    "# Create a view with computed columns\n",
    "spark.sql(\"DROP VIEW IF EXISTS fact_cdr\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS fact_cdr AS\n",
    "SELECT \n",
    "    *,\n",
    "    YEAR(timestamp) as year,\n",
    "    MONTH(timestamp) as month,\n",
    "    DAY(timestamp) as day,\n",
    "    HOUR(timestamp) as hour\n",
    "FROM fact_cdr_raw\n",
    "\"\"\")\n",
    "\n",
    "cdr_count = cdr_df.count()\n",
    "print(\"✅ CDR tables registered\")\n",
    "print(f\"   Records: {cdr_count:,}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 4. Create Essential Views Only\n",
    "\n",
    "# %%\n",
    "print(\"\\n📊 Creating essential views...\")\n",
    "\n",
    "# Daily summary view (lightweight)\n",
    "spark.sql(\"DROP VIEW IF EXISTS v_daily_summary\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS v_daily_summary AS\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    wilaya_name,\n",
    "    service_type,\n",
    "    customer_type,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(*) as total_cdrs,\n",
    "    SUM(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) as total_data_mb,\n",
    "    COUNT(CASE WHEN cdr_type = 'ANOMALY' THEN 1 END) as anomaly_count\n",
    "FROM fact_cdr\n",
    "GROUP BY DATE(timestamp), wilaya_name, service_type, customer_type\n",
    "\"\"\")\n",
    "print(\"✅ Daily summary view created\")\n",
    "\n",
    "# Customer profile view\n",
    "spark.sql(\"DROP VIEW IF EXISTS v_customer_profile\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE VIEW IF NOT EXISTS v_customer_profile AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    wilaya_name,\n",
    "    service_type,\n",
    "    customer_type,\n",
    "    MIN(timestamp) as first_activity,\n",
    "    MAX(timestamp) as last_activity,\n",
    "    COUNT(*) as total_sessions,\n",
    "    SUM(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) as total_data_mb,\n",
    "    COUNT(CASE WHEN cdr_type = 'ANOMALY' THEN 1 END) as anomaly_count\n",
    "FROM fact_cdr\n",
    "GROUP BY customer_id, wilaya_name, service_type, customer_type\n",
    "\"\"\")\n",
    "print(\"✅ Customer profile view created\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 5. Create ONE Small Summary Table\n",
    "\n",
    "# %%\n",
    "print(\"\\n⚡ Creating essential summary table...\")\n",
    "\n",
    "# Create a small daily metrics table\n",
    "daily_metrics = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    COUNT(DISTINCT customer_id) as customers,\n",
    "    COUNT(*) as total_cdrs,\n",
    "    ROUND(SUM(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) / 1024, 2) as data_gb,\n",
    "    COUNT(CASE WHEN cdr_type = 'ANOMALY' THEN 1 END) as anomalies,\n",
    "    COUNT(CASE WHEN cdr_type = 'OUTAGE' THEN 1 END) as outages\n",
    "FROM fact_cdr\n",
    "GROUP BY DATE(timestamp)\n",
    "ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "# Convert to pandas and save (small file)\n",
    "daily_metrics_pd = daily_metrics.toPandas()\n",
    "daily_metrics_pd.to_csv(\"/mnt/d/daily_metrics.csv\", index=False)\n",
    "print(f\"✅ Daily metrics saved ({len(daily_metrics_pd)} rows)\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 6. Quick Validation\n",
    "\n",
    "# %%\n",
    "print(\"\\n🔍 Quick validation...\")\n",
    "\n",
    "# Date range check\n",
    "date_info = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    MIN(timestamp) as min_date,\n",
    "    MAX(timestamp) as max_date,\n",
    "    COUNT(DISTINCT DATE(timestamp)) as unique_days,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers\n",
    "FROM fact_cdr\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"📅 Date range: {date_info['min_date']} to {date_info['max_date']}\")\n",
    "print(f\"📊 Days: {date_info['unique_days']}, Customers: {date_info['unique_customers']:,}\")\n",
    "\n",
    "# Data distribution\n",
    "print(\"\\n📊 CDR Type Distribution:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    cdr_type,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM fact_cdr), 2) as percentage\n",
    "FROM fact_cdr\n",
    "GROUP BY cdr_type\n",
    "ORDER BY count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 7. Save Key Metrics for Presentation\n",
    "\n",
    "# %%\n",
    "print(\"\\n💾 Saving presentation metrics...\")\n",
    "\n",
    "# Collect key metrics\n",
    "metrics = {\n",
    "    \"total_records\": cdr_count,\n",
    "    \"unique_customers\": date_info['unique_customers'],\n",
    "    \"date_range\": f\"{date_info['min_date']} to {date_info['max_date']}\",\n",
    "    \"unique_days\": date_info['unique_days'],\n",
    "    \"data_volume_tb\": spark.sql(\"SELECT ROUND(SUM(data_volume_mb)/1024/1024, 2) FROM fact_cdr WHERE cdr_type='DATA'\").collect()[0][0],\n",
    "    \"anomaly_percentage\": spark.sql(\"SELECT ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM fact_cdr), 2) FROM fact_cdr WHERE cdr_type='ANOMALY'\").collect()[0][0]\n",
    "}\n",
    "\n",
    "# Save as JSON\n",
    "import json\n",
    "with open(\"/mnt/d/key_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2, default=str)\n",
    "\n",
    "print(\"✅ Metrics saved to D: drive\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Summary\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎉 SURVIVAL MODE COMPLETE - READY FOR ANALYSIS!\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "print(\"\\n✅ What we created:\")\n",
    "print(\"  - fact_cdr view: Your main CDR data\")\n",
    "print(\"  - dim_customers: Customer dimension\") \n",
    "print(\"  - v_daily_summary: Pre-aggregated daily view\")\n",
    "print(\"  - v_customer_profile: Customer-level summary\")\n",
    "print(\"  - daily_metrics.csv: Small file for PowerBI\")\n",
    "print(\"  - key_metrics.json: Presentation numbers\")\n",
    "\n",
    "print(\"\\n📊 Data available:\")\n",
    "print(f\"  - {cdr_count:,} CDR records\")\n",
    "print(f\"  - {date_info['unique_customers']:,} unique customers\")\n",
    "print(f\"  - {date_info['unique_days']} days of data\")\n",
    "\n",
    "print(\"\\n⚠️ CRITICAL REMINDERS:\")\n",
    "print(\"  ❌ Do NOT create large tables\")\n",
    "print(\"  ❌ Do NOT use .write.partitionBy()\")\n",
    "print(\"  ✅ Use views for queries\")\n",
    "print(\"  ✅ Save only small aggregated results\")\n",
    "print(\"  ✅ Work directly with DataFrames when possible\")\n",
    "\n",
    "print(\"\\n🚀 Next: Run anomaly detection notebook!\")\n",
    "\n",
    "# Clean cache\n",
    "spark.catalog.clearCache()\n",
    "print(\"\\n🧹 Cache cleared!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ba3c62d-e584-46fd-9e05-46f1889c0ecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TempView registered: dim_customers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 19:57:48 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Records: 530,719\n",
      "✅ TempView registered: fact_cdr_raw\n",
      "   Records: 768,359,379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ## 1. Register All DataFrames as Temporary Views\n",
    "\n",
    "# %%\n",
    "# Customers dimension: register as temp view\n",
    "customer_df = spark.read.parquet(\"/user/hive/warehouse/Raw/customer_dim_enhanced/\")\n",
    "customer_df.createOrReplaceTempView(\"dim_customers\")\n",
    "print(\"✅ TempView registered: dim_customers\")\n",
    "print(f\"   Records: {customer_df.count():,}\")\n",
    "\n",
    "# CDR facts: register as temp view\n",
    "cdr_df = spark.read.parquet(\"/user/hive/warehouse/Raw/raw_cdr_enhanced/\")\n",
    "cdr_df.createOrReplaceTempView(\"fact_cdr_raw\")\n",
    "print(\"✅ TempView registered: fact_cdr_raw\")\n",
    "print(f\"   Records: {cdr_df.count():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcd7c502-4652-42a6-9749-414909d94950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TempView: fact_cdr (core analytical view)\n"
     ]
    }
   ],
   "source": [
    "# ## 2. Analytical Temp Views (with Columns for Fast Analysis)\n",
    "\n",
    "# %%\n",
    "# Main fact CDR view with extra columns\n",
    "spark.sql(\"DROP VIEW IF EXISTS fact_cdr\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW fact_cdr AS\n",
    "SELECT *,\n",
    "    YEAR(timestamp)  as year,\n",
    "    MONTH(timestamp) as month,\n",
    "    DAY(timestamp)   as day,\n",
    "    HOUR(timestamp)  as hour\n",
    "FROM fact_cdr_raw\n",
    "\"\"\")\n",
    "print(\"✅ TempView: fact_cdr (core analytical view)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae743bd7-fad7-4c77-b619-e2c5e5adc396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ TempView: v_daily_summary\n",
      "✅ TempView: v_customer_profile\n"
     ]
    }
   ],
   "source": [
    "# ## 3. Aggregated Analytical Views (Daily, Profile)\n",
    "\n",
    "# %%\n",
    "# Daily summary (by wilaya/service/customer type)\n",
    "spark.sql(\"DROP VIEW IF EXISTS v_daily_summary\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_daily_summary AS\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    wilaya_name,\n",
    "    service_type,\n",
    "    customer_type,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers,\n",
    "    COUNT(*) as total_cdrs,\n",
    "    SUM(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) as total_data_mb,\n",
    "    COUNT(CASE WHEN cdr_type = 'ANOMALY' THEN 1 END) as anomaly_count\n",
    "FROM fact_cdr\n",
    "GROUP BY DATE(timestamp), wilaya_name, service_type, customer_type\n",
    "\"\"\")\n",
    "print(\"✅ TempView: v_daily_summary\")\n",
    "\n",
    "\n",
    "# Customer profile summary\n",
    "spark.sql(\"DROP VIEW IF EXISTS v_customer_profile\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMP VIEW v_customer_profile AS\n",
    "SELECT \n",
    "    customer_id,\n",
    "    wilaya_name,\n",
    "    service_type,\n",
    "    customer_type,\n",
    "    MIN(timestamp) as first_activity,\n",
    "    MAX(timestamp) as last_activity,\n",
    "    COUNT(*) as total_sessions,\n",
    "    SUM(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) as total_data_mb,\n",
    "    COUNT(CASE WHEN cdr_type = 'ANOMALY' THEN 1 END) as anomaly_count\n",
    "FROM fact_cdr\n",
    "GROUP BY customer_id, wilaya_name, service_type, customer_type\n",
    "\"\"\")\n",
    "print(\"✅ TempView: v_customer_profile\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d412608-441e-4871-aaf5-330b16e3329e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported: daily_metrics.csv (122 rows)\n"
     ]
    }
   ],
   "source": [
    "# ## 4. Daily Metrics Table (Exported for BI Tools)\n",
    "\n",
    "# %%\n",
    "# Only export *small* result tables!\n",
    "daily_metrics = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    DATE(timestamp) as date,\n",
    "    COUNT(DISTINCT customer_id) as customers,\n",
    "    COUNT(*) as total_cdrs,\n",
    "    ROUND(SUM(CASE WHEN cdr_type = 'DATA' THEN data_volume_mb ELSE 0 END) / 1024, 2) as data_gb,\n",
    "    COUNT(CASE WHEN cdr_type = 'ANOMALY' THEN 1 END) as anomalies,\n",
    "    COUNT(CASE WHEN cdr_type = 'OUTAGE' THEN 1 END) as outages\n",
    "FROM fact_cdr\n",
    "GROUP BY DATE(timestamp)\n",
    "ORDER BY date\n",
    "\"\"\")\n",
    "daily_metrics_pd = daily_metrics.toPandas()\n",
    "daily_metrics_pd.to_csv(\"/mnt/d/daily_metrics.csv\", index=False)\n",
    "print(f\"✅ Exported: daily_metrics.csv ({len(daily_metrics_pd)} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c50d30f3-1bec-4424-9e1d-641008233d2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📅 Date range: 2025-03-20 00:00:00 to 2025-07-19 23:59:59\n",
      "📊 Days: 122, Customers: 519,912\n",
      "\n",
      "📊 CDR Type Distribution:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+----------+\n",
      "|cdr_type    |count    |percentage|\n",
      "+------------+---------+----------+\n",
      "|DATA        |757223524|98.55     |\n",
      "|ANOMALY     |10646236 |1.39      |\n",
      "|OUTAGE      |372880   |0.05      |\n",
      "|PLAN_CHANGE |62832    |0.01      |\n",
      "|TEMP_UPGRADE|53907    |0.01      |\n",
      "+------------+---------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## 5. Validation & Quick Analysis\n",
    "\n",
    "# %%\n",
    "# Basic stats\n",
    "cdr_count = cdr_df.count()\n",
    "date_info = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    MIN(timestamp) as min_date,\n",
    "    MAX(timestamp) as max_date,\n",
    "    COUNT(DISTINCT DATE(timestamp)) as unique_days,\n",
    "    COUNT(DISTINCT customer_id) as unique_customers\n",
    "FROM fact_cdr\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"📅 Date range: {date_info['min_date']} to {date_info['max_date']}\")\n",
    "print(f\"📊 Days: {date_info['unique_days']}, Customers: {date_info['unique_customers']:,}\")\n",
    "\n",
    "# CDR Type breakdown\n",
    "print(\"\\n📊 CDR Type Distribution:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT \n",
    "    cdr_type,\n",
    "    COUNT(*) as count,\n",
    "    ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM fact_cdr), 2) as percentage\n",
    "FROM fact_cdr\n",
    "GROUP BY cdr_type\n",
    "ORDER BY count DESC\n",
    "\"\"\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d77d7cb-50bc-404f-82a8-5197966c8ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 39:===============================================>        (41 + 7) / 48]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Exported: key_metrics.json\n",
      "   total_records: 768359379\n",
      "   unique_customers: 519912\n",
      "   date_range: 2025-03-20 00:00:00 to 2025-07-19 23:59:59\n",
      "   unique_days: 122\n",
      "   data_volume_tb: 119046.54\n",
      "   anomaly_percentage: 1.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# ## 6. Export Key Metrics for Reporting\n",
    "import json \n",
    "# %%\n",
    "metrics = {\n",
    "    \"total_records\": cdr_count,\n",
    "    \"unique_customers\": date_info['unique_customers'],\n",
    "    \"date_range\": f\"{date_info['min_date']} to {date_info['max_date']}\",\n",
    "    \"unique_days\": date_info['unique_days'],\n",
    "    \"data_volume_tb\": spark.sql(\"SELECT ROUND(SUM(data_volume_mb)/1024/1024, 2) FROM fact_cdr WHERE cdr_type='DATA'\").collect()[0][0],\n",
    "    \"anomaly_percentage\": spark.sql(\"SELECT ROUND(100.0 * COUNT(*) / (SELECT COUNT(*) FROM fact_cdr), 2) FROM fact_cdr WHERE cdr_type='ANOMALY'\").collect()[0][0]\n",
    "}\n",
    "with open(\"/mnt/d/key_metrics.json\", \"w\") as f:\n",
    "    json.dump(metrics, f, indent=2, default=str)\n",
    "print(\"✅ Exported: key_metrics.json\")\n",
    "for k, v in metrics.items():\n",
    "    print(f\"   {k}: {v}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17852f41-4f17-40e1-ad43-79bde0fe5705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "🎉 SURVIVAL MODE COMPLETE - SESSION IS EPHEMERAL!\n",
      "==================================================\n",
      "\n",
      "✅ What’s available now:\n",
      "  - fact_cdr: Main CDR analytic view (temporary)\n",
      "  - dim_customers: Customer dimension (temporary)\n",
      "  - v_daily_summary: Daily aggregate view\n",
      "  - v_customer_profile: Per-customer summary\n",
      "  - daily_metrics.csv: Ready for BI/PowerBI/Superset\n",
      "  - key_metrics.json: Quick reporting\n",
      "\n",
      "📊 Data:\n",
      "  - 768,359,379 CDR records\n",
      "  - 519,912 unique customers\n",
      "  - 122 days covered\n",
      "\n",
      "⚠️ REMINDERS:\n",
      "  - No Hive tables/views created (safe to re-run, no warehouse bloat)\n",
      "  - Everything disappears after Spark stops\n",
      "  - Export only small Pandas files for presentations/BI\n",
      "🧹 Cache cleared. Survival mode session complete!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/07/07 19:56:14 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/07/07 19:56:14 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# ## 7. Survival Mode Completion\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎉 SURVIVAL MODE COMPLETE - SESSION IS EPHEMERAL!\")\n",
    "print(\"=\"*50)\n",
    "print(\"\\n✅ What’s available now:\")\n",
    "print(\"  - fact_cdr: Main CDR analytic view (temporary)\")\n",
    "print(\"  - dim_customers: Customer dimension (temporary)\")\n",
    "print(\"  - v_daily_summary: Daily aggregate view\")\n",
    "print(\"  - v_customer_profile: Per-customer summary\")\n",
    "print(\"  - daily_metrics.csv: Ready for BI/PowerBI/Superset\")\n",
    "print(\"  - key_metrics.json: Quick reporting\")\n",
    "\n",
    "print(\"\\n📊 Data:\")\n",
    "print(f\"  - {cdr_count:,} CDR records\")\n",
    "print(f\"  - {date_info['unique_customers']:,} unique customers\")\n",
    "print(f\"  - {date_info['unique_days']} days covered\")\n",
    "\n",
    "print(\"\\n⚠️ REMINDERS:\")\n",
    "print(\"  - No Hive tables/views created (safe to re-run, no warehouse bloat)\")\n",
    "print(\"  - Everything disappears after Spark stops\")\n",
    "print(\"  - Export only small Pandas files for presentations/BI\")\n",
    "\n",
    "# Clean Spark cache\n",
    "spark.catalog.clearCache()\n",
    "print(\"🧹 Cache cleared. Survival mode session complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
