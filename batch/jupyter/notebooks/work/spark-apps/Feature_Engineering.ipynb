{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df06c81f-512b-44df-b2c8-af607c08cea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/21 16:07:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SparkSession initialized (App: CDR Advanced Data Engineering & Analytics, Spark: 3.5.1)\n",
      "âœ… Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "âœ… Hive Metastore URI: thrift://hive-metastore:9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/21 16:07:37 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Advanced Processing Configuration:\n",
      "   Database: algerie_telecom_cdr\n",
      "   Source Table: cdr_anonymized\n",
      "   Total Tables to Create: 24\n",
      "   Spark Version: 3.5.1\n",
      "   Processing Started: 2025-06-21 16:07:37.407052\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# NOTEBOOK 03: ADVANCED DATA ENGINEERING & TELECOM ANALYTICS\n",
    "# Project: CDR Telecom Big Data Engineering Final Year Internship\n",
    "# Enhanced Version with Sophisticated Analytics\n",
    "# ============================================================\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 1 â€“ Setup, Imports, and Advanced Config\n",
    "# ------------------------------------------------------------\n",
    "import sys, os\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import functions as F, types as T, Window\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "import numpy as np\n",
    "\n",
    "# Cluster bootstrap\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "spark = init_spark(\"CDR Advanced Data Engineering & Analytics\")\n",
    "\n",
    "# Advanced Configuration\n",
    "DATABASE_NAME = \"algerie_telecom_cdr\"\n",
    "MAIN_TABLE = \"cdr_anonymized\"\n",
    "\n",
    "# Core tables\n",
    "CLEANED_TABLE = \"cdr_cleaned\"\n",
    "ENRICHED_TABLE = \"cdr_enriched\"\n",
    "\n",
    "# Time-based aggregation tables\n",
    "TRAFFIC_DAILY = \"traffic_daily\"\n",
    "TRAFFIC_HOURLY = \"traffic_hourly\"\n",
    "TRAFFIC_15MIN = \"traffic_15min\"\n",
    "TRAFFIC_WEEKLY = \"traffic_weekly\"\n",
    "SPECIAL_DAYS_TABLE = \"traffic_specialdays\"\n",
    "\n",
    "# Subscriber analytics tables\n",
    "SUBSCRIBER_METRICS = \"subscriber_metrics\"\n",
    "SUBSCRIBER_BEHAVIOR = \"subscriber_behavior_patterns\"\n",
    "SUBSCRIBER_LIFETIME = \"subscriber_lifetime_value\"\n",
    "CHURN_RISK = \"subscriber_churn_risk\"\n",
    "\n",
    "# Network performance tables\n",
    "NETWORK_PERF = \"network_performance_agg\"\n",
    "CELL_DAILY_KPIS = \"cell_daily_kpis\"\n",
    "CELL_HOURLY_KPIS = \"cell_hourly_kpis\"\n",
    "CELL_HOURLY_SPIKES = \"cell_hourly_spikes\"\n",
    "CELL_HEALTH_SCORE = \"cell_health_scores\"\n",
    "\n",
    "# Advanced analytics tables\n",
    "ROLLING_KPIS = \"traffic_rolling_kpis\"\n",
    "SEASONAL_PATTERNS = \"seasonal_patterns\"\n",
    "PEAK_ANALYSIS = \"peak_traffic_analysis\"\n",
    "REVENUE_ANALYTICS = \"revenue_analytics\"\n",
    "SERVICE_QUALITY = \"service_quality_metrics\"\n",
    "\n",
    "# Regional/Special tables\n",
    "YENNAYER_ANALYSIS = \"yennayer_special_analysis\"\n",
    "REGIONAL_METRICS = \"regional_metrics\"\n",
    "WILAYA_PERFORMANCE = \"wilaya_performance\"\n",
    "\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "print(f\"ðŸ”§ Advanced Processing Configuration:\")\n",
    "print(f\"   Database: {DATABASE_NAME}\")\n",
    "print(f\"   Source Table: {MAIN_TABLE}\")\n",
    "print(f\"   Total Tables to Create: 24\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n",
    "print(f\"   Processing Started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623e2946-efa2-4d34-98d4-49d54612df33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------+--------------+--------------+\n",
      "|Earliest_Day|Latest_Day|Earliest_Start|Latest_Start  |\n",
      "+------------+----------+--------------+--------------+\n",
      "|2024-12-31  |2025-01-01|20241231211909|20250101133522|\n",
      "+------------+----------+--------------+--------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/21 16:38:31 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/06/21 16:38:31 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Replace \"cdr_anonymized\" with your table if different\n",
    "df = spark.table(\"cdr_anonymized\")\n",
    "\n",
    "# 1. Check the min and max dates in the START_DATE or CDR_DAY column\n",
    "df.select(\n",
    "    F.min(\"CDR_DAY\").alias(\"Earliest_Day\"),\n",
    "    F.max(\"CDR_DAY\").alias(\"Latest_Day\"),\n",
    "    F.min(\"START_DATE\").alias(\"Earliest_Start\"),\n",
    "    F.max(\"START_DATE\").alias(\"Latest_Start\")\n",
    ").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0795b05e-11df-462a-ad05-a8837809cf27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Advanced Data Quality Profiling...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/21 16:07:46 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“ˆ Data Quality Summary:\n",
      "   Total Records: 89,911\n",
      "   Unique Subscribers: 40,843\n",
      "   Data Completeness: 100.00%\n",
      "   Date Range: 20241231211909 to 20250101133522\n",
      "\n",
      "ðŸ“… Daily Coverage Statistics:\n",
      "   Average Daily Records: 44,956\n",
      "   Standard Deviation: 53,421\n",
      "   Min/Max: 7,181 / 82,730\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 2 â€“ Data Quality Profiling & Coverage Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ“Š Advanced Data Quality Profiling...\")\n",
    "\n",
    "cdr_df = spark.table(MAIN_TABLE).cache()\n",
    "total_records = cdr_df.count()\n",
    "\n",
    "# Data quality scores\n",
    "quality_metrics = cdr_df.select(\n",
    "    F.count(\"*\").alias(\"total_records\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_subscribers\"),\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\").isNull(), 1).otherwise(0)).alias(\"null_usage\"),\n",
    "    F.sum(F.when(F.col(\"DEBIT_AMOUNT\").isNull(), 1).otherwise(0)).alias(\"null_revenue\"),\n",
    "    F.sum(F.when(F.col(\"CallingCellID\").isNull(), 1).otherwise(0)).alias(\"null_cells\"),\n",
    "    F.min(\"START_DATE\").alias(\"earliest_record\"),\n",
    "    F.max(\"START_DATE\").alias(\"latest_record\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"ðŸ“ˆ Data Quality Summary:\")\n",
    "print(f\"   Total Records: {quality_metrics['total_records']:,}\")\n",
    "print(f\"   Unique Subscribers: {quality_metrics['unique_subscribers']:,}\")\n",
    "print(f\"   Data Completeness: {(1 - quality_metrics['null_usage']/total_records)*100:.2f}%\")\n",
    "print(f\"   Date Range: {quality_metrics['earliest_record']} to {quality_metrics['latest_record']}\")\n",
    "\n",
    "# Data coverage analysis\n",
    "coverage = cdr_df.groupBy(\"CDR_DAY\").agg(\n",
    "    F.count(\"*\").alias(\"records\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_subs\"),\n",
    "    F.min(\"START_DATE\").alias(\"min_start\"),\n",
    "    F.max(\"START_DATE\").alias(\"max_start\"),\n",
    "    F.countDistinct(\"CallingCellID\").alias(\"active_cells\")\n",
    ").orderBy(\"CDR_DAY\")\n",
    "\n",
    "coverage_stats = coverage.agg(\n",
    "    F.avg(\"records\").alias(\"avg_daily_records\"),\n",
    "    F.stddev(\"records\").alias(\"std_daily_records\"),\n",
    "    F.min(\"records\").alias(\"min_daily_records\"),\n",
    "    F.max(\"records\").alias(\"max_daily_records\")\n",
    ").collect()[0]\n",
    "\n",
    "print(f\"\\nðŸ“… Daily Coverage Statistics:\")\n",
    "print(f\"   Average Daily Records: {coverage_stats['avg_daily_records']:,.0f}\")\n",
    "print(f\"   Standard Deviation: {coverage_stats['std_daily_records']:,.0f}\")\n",
    "print(f\"   Min/Max: {coverage_stats['min_daily_records']:,} / {coverage_stats['max_daily_records']:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "465ad784-ab63-44ee-a12d-bbc28801417f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ—“ï¸ Building Advanced Calendar Features...\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 3 â€“ Calendar Features & Algerian Holidays\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ—“ï¸ Building Advanced Calendar Features...\")\n",
    "\n",
    "# Algerian holidays within our data period (2024-12-31 to 2025-01-15)\n",
    "algerian_holidays = {\n",
    "    \"2024-12-31\": (\"New Year's Eve\", \"International\"),\n",
    "    \"2025-01-01\": (\"New Year's Day\", \"International\"),\n",
    "    \"2025-01-12\": (\"Yennayer\", \"Cultural\"),\n",
    "    # Note: We only have data until 2025-01-15\n",
    "}\n",
    "\n",
    "# Create holiday lookup DataFrame\n",
    "holiday_data = [(date, name, type_) for date, (name, type_) in algerian_holidays.items()]\n",
    "holidays_df = spark.createDataFrame(holiday_data, [\"date\", \"holiday_name\", \"holiday_type\"])\n",
    "\n",
    "# Enrich CDR data with calendar features\n",
    "cdr_df = cdr_df.withColumn(\"day_of_week\", F.dayofweek(\"CDR_DAY\"))\n",
    "cdr_df = cdr_df.withColumn(\"day_name\", \n",
    "    F.when(F.col(\"day_of_week\") == 1, \"Sunday\")\n",
    "     .when(F.col(\"day_of_week\") == 2, \"Monday\")\n",
    "     .when(F.col(\"day_of_week\") == 3, \"Tuesday\")\n",
    "     .when(F.col(\"day_of_week\") == 4, \"Wednesday\")\n",
    "     .when(F.col(\"day_of_week\") == 5, \"Thursday\")\n",
    "     .when(F.col(\"day_of_week\") == 6, \"Friday\")\n",
    "     .otherwise(\"Saturday\")\n",
    ")\n",
    "\n",
    "# Algerian weekend (Friday=6, Saturday=7)\n",
    "cdr_df = cdr_df.withColumn(\"is_weekend\", F.col(\"day_of_week\").isin([6,7]).cast(\"int\"))\n",
    "\n",
    "# Add month, quarter, week of year\n",
    "cdr_df = cdr_df.withColumn(\"month\", F.month(\"CDR_DAY\"))\n",
    "cdr_df = cdr_df.withColumn(\"quarter\", F.quarter(\"CDR_DAY\"))\n",
    "cdr_df = cdr_df.withColumn(\"week_of_year\", F.weekofyear(\"CDR_DAY\"))\n",
    "\n",
    "# Join with holidays\n",
    "cdr_df = cdr_df.join(\n",
    "    holidays_df, \n",
    "    cdr_df.CDR_DAY.cast(\"string\") == holidays_df.date, \n",
    "    \"left\"\n",
    ").withColumn(\"is_holiday\", F.when(F.col(\"holiday_name\").isNotNull(), 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1249b286-3518-4d27-ae14-700e8bc5512c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ§¹ Advanced Data Cleaning & Validation...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'array_sum'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;66;03m# Data quality score per record\u001b[39;00m\n\u001b[1;32m     38\u001b[0m quality_cols \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_outlier\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m numeric_validations\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_outlier\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m cdr_df\u001b[38;5;241m.\u001b[39mcolumns]\n\u001b[1;32m     39\u001b[0m cdr_df \u001b[38;5;241m=\u001b[39m cdr_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata_quality_score\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m---> 40\u001b[0m     F\u001b[38;5;241m.\u001b[39mlit(\u001b[38;5;241m100\u001b[39m) \u001b[38;5;241m-\u001b[39m (\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray_sum\u001b[49m(F\u001b[38;5;241m.\u001b[39marray(\u001b[38;5;241m*\u001b[39m[F\u001b[38;5;241m.\u001b[39mcol(c) \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m quality_cols])) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m     41\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Processing metadata\u001b[39;00m\n\u001b[1;32m     44\u001b[0m cdr_df \u001b[38;5;241m=\u001b[39m cdr_df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocessing_timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcurrent_timestamp())\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'pyspark.sql.functions' has no attribute 'array_sum'"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 4 â€“ Advanced Data Cleaning & Validation\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ§¹ Advanced Data Cleaning & Validation...\")\n",
    "\n",
    "# Numeric columns with validation\n",
    "numeric_validations = {\n",
    "    'ACTUAL_USAGE': (0, 86400),  # Max 24 hours in seconds\n",
    "    'RATE_USAGE': (0, 86400),\n",
    "    'DEBIT_AMOUNT': (0, 100000),  # Reasonable max amount\n",
    "    'UN_DEBIT_AMOUNT': (0, 100000),\n",
    "    'TOTAL_TAX': (0, 10000),\n",
    "    'ChargingTime': (0, 3600)  # Max 1 hour charging time\n",
    "}\n",
    "\n",
    "for col, (min_val, max_val) in numeric_validations.items():\n",
    "    if col in cdr_df.columns:\n",
    "        cdr_df = cdr_df.withColumn(f\"{col}_clean\", \n",
    "            F.when(F.col(col).between(min_val, max_val), F.col(col).cast(\"double\"))\n",
    "             .otherwise(None)\n",
    "        )\n",
    "        # Track outliers\n",
    "        cdr_df = cdr_df.withColumn(f\"{col}_outlier\",\n",
    "            F.when(F.col(col) > max_val, 1)\n",
    "             .when(F.col(col) < min_val, 1)\n",
    "             .otherwise(0)\n",
    "        )\n",
    "\n",
    "# Date columns with validation\n",
    "date_columns = ['START_DATE', 'END_DATE', 'CREATE_DATE', 'CUST_LOCAL_START_DATE', 'CUST_LOCAL_END_DATE']\n",
    "for col in date_columns:\n",
    "    if col in cdr_df.columns and col != \"CDR_DAY\":\n",
    "        cdr_df = cdr_df.withColumn(f\"{col}_clean\", \n",
    "            F.to_timestamp(F.col(col), \"yyyyMMddHHmmss\")\n",
    "        )\n",
    "\n",
    "# Data quality score per record\n",
    "quality_cols = [f\"{col}_outlier\" for col in numeric_validations.keys() if f\"{col}_outlier\" in cdr_df.columns]\n",
    "cdr_df = cdr_df.withColumn(\"data_quality_score\",\n",
    "    F.lit(100) - (F.array_sum(F.array(*[F.col(c) for c in quality_cols])) * 10)\n",
    ")\n",
    "\n",
    "# Processing metadata\n",
    "cdr_df = cdr_df.withColumn(\"processing_timestamp\", F.current_timestamp())\n",
    "cdr_df = cdr_df.withColumn(\"processing_date\", F.current_date())\n",
    "\n",
    "cdr_df.write.mode(\"overwrite\").saveAsTable(CLEANED_TABLE)\n",
    "print(f\"âœ… Advanced cleaned table saved: {CLEANED_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f69ff-b604-41a0-ae7e-51a134eb9fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 5 â€“ Feature Engineering & Derived Metrics\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ”§ Advanced Feature Engineering...\")\n",
    "\n",
    "enriched_df = cdr_df\n",
    "\n",
    "# Call duration categorization with more granularity\n",
    "enriched_df = enriched_df.withColumn(\"call_duration_category\",\n",
    "    F.when(F.col(\"ACTUAL_USAGE_clean\") == 0, \"Failed\")\n",
    "     .when(F.col(\"ACTUAL_USAGE_clean\") <= 10, \"Very Short\")\n",
    "     .when(F.col(\"ACTUAL_USAGE_clean\") <= 30, \"Short\")\n",
    "     .when(F.col(\"ACTUAL_USAGE_clean\") <= 120, \"Normal\")\n",
    "     .when(F.col(\"ACTUAL_USAGE_clean\") <= 300, \"Medium\")\n",
    "     .when(F.col(\"ACTUAL_USAGE_clean\") <= 1800, \"Long\")\n",
    "     .otherwise(\"Very Long\")\n",
    ")\n",
    "\n",
    "# Temporal features\n",
    "enriched_df = enriched_df \\\n",
    "    .withColumn(\"call_hour\", F.hour(F.col(\"START_DATE_clean\"))) \\\n",
    "    .withColumn(\"call_minute\", F.minute(F.col(\"START_DATE_clean\"))) \\\n",
    "    .withColumn(\"call_15min_bucket\", F.floor(F.col(\"call_hour\") * 4 + F.col(\"call_minute\") / 15)) \\\n",
    "    .withColumn(\"call_dayofweek\", F.dayofweek(F.col(\"START_DATE_clean\"))) \\\n",
    "    .withColumn(\"call_month\", F.month(F.col(\"START_DATE_clean\"))) \\\n",
    "    .withColumn(\"is_weekend_call\", F.when(F.dayofweek(F.col(\"START_DATE_clean\")).isin([6,7]), 1).otherwise(0))\n",
    "\n",
    "# Advanced time period categorization\n",
    "enriched_df = enriched_df.withColumn(\"time_period\",\n",
    "    F.when(F.hour(F.col(\"START_DATE_clean\")).between(0, 5), \"Late Night\")\n",
    "     .when(F.hour(F.col(\"START_DATE_clean\")).between(6, 8), \"Early Morning\")\n",
    "     .when(F.hour(F.col(\"START_DATE_clean\")).between(9, 11), \"Morning\")\n",
    "     .when(F.hour(F.col(\"START_DATE_clean\")).between(12, 13), \"Noon\")\n",
    "     .when(F.hour(F.col(\"START_DATE_clean\")).between(14, 17), \"Afternoon\")\n",
    "     .when(F.hour(F.col(\"START_DATE_clean\")).between(18, 20), \"Evening\")\n",
    "     .when(F.hour(F.col(\"START_DATE_clean\")).between(21, 23), \"Night\")\n",
    "     .otherwise(\"Late Night\")\n",
    ")\n",
    "\n",
    "# Service type mapping with more detail\n",
    "enriched_df = enriched_df.withColumn(\"service_type_group\",\n",
    "    F.when(F.col(\"SERVICE_CATEGORY\") == \"1\", \"Voice\")\n",
    "     .when(F.col(\"SERVICE_CATEGORY\") == \"2\", \"SMS\")\n",
    "     .when(F.col(\"SERVICE_CATEGORY\") == \"3\", \"Data\")\n",
    "     .when(F.col(\"SERVICE_CATEGORY\") == \"4\", \"MMS\")\n",
    "     .when(F.col(\"SERVICE_CATEGORY\") == \"5\", \"VAS\")\n",
    "     .otherwise(\"Other\")\n",
    ")\n",
    "\n",
    "# Revenue categorization with percentiles\n",
    "revenue_percentiles = enriched_df.filter(F.col(\"DEBIT_AMOUNT_clean\") > 0).select(\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT_clean, 0.25)\").alias(\"p25\"),\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT_clean, 0.50)\").alias(\"p50\"),\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT_clean, 0.75)\").alias(\"p75\"),\n",
    "    F.expr(\"percentile_approx(DEBIT_AMOUNT_clean, 0.95)\").alias(\"p95\")\n",
    ").collect()[0]\n",
    "\n",
    "enriched_df = enriched_df.withColumn(\"revenue_category\",\n",
    "    F.when(F.col(\"DEBIT_AMOUNT_clean\") == 0, \"Free\")\n",
    "     .when(F.col(\"DEBIT_AMOUNT_clean\") <= revenue_percentiles[\"p25\"], \"Low\")\n",
    "     .when(F.col(\"DEBIT_AMOUNT_clean\") <= revenue_percentiles[\"p50\"], \"Medium-Low\")\n",
    "     .when(F.col(\"DEBIT_AMOUNT_clean\") <= revenue_percentiles[\"p75\"], \"Medium-High\")\n",
    "     .when(F.col(\"DEBIT_AMOUNT_clean\") <= revenue_percentiles[\"p95\"], \"High\")\n",
    "     .otherwise(\"Premium\")\n",
    ")\n",
    "\n",
    "# Call success and quality metrics\n",
    "enriched_df = enriched_df.withColumn(\"call_success\", \n",
    "    F.when(F.col(\"ACTUAL_USAGE_clean\") > 0, 1).otherwise(0)\n",
    ")\n",
    "enriched_df = enriched_df.withColumn(\"call_completion_rate\",\n",
    "    F.when(F.col(\"ACTUAL_USAGE_clean\").isNotNull() & F.col(\"RATE_USAGE_clean\").isNotNull(),\n",
    "        F.col(\"ACTUAL_USAGE_clean\") / F.col(\"RATE_USAGE_clean\")\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# Network quality indicators\n",
    "enriched_df = enriched_df.withColumn(\"network_quality_indicator\",\n",
    "    F.when(F.col(\"call_success\") == 0, \"Poor\")\n",
    "     .when(F.col(\"call_completion_rate\") < 0.5, \"Fair\")\n",
    "     .when(F.col(\"call_completion_rate\") < 0.8, \"Good\")\n",
    "     .otherwise(\"Excellent\")\n",
    ")\n",
    "\n",
    "# Roaming and location features\n",
    "enriched_df = enriched_df.withColumn(\"is_roaming\",\n",
    "    F.when(F.col(\"RoamState\") == \"1\", 1).otherwise(0)\n",
    ")\n",
    "\n",
    "# Processing metadata\n",
    "enriched_df = enriched_df.withColumn(\"processing_batch_id\", \n",
    "    F.lit(datetime.now().strftime(\"%Y%m%d_%H%M%S\"))\n",
    ")\n",
    "enriched_df = enriched_df.withColumn(\"record_id\", F.monotonically_increasing_id())\n",
    "\n",
    "enriched_df.write.mode(\"overwrite\").saveAsTable(ENRICHED_TABLE)\n",
    "print(f\"âœ… Advanced enriched table saved: {ENRICHED_TABLE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6fa32d2-9c09-4a39-be73-8295a99572f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 6 â€“ Multi-Granularity Time Aggregations\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ“Š Building Multi-Granularity Aggregations...\")\n",
    "\n",
    "# --- 15-Minute Aggregations ---\n",
    "traffic_15min = enriched_df.groupBy(\"CDR_DAY\", \"call_15min_bucket\").agg(\n",
    "    F.count(\"*\").alias(\"calls_15min\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users_15min\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 1, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 0, 1).otherwise(0)).alias(\"failed_calls\"),\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"revenue_15min\"),\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_duration_15min\"),\n",
    "    F.max(\"ACTUAL_USAGE_clean\").alias(\"max_duration_15min\"),\n",
    "    F.stddev(\"ACTUAL_USAGE_clean\").alias(\"stddev_duration_15min\")\n",
    ").withColumn(\"failure_rate_15min\", \n",
    "    F.round(F.col(\"failed_calls\") / F.col(\"calls_15min\") * 100, 2)\n",
    ")\n",
    "traffic_15min.write.mode(\"overwrite\").saveAsTable(TRAFFIC_15MIN)\n",
    "print(f\"âœ… 15-minute traffic table created: {TRAFFIC_15MIN}\")\n",
    "\n",
    "# --- Hourly Aggregations (Enhanced) ---\n",
    "traffic_hourly = enriched_df.groupBy(\"CDR_DAY\", \"call_hour\").agg(\n",
    "    F.count(\"*\").alias(\"hourly_calls\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users_hourly\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 1, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 0, 1).otherwise(0)).alias(\"failed_calls\"),\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"hourly_revenue\"),\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_duration\"),\n",
    "    F.stddev(\"ACTUAL_USAGE_clean\").alias(\"stddev_duration\"),\n",
    "    F.expr(\"percentile_approx(ACTUAL_USAGE_clean, 0.5)\").alias(\"median_duration\"),\n",
    "    F.expr(\"percentile_approx(ACTUAL_USAGE_clean, 0.95)\").alias(\"p95_duration\"),\n",
    "    F.max(\"is_holiday\").alias(\"is_holiday\"),\n",
    "    F.max(\"holiday_name\").alias(\"holiday_name\"),\n",
    "    F.avg(\"data_quality_score\").alias(\"avg_quality_score\")\n",
    ").withColumn(\"calls_per_user\", \n",
    "    F.round(F.col(\"hourly_calls\") / F.col(\"unique_users_hourly\"), 2)\n",
    ").withColumn(\"success_rate\", \n",
    "    F.round(F.col(\"successful_calls\") / F.col(\"hourly_calls\") * 100, 2)\n",
    ").orderBy(\"CDR_DAY\", \"call_hour\")\n",
    "\n",
    "traffic_hourly.write.mode(\"overwrite\").saveAsTable(TRAFFIC_HOURLY)\n",
    "print(f\"âœ… Enhanced hourly traffic table created: {TRAFFIC_HOURLY}\")\n",
    "\n",
    "# --- Daily Aggregations (Comprehensive) ---\n",
    "traffic_daily = enriched_df.groupBy(\"CDR_DAY\").agg(\n",
    "    # Volume metrics\n",
    "    F.count(\"*\").alias(\"total_records\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_subscribers\"),\n",
    "    F.countDistinct(\"CallingCellID\").alias(\"active_cells\"),\n",
    "    \n",
    "    # Success metrics\n",
    "    F.sum(F.when(F.col(\"call_success\") == 0, 1).otherwise(0)).alias(\"failed_calls\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 1, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    \n",
    "    # Revenue metrics\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"total_revenue\"),\n",
    "    F.avg(\"DEBIT_AMOUNT_clean\").alias(\"avg_revenue_per_call\"),\n",
    "    F.sum(F.when(F.col(\"DEBIT_AMOUNT_clean\") > 0, 1).otherwise(0)).alias(\"paid_calls\"),\n",
    "    \n",
    "    # Duration metrics\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_call_duration\"),\n",
    "    F.sum(\"ACTUAL_USAGE_clean\").alias(\"total_call_minutes\"),\n",
    "    F.stddev(\"ACTUAL_USAGE_clean\").alias(\"stddev_duration\"),\n",
    "    F.expr(\"percentile_approx(ACTUAL_USAGE_clean, 0.5)\").alias(\"median_duration\"),\n",
    "    \n",
    "    # Quality metrics\n",
    "    F.avg(\"data_quality_score\").alias(\"avg_quality_score\"),\n",
    "    F.avg(\"call_completion_rate\").alias(\"avg_completion_rate\"),\n",
    "    \n",
    "    # Calendar features\n",
    "    F.max(\"is_weekend\").alias(\"is_weekend\"),\n",
    "    F.max(\"is_holiday\").alias(\"is_holiday\"),\n",
    "    F.max(\"holiday_name\").alias(\"holiday_name\"),\n",
    "    F.max(\"day_name\").alias(\"day_name\")\n",
    ").withColumn(\"success_rate\", \n",
    "    F.round(F.col(\"successful_calls\") / F.col(\"total_records\") * 100, 2)\n",
    ").withColumn(\"arpu\", \n",
    "    F.round(F.col(\"total_revenue\") / F.col(\"unique_subscribers\"), 2)\n",
    ").withColumn(\"avg_calls_per_user\", \n",
    "    F.round(F.col(\"total_records\") / F.col(\"unique_subscribers\"), 2)\n",
    ").orderBy(\"CDR_DAY\")\n",
    "\n",
    "traffic_daily.write.mode(\"overwrite\").saveAsTable(TRAFFIC_DAILY)\n",
    "print(f\"âœ… Comprehensive daily traffic table created: {TRAFFIC_DAILY}\")\n",
    "\n",
    "# --- Weekly Aggregations ---\n",
    "traffic_weekly = enriched_df.groupBy(\"week_of_year\").agg(\n",
    "    F.count(\"*\").alias(\"weekly_calls\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users_weekly\"),\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"weekly_revenue\"),\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_duration_weekly\"),\n",
    "    F.countDistinct(\"CDR_DAY\").alias(\"active_days\"),\n",
    "    F.countDistinct(\"CallingCellID\").alias(\"active_cells_weekly\")\n",
    ").orderBy(\"week_of_year\")\n",
    "\n",
    "traffic_weekly.write.mode(\"overwrite\").saveAsTable(TRAFFIC_WEEKLY)\n",
    "print(f\"âœ… Weekly traffic table created: {TRAFFIC_WEEKLY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0cecd3-c829-42f8-93d4-4407a6674f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 7 â€“ Special Days & Regional Analysis (Yennayer Focus)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸŽŠ Special Days & Regional Analysis...\")\n",
    "\n",
    "# Yennayer (Amazigh New Year) Analysis\n",
    "yennayer_df = enriched_df.filter(\n",
    "    (F.col(\"CDR_DAY\").cast(\"string\") == \"2025-01-12\") | \n",
    "    (F.col(\"CDR_DAY\").cast(\"string\").between(\"2025-01-11\", \"2025-01-13\"))  # Include surrounding days\n",
    ").withColumn(\"yennayer_period\",\n",
    "    F.when(F.col(\"CDR_DAY\").cast(\"string\") == \"2025-01-11\", \"Eve\")\n",
    "     .when(F.col(\"CDR_DAY\").cast(\"string\") == \"2025-01-12\", \"Yennayer Day\")\n",
    "     .when(F.col(\"CDR_DAY\").cast(\"string\") == \"2025-01-13\", \"Day After\")\n",
    ")\n",
    "\n",
    "# Yennayer cell-level analysis\n",
    "yennayer_cells = yennayer_df.groupBy(\"CallingCellID\", \"yennayer_period\").agg(\n",
    "    F.count(\"*\").alias(\"calls\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 1, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"revenue\"),\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_duration\"),\n",
    "    F.sum(F.when(F.col(\"time_period\").isin([\"Evening\", \"Night\"]), 1).otherwise(0)).alias(\"evening_calls\")\n",
    ").withColumn(\"evening_call_ratio\", \n",
    "    F.round(F.col(\"evening_calls\") / F.col(\"calls\") * 100, 2)\n",
    ")\n",
    "\n",
    "# Identify cells with highest Yennayer activity\n",
    "yennayer_spike_cells = yennayer_cells.filter(\n",
    "    F.col(\"yennayer_period\") == \"Yennayer Day\"\n",
    ").orderBy(F.desc(\"calls\"))\n",
    "\n",
    "yennayer_spike_cells.write.mode(\"overwrite\").saveAsTable(YENNAYER_ANALYSIS)\n",
    "print(f\"âœ… Yennayer analysis table created: {YENNAYER_ANALYSIS}\")\n",
    "\n",
    "# All special days analysis\n",
    "special_days_traffic = enriched_df.filter(F.col(\"is_holiday\") == 1).groupBy(\n",
    "    \"CDR_DAY\", \"holiday_name\", \"holiday_type\"\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"holiday_calls\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users\"),\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"holiday_revenue\"),\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_duration\"),\n",
    "    F.sum(F.when(F.col(\"call_hour\").between(18, 23), 1).otherwise(0)).alias(\"evening_calls\"),\n",
    "    F.sum(F.when(F.col(\"service_type_group\") == \"Voice\", 1).otherwise(0)).alias(\"voice_calls\"),\n",
    "    F.sum(F.when(F.col(\"service_type_group\") == \"SMS\", 1).otherwise(0)).alias(\"sms_messages\")\n",
    ").orderBy(\"CDR_DAY\")\n",
    "\n",
    "special_days_traffic.write.mode(\"overwrite\").saveAsTable(SPECIAL_DAYS_TABLE)\n",
    "print(f\"âœ… Special days analysis table created: {SPECIAL_DAYS_TABLE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ad450e-931e-4b77-90ae-796ffc2d0555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 8 â€“ Advanced Subscriber Analytics\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ‘¥ Advanced Subscriber Analytics...\")\n",
    "\n",
    "# Subscriber behavior patterns\n",
    "subscriber_window = Window.partitionBy(\"PRI_IDENTITY_HASH\").orderBy(\"START_DATE_clean\")\n",
    "\n",
    "subscriber_behavior = enriched_df.withColumn(\n",
    "    \"prev_call_time\", F.lag(\"START_DATE_clean\").over(subscriber_window)\n",
    ").withColumn(\n",
    "    \"time_since_last_call\", \n",
    "    F.unix_timestamp(\"START_DATE_clean\") - F.unix_timestamp(\"prev_call_time\")\n",
    ").groupBy(\"PRI_IDENTITY_HASH\").agg(\n",
    "    # Volume metrics\n",
    "    F.count(\"*\").alias(\"total_calls\"),\n",
    "    F.countDistinct(\"CDR_DAY\").alias(\"active_days\"),\n",
    "    F.countDistinct(\"week_of_year\").alias(\"active_weeks\"),\n",
    "    \n",
    "    # Success metrics\n",
    "    F.sum(F.when(F.col(\"call_success\") == 1, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 0, 1).otherwise(0)).alias(\"failed_calls\"),\n",
    "    \n",
    "    # Revenue metrics\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"total_revenue\"),\n",
    "    F.avg(\"DEBIT_AMOUNT_clean\").alias(\"avg_revenue_per_call\"),\n",
    "    F.stddev(\"DEBIT_AMOUNT_clean\").alias(\"revenue_volatility\"),\n",
    "    \n",
    "    # Usage patterns\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_call_duration\"),\n",
    "    F.stddev(\"ACTUAL_USAGE_clean\").alias(\"duration_volatility\"),\n",
    "    F.max(\"ACTUAL_USAGE_clean\").alias(\"longest_call\"),\n",
    "    \n",
    "    # Time patterns\n",
    "    F.avg(\"time_since_last_call\").alias(\"avg_time_between_calls\"),\n",
    "    F.min(\"START_DATE_clean\").alias(\"first_activity\"),\n",
    "    F.max(\"START_DATE_clean\").alias(\"last_activity\"),\n",
    "    \n",
    "    # Preferred time periods\n",
    "    F.sum(F.when(F.col(\"time_period\") == \"Morning\", 1).otherwise(0)).alias(\"morning_calls\"),\n",
    "    F.sum(F.when(F.col(\"time_period\") == \"Evening\", 1).otherwise(0)).alias(\"evening_calls\"),\n",
    "    F.sum(F.when(F.col(\"is_weekend_call\") == 1, 1).otherwise(0)).alias(\"weekend_calls\"),\n",
    "    \n",
    "    # Service preferences\n",
    "    F.sum(F.when(F.col(\"service_type_group\") == \"Voice\", 1).otherwise(0)).alias(\"voice_calls\"),\n",
    "    F.sum(F.when(F.col(\"service_type_group\") == \"SMS\", 1).otherwise(0)).alias(\"sms_count\"),\n",
    "    F.sum(F.when(F.col(\"service_type_group\") == \"Data\", 1).otherwise(0)).alias(\"data_sessions\"),\n",
    "    \n",
    "    # Network quality experience\n",
    "    F.avg(\"call_completion_rate\").alias(\"avg_completion_rate\"),\n",
    "    F.avg(\"data_quality_score\").alias(\"avg_quality_score\")\n",
    ")\n",
    "\n",
    "# Calculate derived metrics\n",
    "subscriber_behavior = subscriber_behavior.withColumn(\n",
    "    \"success_rate\", F.round(F.col(\"successful_calls\") / F.col(\"total_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"daily_arpu\", F.round(F.col(\"total_revenue\") / F.col(\"active_days\"), 2)\n",
    ").withColumn(\n",
    "    \"weekend_preference\", F.round(F.col(\"weekend_calls\") / F.col(\"total_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"evening_preference\", F.round(F.col(\"evening_calls\") / F.col(\"total_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"days_since_last_activity\", \n",
    "    F.datediff(F.current_date(), F.col(\"last_activity\"))\n",
    ").withColumn(\n",
    "    \"customer_lifetime_days\", \n",
    "    F.datediff(F.col(\"last_activity\"), F.col(\"first_activity\"))\n",
    ")\n",
    "\n",
    "# Subscriber segmentation\n",
    "subscriber_behavior = subscriber_behavior.withColumn(\n",
    "    \"usage_segment\",\n",
    "    F.when(F.col(\"total_calls\") >= 100, \"Power User\")\n",
    "     .when(F.col(\"total_calls\") >= 50, \"Heavy User\")\n",
    "     .when(F.col(\"total_calls\") >= 20, \"Regular User\")\n",
    "     .when(F.col(\"total_calls\") >= 5, \"Light User\")\n",
    "     .otherwise(\"Minimal User\")\n",
    ").withColumn(\n",
    "    \"revenue_segment\",\n",
    "    F.when(F.col(\"total_revenue\") >= 10000, \"Premium\")\n",
    "     .when(F.col(\"total_revenue\") >= 5000, \"High Value\")\n",
    "     .when(F.col(\"total_revenue\") >= 1000, \"Medium Value\")\n",
    "     .when(F.col(\"total_revenue\") > 0, \"Low Value\")\n",
    "     .otherwise(\"No Revenue\")\n",
    ").withColumn(\n",
    "    \"activity_segment\",\n",
    "    F.when(F.col(\"active_days\") >= 25, \"Daily Active\")\n",
    "     .when(F.col(\"active_days\") >= 15, \"Very Active\")\n",
    "     .when(F.col(\"active_days\") >= 7, \"Active\")\n",
    "     .when(F.col(\"active_days\") >= 3, \"Occasional\")\n",
    "     .otherwise(\"Rare\")\n",
    ")\n",
    "\n",
    "subscriber_behavior.write.mode(\"overwrite\").saveAsTable(SUBSCRIBER_BEHAVIOR)\n",
    "print(f\"âœ… Subscriber behavior patterns table created: {SUBSCRIBER_BEHAVIOR}\")\n",
    "\n",
    "# Churn risk analysis\n",
    "churn_risk = subscriber_behavior.withColumn(\n",
    "    \"churn_risk_score\",\n",
    "    F.when(F.col(\"days_since_last_activity\") > 30, 100)\n",
    "     .when(F.col(\"days_since_last_activity\") > 14, 75)\n",
    "     .when(F.col(\"days_since_last_activity\") > 7, 50)\n",
    "     .when(F.col(\"days_since_last_activity\") > 3, 25)\n",
    "     .otherwise(0)\n",
    ").withColumn(\n",
    "    \"churn_risk_category\",\n",
    "    F.when(F.col(\"churn_risk_score\") >= 75, \"High Risk\")\n",
    "     .when(F.col(\"churn_risk_score\") >= 50, \"Medium Risk\")\n",
    "     .when(F.col(\"churn_risk_score\") >= 25, \"Low Risk\")\n",
    "     .otherwise(\"Active\")\n",
    ").select(\n",
    "    \"PRI_IDENTITY_HASH\", \"churn_risk_score\", \"churn_risk_category\",\n",
    "    \"days_since_last_activity\", \"total_calls\", \"total_revenue\",\n",
    "    \"customer_lifetime_days\", \"usage_segment\", \"revenue_segment\"\n",
    ")\n",
    "\n",
    "churn_risk.write.mode(\"overwrite\").saveAsTable(CHURN_RISK)\n",
    "print(f\"âœ… Churn risk analysis table created: {CHURN_RISK}\")\n",
    "\n",
    "# Customer Lifetime Value (CLV) calculation\n",
    "clv_df = subscriber_behavior.withColumn(\n",
    "    \"monthly_revenue\", F.col(\"total_revenue\") / (F.col(\"customer_lifetime_days\") / 30)\n",
    ").withColumn(\n",
    "    \"predicted_lifetime_months\", \n",
    "    F.when(F.col(\"churn_risk_score\") >= 75, 1)\n",
    "     .when(F.col(\"churn_risk_score\") >= 50, 3)\n",
    "     .when(F.col(\"churn_risk_score\") >= 25, 6)\n",
    "     .otherwise(12)\n",
    ").withColumn(\n",
    "    \"estimated_clv\", F.col(\"monthly_revenue\") * F.col(\"predicted_lifetime_months\")\n",
    ").select(\n",
    "    \"PRI_IDENTITY_HASH\", \"total_revenue\", \"monthly_revenue\",\n",
    "    \"predicted_lifetime_months\", \"estimated_clv\", \"usage_segment\", \"revenue_segment\"\n",
    ")\n",
    "\n",
    "clv_df.write.mode(\"overwrite\").saveAsTable(SUBSCRIBER_LIFETIME)\n",
    "print(f\"âœ… Customer lifetime value table created: {SUBSCRIBER_LIFETIME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f310736c-acee-464c-b9c5-599b111f7e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 9 â€“ Network Performance & Cell Analytics\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ“¡ Advanced Network Performance Analytics...\")\n",
    "\n",
    "# Cell-level daily KPIs with advanced metrics\n",
    "cell_daily_kpis = enriched_df.groupBy(\"CallingCellID\", \"CDR_DAY\").agg(\n",
    "    # Volume metrics\n",
    "    F.count(\"*\").alias(\"total_calls\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users\"),\n",
    "    F.countDistinct(\"call_hour\").alias(\"active_hours\"),\n",
    "    \n",
    "    # Success metrics\n",
    "    F.sum(F.when(F.col(\"call_success\") == 1, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 0, 1).otherwise(0)).alias(\"failed_calls\"),\n",
    "    \n",
    "    # Revenue metrics\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"total_revenue\"),\n",
    "    F.avg(\"DEBIT_AMOUNT_clean\").alias(\"avg_revenue_per_call\"),\n",
    "    \n",
    "    # Duration metrics\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_call_duration\"),\n",
    "    F.sum(\"ACTUAL_USAGE_clean\").alias(\"total_minutes\"),\n",
    "    F.stddev(\"ACTUAL_USAGE_clean\").alias(\"duration_variance\"),\n",
    "    \n",
    "    # Quality metrics\n",
    "    F.avg(\"call_completion_rate\").alias(\"avg_completion_rate\"),\n",
    "    F.avg(\"data_quality_score\").alias(\"avg_quality_score\"),\n",
    "    \n",
    "    # Time distribution\n",
    "    F.sum(F.when(F.col(\"time_period\").isin([\"Morning\", \"Early Morning\"]), 1).otherwise(0)).alias(\"morning_calls\"),\n",
    "    F.sum(F.when(F.col(\"time_period\").isin([\"Evening\", \"Night\", \"Late Night\"]), 1).otherwise(0)).alias(\"evening_calls\"),\n",
    "    F.sum(F.when(F.col(\"is_weekend_call\") == 1, 1).otherwise(0)).alias(\"weekend_calls\")\n",
    ").withColumn(\n",
    "    \"failure_rate\", F.round(F.col(\"failed_calls\") / F.col(\"total_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"calls_per_user\", F.round(F.col(\"total_calls\") / F.col(\"unique_users\"), 2)\n",
    ").withColumn(\n",
    "    \"revenue_per_user\", F.round(F.col(\"total_revenue\") / F.col(\"unique_users\"), 2)\n",
    ").withColumn(\n",
    "    \"cell_load_score\", \n",
    "    F.when(F.col(\"total_calls\") > 1000, \"Very High\")\n",
    "     .when(F.col(\"total_calls\") > 500, \"High\")\n",
    "     .when(F.col(\"total_calls\") > 100, \"Medium\")\n",
    "     .otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "cell_daily_kpis.write.mode(\"overwrite\").saveAsTable(CELL_DAILY_KPIS)\n",
    "print(f\"âœ… Cell-level daily KPIs table created: {CELL_DAILY_KPIS}\")\n",
    "\n",
    "# Cell health scoring\n",
    "cell_health = cell_daily_kpis.groupBy(\"CallingCellID\").agg(\n",
    "    F.avg(\"failure_rate\").alias(\"avg_failure_rate\"),\n",
    "    F.stddev(\"failure_rate\").alias(\"failure_rate_volatility\"),\n",
    "    F.avg(\"total_calls\").alias(\"avg_daily_calls\"),\n",
    "    F.avg(\"unique_users\").alias(\"avg_daily_users\"),\n",
    "    F.sum(\"total_revenue\").alias(\"total_cell_revenue\"),\n",
    "    F.count(\"*\").alias(\"active_days\")\n",
    ").withColumn(\n",
    "    \"reliability_score\", \n",
    "    F.round(100 - F.col(\"avg_failure_rate\"), 2)\n",
    ").withColumn(\n",
    "    \"stability_score\",\n",
    "    F.round(100 - F.least(F.col(\"failure_rate_volatility\") * 10, F.lit(100)), 2)\n",
    ").withColumn(\n",
    "    \"cell_health_score\",\n",
    "    F.round((F.col(\"reliability_score\") + F.col(\"stability_score\")) / 2, 2)\n",
    ").withColumn(\n",
    "    \"cell_category\",\n",
    "    F.when(F.col(\"cell_health_score\") >= 90, \"Excellent\")\n",
    "     .when(F.col(\"cell_health_score\") >= 80, \"Good\")\n",
    "     .when(F.col(\"cell_health_score\") >= 70, \"Fair\")\n",
    "     .when(F.col(\"cell_health_score\") >= 60, \"Poor\")\n",
    "     .otherwise(\"Critical\")\n",
    ")\n",
    "\n",
    "cell_health.write.mode(\"overwrite\").saveAsTable(CELL_HEALTH_SCORE)\n",
    "print(f\"âœ… Cell health scores table created: {CELL_HEALTH_SCORE}\")\n",
    "\n",
    "# Hourly cell patterns for capacity planning\n",
    "cell_hourly = enriched_df.groupBy(\"CallingCellID\", \"CDR_DAY\", \"call_hour\").agg(\n",
    "    F.count(\"*\").alias(\"hourly_calls\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users\"),\n",
    "    F.sum(F.when(F.col(\"call_success\") == 0, 1).otherwise(0)).alias(\"failed_calls\"),\n",
    "    F.avg(\"ACTUAL_USAGE_clean\").alias(\"avg_duration\"),\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"hourly_revenue\")\n",
    ").withColumn(\n",
    "    \"hour_failure_rate\", \n",
    "    F.round(F.col(\"failed_calls\") / F.col(\"hourly_calls\") * 100, 2)\n",
    ")\n",
    "\n",
    "# Detect hourly spikes using statistical methods\n",
    "window_cell_stats = Window.partitionBy(\"CallingCellID\", \"CDR_DAY\")\n",
    "cell_hourly_stats = cell_hourly.withColumn(\n",
    "    \"daily_avg_calls\", F.avg(\"hourly_calls\").over(window_cell_stats)\n",
    ").withColumn(\n",
    "    \"daily_stddev_calls\", F.stddev(\"hourly_calls\").over(window_cell_stats)\n",
    ").withColumn(\n",
    "    \"z_score\", \n",
    "    F.when(F.col(\"daily_stddev_calls\") > 0,\n",
    "        (F.col(\"hourly_calls\") - F.col(\"daily_avg_calls\")) / F.col(\"daily_stddev_calls\")\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"spike_indicator\",\n",
    "    F.when(F.col(\"z_score\") > 3, \"Extreme Spike\")\n",
    "     .when(F.col(\"z_score\") > 2, \"Major Spike\")\n",
    "     .when(F.col(\"z_score\") > 1, \"Minor Spike\")\n",
    "     .when(F.col(\"z_score\") < -2, \"Major Drop\")\n",
    "     .when(F.col(\"z_score\") < -1, \"Minor Drop\")\n",
    "     .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "cell_hourly_stats.write.mode(\"overwrite\").saveAsTable(CELL_HOURLY_SPIKES)\n",
    "print(f\"âœ… Cell hourly patterns with spike detection table created: {CELL_HOURLY_SPIKES}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61058b6a-270e-489b-9436-b69d6d50b614",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 10 â€“ Advanced Time Series Analytics\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ“ˆ Advanced Time Series & Trend Analytics...\")\n",
    "\n",
    "# Multiple rolling windows for trend analysis\n",
    "window_3d = Window.orderBy(\"CDR_DAY\").rowsBetween(-2, 0)\n",
    "window_7d = Window.orderBy(\"CDR_DAY\").rowsBetween(-6, 0)\n",
    "window_14d = Window.orderBy(\"CDR_DAY\").rowsBetween(-13, 0)\n",
    "window_30d = Window.orderBy(\"CDR_DAY\").rowsBetween(-29, 0)\n",
    "\n",
    "traffic_daily_df = spark.table(TRAFFIC_DAILY)\n",
    "\n",
    "rolling_df = traffic_daily_df.withColumn(\n",
    "    \"calls_3d_avg\", F.avg(\"total_records\").over(window_3d)\n",
    ").withColumn(\n",
    "    \"calls_7d_avg\", F.avg(\"total_records\").over(window_7d)\n",
    ").withColumn(\n",
    "    \"calls_14d_avg\", F.avg(\"total_records\").over(window_14d)\n",
    ").withColumn(\n",
    "    \"calls_30d_avg\", F.avg(\"total_records\").over(window_30d)\n",
    ").withColumn(\n",
    "    \"revenue_7d_avg\", F.avg(\"total_revenue\").over(window_7d)\n",
    ").withColumn(\n",
    "    \"revenue_30d_avg\", F.avg(\"total_revenue\").over(window_30d)\n",
    ").withColumn(\n",
    "    \"calls_wow_change\", \n",
    "    F.round((F.col(\"total_records\") - F.lag(\"total_records\", 7).over(Window.orderBy(\"CDR_DAY\"))) / \n",
    "    F.lag(\"total_records\", 7).over(Window.orderBy(\"CDR_DAY\")) * 100, 2)\n",
    ").withColumn(\n",
    "    \"calls_mom_change\",\n",
    "    F.round((F.col(\"total_records\") - F.lag(\"total_records\", 30).over(Window.orderBy(\"CDR_DAY\"))) / \n",
    "    F.lag(\"total_records\", 30).over(Window.orderBy(\"CDR_DAY\")) * 100, 2)\n",
    ").withColumn(\n",
    "    \"trend_strength\",\n",
    "    F.abs(F.col(\"calls_7d_avg\") - F.col(\"calls_30d_avg\")) / F.col(\"calls_30d_avg\") * 100\n",
    ").withColumn(\n",
    "    \"trend_direction\",\n",
    "    F.when(F.col(\"calls_7d_avg\") > F.col(\"calls_30d_avg\") * 1.05, \"Strong Upward\")\n",
    "     .when(F.col(\"calls_7d_avg\") > F.col(\"calls_30d_avg\") * 1.02, \"Upward\")\n",
    "     .when(F.col(\"calls_7d_avg\") < F.col(\"calls_30d_avg\") * 0.95, \"Strong Downward\")\n",
    "     .when(F.col(\"calls_7d_avg\") < F.col(\"calls_30d_avg\") * 0.98, \"Downward\")\n",
    "     .otherwise(\"Stable\")\n",
    ").withColumn(\n",
    "    \"volatility_score\",\n",
    "    F.stddev(\"total_records\").over(window_7d) / F.avg(\"total_records\").over(window_7d) * 100\n",
    ")\n",
    "\n",
    "rolling_df.write.mode(\"overwrite\").saveAsTable(ROLLING_KPIS)\n",
    "print(f\"âœ… Advanced rolling KPIs table created: {ROLLING_KPIS}\")\n",
    "\n",
    "# Seasonal pattern detection\n",
    "seasonal_patterns = enriched_df.groupBy(\"call_hour\", \"day_name\").agg(\n",
    "    F.avg(\"hourly_calls\").alias(\"avg_calls\"),\n",
    "    F.stddev(\"hourly_calls\").alias(\"stddev_calls\"),\n",
    "    F.expr(\"percentile_approx(hourly_calls, 0.5)\").alias(\"median_calls\"),\n",
    "    F.expr(\"percentile_approx(hourly_calls, 0.95)\").alias(\"p95_calls\")\n",
    ").withColumn(\n",
    "    \"pattern_reliability\",\n",
    "    F.when(F.col(\"stddev_calls\") / F.col(\"avg_calls\") < 0.3, \"High\")\n",
    "     .when(F.col(\"stddev_calls\") / F.col(\"avg_calls\") < 0.5, \"Medium\")\n",
    "     .otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "seasonal_patterns.write.mode(\"overwrite\").saveAsTable(SEASONAL_PATTERNS)\n",
    "print(f\"âœ… Seasonal patterns table created: {SEASONAL_PATTERNS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da6c4100-543a-4a93-9409-64a0d187475d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 11 â€“ Revenue Analytics & Optimization\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ’° Advanced Revenue Analytics...\")\n",
    "\n",
    "revenue_analytics = enriched_df.groupBy(\"CDR_DAY\", \"service_type_group\").agg(\n",
    "    F.count(\"*\").alias(\"service_calls\"),\n",
    "    F.sum(\"DEBIT_AMOUNT_clean\").alias(\"service_revenue\"),\n",
    "    F.avg(\"DEBIT_AMOUNT_clean\").alias(\"avg_service_charge\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users\"),\n",
    "    F.sum(F.when(F.col(\"DEBIT_AMOUNT_clean\") == 0, 1).otherwise(0)).alias(\"free_calls\"),\n",
    "    F.sum(F.when(F.col(\"DEBIT_AMOUNT_clean\") > 0, 1).otherwise(0)).alias(\"paid_calls\")\n",
    ").withColumn(\n",
    "    \"monetization_rate\", \n",
    "    F.round(F.col(\"paid_calls\") / F.col(\"service_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"arpu_service\", \n",
    "    F.round(F.col(\"service_revenue\") / F.col(\"unique_users\"), 2)\n",
    ").pivot(\"service_type_group\").sum(\"service_revenue\")\n",
    "\n",
    "revenue_analytics.write.mode(\"overwrite\").saveAsTable(REVENUE_ANALYTICS)\n",
    "print(f\"âœ… Revenue analytics table created: {REVENUE_ANALYTICS}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d109ca56-846b-4e2c-9569-f774c6ed15fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 12 â€“ Service Quality Metrics\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸŽ¯ Service Quality Metrics...\")\n",
    "\n",
    "service_quality = enriched_df.groupBy(\"CDR_DAY\", \"time_period\").agg(\n",
    "    F.count(\"*\").alias(\"total_calls\"),\n",
    "    F.avg(\"call_completion_rate\").alias(\"avg_completion_rate\"),\n",
    "    F.avg(\"data_quality_score\").alias(\"avg_quality_score\"),\n",
    "    F.sum(F.when(F.col(\"network_quality_indicator\") == \"Poor\", 1).otherwise(0)).alias(\"poor_quality_calls\"),\n",
    "    F.sum(F.when(F.col(\"network_quality_indicator\") == \"Excellent\", 1).otherwise(0)).alias(\"excellent_quality_calls\"),\n",
    "    F.avg(F.when(F.col(\"call_success\") == 1, F.col(\"ACTUAL_USAGE_clean\")).otherwise(None)).alias(\"avg_successful_duration\"),\n",
    "    F.stddev(\"ACTUAL_USAGE_clean\").alias(\"duration_consistency\")\n",
    ").withColumn(\n",
    "    \"quality_index\", \n",
    "    F.round((F.col(\"avg_completion_rate\") * 0.4 + \n",
    "             F.col(\"avg_quality_score\") / 100 * 0.3 +\n",
    "             (1 - F.col(\"poor_quality_calls\") / F.col(\"total_calls\")) * 0.3) * 100, 2)\n",
    ").withColumn(\n",
    "    \"service_grade\",\n",
    "    F.when(F.col(\"quality_index\") >= 90, \"A\")\n",
    "     .when(F.col(\"quality_index\") >= 80, \"B\")\n",
    "     .when(F.col(\"quality_index\") >= 70, \"C\")\n",
    "     .when(F.col(\"quality_index\") >= 60, \"D\")\n",
    "     .otherwise(\"F\")\n",
    ")\n",
    "\n",
    "service_quality.write.mode(\"overwrite\").saveAsTable(SERVICE_QUALITY)\n",
    "print(f\"âœ… Service quality metrics table created: {SERVICE_QUALITY}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba9154c-893d-46ee-8307-9a95c892698b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 13 â€“ Peak Traffic Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nâš¡ Peak Traffic Analysis...\")\n",
    "\n",
    "# Identify peak hours and patterns\n",
    "peak_analysis = traffic_hourly.withColumn(\n",
    "    \"is_peak_hour\",\n",
    "    F.when(F.col(\"hourly_calls\") > F.col(\"avg_daily_calls\") * 1.5, 1).otherwise(0)\n",
    ").filter(F.col(\"is_peak_hour\") == 1).groupBy(\"call_hour\").agg(\n",
    "    F.count(\"*\").alias(\"peak_occurrences\"),\n",
    "    F.avg(\"hourly_calls\").alias(\"avg_peak_calls\"),\n",
    "    F.max(\"hourly_calls\").alias(\"max_peak_calls\"),\n",
    "    F.avg(\"success_rate\").alias(\"avg_peak_success_rate\"),\n",
    "    F.avg(\"hourly_revenue\").alias(\"avg_peak_revenue\")\n",
    ").withColumn(\n",
    "    \"peak_frequency\", \n",
    "    F.round(F.col(\"peak_occurrences\") / 30 * 100, 2)  # Percentage of days this hour is peak\n",
    ").orderBy(F.desc(\"peak_frequency\"))\n",
    "\n",
    "peak_analysis.write.mode(\"overwrite\").saveAsTable(PEAK_ANALYSIS)\n",
    "print(f\"âœ… Peak traffic analysis table created: {PEAK_ANALYSIS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743555e4-29bc-473e-9397-855e307434c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 14 â€“ Create Advanced Views for BI & Visualization\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ” Creating Advanced BI Views...\")\n",
    "\n",
    "# Executive dashboard view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_executive_dashboard AS\n",
    "SELECT \n",
    "    CDR_DAY,\n",
    "    total_records,\n",
    "    unique_subscribers,\n",
    "    ROUND(total_revenue, 2) as daily_revenue,\n",
    "    ROUND(arpu, 2) as daily_arpu,\n",
    "    success_rate,\n",
    "    is_weekend,\n",
    "    is_holiday,\n",
    "    holiday_name,\n",
    "    ROUND(calls_7d_avg, 0) as weekly_avg_calls,\n",
    "    trend_direction\n",
    "FROM {TRAFFIC_DAILY} d\n",
    "JOIN {ROLLING_KPIS} r ON d.CDR_DAY = r.CDR_DAY\n",
    "ORDER BY CDR_DAY DESC\n",
    "\"\"\")\n",
    "\n",
    "# Network health monitoring view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_network_health AS\n",
    "SELECT \n",
    "    c.CallingCellID,\n",
    "    c.cell_health_score,\n",
    "    c.cell_category,\n",
    "    c.avg_failure_rate,\n",
    "    c.reliability_score,\n",
    "    c.stability_score,\n",
    "    c.avg_daily_calls,\n",
    "    c.total_cell_revenue\n",
    "FROM {CELL_HEALTH_SCORE} c\n",
    "ORDER BY cell_health_score ASC\n",
    "\"\"\")\n",
    "\n",
    "# Subscriber insights view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_subscriber_insights AS\n",
    "SELECT \n",
    "    usage_segment,\n",
    "    revenue_segment,\n",
    "    COUNT(*) as subscriber_count,\n",
    "    ROUND(AVG(total_calls), 2) as avg_calls,\n",
    "    ROUND(AVG(total_revenue), 2) as avg_revenue,\n",
    "    ROUND(AVG(daily_arpu), 2) as avg_daily_arpu,\n",
    "    ROUND(AVG(success_rate), 2) as avg_success_rate\n",
    "FROM {SUBSCRIBER_BEHAVIOR}\n",
    "GROUP BY usage_segment, revenue_segment\n",
    "ORDER BY subscriber_count DESC\n",
    "\"\"\")\n",
    "\n",
    "# Peak hours view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_peak_hours_analysis AS\n",
    "SELECT \n",
    "    call_hour,\n",
    "    peak_frequency as peak_frequency_pct,\n",
    "    ROUND(avg_peak_calls, 0) as avg_calls_during_peak,\n",
    "    ROUND(max_peak_calls, 0) as max_calls_recorded,\n",
    "    ROUND(avg_peak_success_rate, 2) as success_rate_during_peak,\n",
    "    ROUND(avg_peak_revenue, 2) as avg_revenue_during_peak\n",
    "FROM {PEAK_ANALYSIS}\n",
    "ORDER BY call_hour\n",
    "\"\"\")\n",
    "\n",
    "# Yennayer special analysis view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_yennayer_impact AS\n",
    "SELECT \n",
    "    CallingCellID,\n",
    "    yennayer_period,\n",
    "    calls,\n",
    "    unique_users,\n",
    "    ROUND(revenue, 2) as revenue,\n",
    "    ROUND(avg_duration, 2) as avg_call_duration,\n",
    "    evening_call_ratio\n",
    "FROM {YENNAYER_ANALYSIS}\n",
    "WHERE calls > 100\n",
    "ORDER BY calls DESC\n",
    "\"\"\")\n",
    "\n",
    "# Service quality view\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_service_quality AS\n",
    "SELECT \n",
    "    CDR_DAY,\n",
    "    time_period,\n",
    "    quality_index,\n",
    "    service_grade,\n",
    "    total_calls,\n",
    "    ROUND(avg_completion_rate * 100, 2) as completion_rate_pct,\n",
    "    poor_quality_calls,\n",
    "    excellent_quality_calls\n",
    "FROM {SERVICE_QUALITY}\n",
    "ORDER BY CDR_DAY DESC, quality_index DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… Created advanced views:\")\n",
    "print(\"   - v_executive_dashboard\")\n",
    "print(\"   - v_network_health\")\n",
    "print(\"   - v_subscriber_insights\")\n",
    "print(\"   - v_peak_hours_analysis\")\n",
    "print(\"   - v_yennayer_impact\")\n",
    "print(\"   - v_service_quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d920053-b7b0-41b8-b4c4-8df0d43f12a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 15 â€“ Summary & Verification\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š ADVANCED DATA ENGINEERING PROCESSING COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Show tables created\n",
    "print(\"\\nðŸ“‹ Tables Created:\")\n",
    "all_tables = spark.sql(\"SHOW TABLES\").filter(F.col(\"isTemporary\") == False).collect()\n",
    "for table in all_tables:\n",
    "    if table.tableName not in ['cdr_anonymized', 'cdr_daily_summary', 'cdr_network_metrics']:\n",
    "        row_count = spark.table(table.tableName).count()\n",
    "        print(f\"   - {table.tableName}: {row_count:,} rows\")\n",
    "\n",
    "# Sample executive dashboard\n",
    "print(\"\\nðŸ“ˆ Executive Dashboard Sample:\")\n",
    "spark.sql(\"SELECT * FROM v_executive_dashboard LIMIT 5\").show(truncate=False)\n",
    "\n",
    "# Network health summary\n",
    "print(\"\\nðŸ¥ Network Health Summary:\")\n",
    "spark.sql(\"\"\"\n",
    "SELECT cell_category, COUNT(*) as cell_count, ROUND(AVG(cell_health_score), 2) as avg_score\n",
    "FROM v_network_health\n",
    "GROUP BY cell_category\n",
    "ORDER BY avg_score DESC\n",
    "\"\"\").show()\n",
    "\n",
    "# Subscriber segmentation summary\n",
    "print(\"\\nðŸ‘¥ Subscriber Segmentation:\")\n",
    "spark.sql(\"SELECT * FROM v_subscriber_insights LIMIT 10\").show(truncate=False)\n",
    "\n",
    "print(\"\\nâœ… All data engineering transformations completed successfully!\")\n",
    "print(\"ðŸ“Š Total processing time:\", datetime.now())\n",
    "print(\"\\nðŸš€ Next Steps:\")\n",
    "print(\"   1. Run Notebook 04 for Advanced Anomaly Detection\")\n",
    "print(\"   2. Run Notebook 05 for BI Visualizations\")\n",
    "print(\"   3. Configure Superset/PowerBI dashboards with created views\")\n",
    "\n",
    "# Keep session active for next notebook\n",
    "print(\"\\nðŸ’¡ Spark session kept active for next notebook\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7100737a-0fd1-4e7d-904d-281e45ce7145",
   "metadata": {},
   "source": [
    "#### Generated CDR Part "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3a5a60-bbeb-458d-b802-2b28c4af18ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================================\n",
    "# NOTEBOOK 03: ADVANCED DATA ENGINEERING & TRANSFORMATIONS\n",
    "# Algerie Telecom Big Data Project - Feature Engineering\n",
    "# =====================================================\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark\n",
    "spark = init_spark(\"Data Engineering - Advanced Transformations\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ðŸ”§ ADVANCED DATA ENGINEERING & FEATURE EXTRACTION\")\n",
    "print(\"ðŸ“Š Algerie Telecom CDR Analytics Pipeline\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use the database\n",
    "spark.sql(\"USE algerie_telecom_gen\")\n",
    "\n",
    "# =====================================================\n",
    "# 1. CUSTOMER BEHAVIOR PROFILING\n",
    "# =====================================================\n",
    "print(\"\\nðŸ‘¤ BUILDING CUSTOMER BEHAVIOR PROFILES...\")\n",
    "\n",
    "# Create comprehensive customer profile\n",
    "customer_profile = spark.sql(\"\"\"\n",
    "    WITH customer_metrics AS (\n",
    "        SELECT \n",
    "            subscriber_id,\n",
    "            customer_segment,\n",
    "            age_group,\n",
    "            gender,\n",
    "            operator,\n",
    "            payment_type,\n",
    "            \n",
    "            -- Service usage patterns\n",
    "            COUNT(DISTINCT CASE WHEN service_type = 'VOICE' THEN cdr_id END) as voice_calls_count,\n",
    "            COUNT(DISTINCT CASE WHEN service_type = 'DATA' THEN cdr_id END) as data_sessions_count,\n",
    "            COUNT(DISTINCT CASE WHEN service_type = 'SMS' THEN cdr_id END) as sms_count,\n",
    "            \n",
    "            -- Duration metrics\n",
    "            SUM(CASE WHEN service_type = 'VOICE' THEN duration ELSE 0 END) as total_voice_duration,\n",
    "            AVG(CASE WHEN service_type = 'VOICE' THEN duration ELSE 0 END) as avg_voice_duration,\n",
    "            \n",
    "            -- Data usage metrics\n",
    "            SUM(data_volume_mb) as total_data_mb,\n",
    "            AVG(CASE WHEN service_type = 'DATA' THEN data_volume_mb ELSE NULL END) as avg_data_per_session,\n",
    "            \n",
    "            -- Financial metrics\n",
    "            SUM(charging_amount) as total_spending,\n",
    "            AVG(charging_amount) as avg_transaction_value,\n",
    "            SUM(promotional_discount) / COUNT(*) as avg_discount_rate,\n",
    "            \n",
    "            -- Quality metrics\n",
    "            AVG(quality_score) as avg_quality_score,\n",
    "            SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_calls,\n",
    "            \n",
    "            -- Temporal patterns\n",
    "            COUNT(DISTINCT DATE(start_time)) as active_days,\n",
    "            COUNT(DISTINCT CASE WHEN is_weekend THEN DATE(start_time) END) as weekend_days,\n",
    "            COUNT(DISTINCT CASE WHEN time_of_day_category = 'NIGHT' THEN DATE(start_time) END) as night_usage_days,\n",
    "            \n",
    "            -- Special features usage\n",
    "            COUNT(DISTINCT CASE WHEN special_offer_applied != 'None' THEN DATE(start_time) END) as offer_usage_days,\n",
    "            COUNT(DISTINCT CASE WHEN roaming_flag THEN cdr_id END) as roaming_events,\n",
    "            \n",
    "            -- Anomaly indicators\n",
    "            MAX(fraud_indicator) as has_fraud_flag,\n",
    "            SUM(CASE WHEN unusual_pattern_flag THEN 1 ELSE 0 END) as unusual_patterns_count,\n",
    "            \n",
    "            -- App preferences (for data users)\n",
    "            COUNT(DISTINCT application_used) as unique_apps_used,\n",
    "            MIN(start_time) as first_activity,\n",
    "            MAX(start_time) as last_activity\n",
    "            \n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month IN (1, 2, 3)  -- Q1 2025\n",
    "        GROUP BY subscriber_id, customer_segment, age_group, gender, operator, payment_type\n",
    "    )\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Derived metrics\n",
    "        DATEDIFF(last_activity, first_activity) + 1 as customer_lifetime_days,\n",
    "        total_spending / NULLIF(active_days, 0) as daily_avg_spending,\n",
    "        voice_calls_count / NULLIF(active_days, 0) as calls_per_day,\n",
    "        total_data_mb / NULLIF(active_days, 0) as data_mb_per_day,\n",
    "        \n",
    "        -- Customer value score\n",
    "        CASE \n",
    "            WHEN total_spending > 5000 AND active_days > 60 THEN 'Premium'\n",
    "            WHEN total_spending > 2000 AND active_days > 30 THEN 'High'\n",
    "            WHEN total_spending > 500 THEN 'Medium'\n",
    "            ELSE 'Low'\n",
    "        END as value_category,\n",
    "        \n",
    "        -- Usage pattern classification\n",
    "        CASE \n",
    "            WHEN voice_calls_count > data_sessions_count * 2 THEN 'Voice Heavy'\n",
    "            WHEN data_sessions_count > voice_calls_count * 2 THEN 'Data Heavy'\n",
    "            ELSE 'Balanced'\n",
    "        END as usage_pattern\n",
    "        \n",
    "    FROM customer_metrics\n",
    "\"\"\")\n",
    "\n",
    "# Cache for performance\n",
    "customer_profile.cache()\n",
    "customer_profile.createOrReplaceTempView(\"customer_profiles\")\n",
    "\n",
    "print(f\"âœ… Created customer profiles for {customer_profile.count():,} subscribers\")\n",
    "\n",
    "# Save as table\n",
    "customer_profile.write.mode(\"overwrite\").saveAsTable(\"customer_behavior_profiles\")\n",
    "\n",
    "# Show sample profile\n",
    "print(\"\\nðŸ“Š Sample Customer Profiles:\")\n",
    "customer_profile.select(\n",
    "    \"subscriber_id\", \"customer_segment\", \"operator\", \"value_category\", \n",
    "    \"usage_pattern\", \"total_spending\", \"active_days\"\n",
    ").show(5)\n",
    "\n",
    "# =====================================================\n",
    "# 2. NETWORK PERFORMANCE ENGINEERING\n",
    "# =====================================================\n",
    "print(\"\\nðŸ“¡ ENGINEERING NETWORK PERFORMANCE FEATURES...\")\n",
    "\n",
    "network_performance = spark.sql(\"\"\"\n",
    "    WITH network_metrics AS (\n",
    "        SELECT \n",
    "            location_area,\n",
    "            operator,\n",
    "            network_type,\n",
    "            cell_id,\n",
    "            HOUR(start_time) as hour_of_day,\n",
    "            \n",
    "            -- Performance metrics\n",
    "            COUNT(*) as total_transactions,\n",
    "            AVG(quality_score) as avg_quality_score,\n",
    "            STDDEV(quality_score) as quality_stddev,\n",
    "            AVG(signal_strength) as avg_signal_strength,\n",
    "            \n",
    "            -- Failure metrics\n",
    "            SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_calls,\n",
    "            SUM(CASE WHEN call_result = 'FAILED' THEN 1 ELSE 0 END) as failed_transactions,\n",
    "            SUM(CASE WHEN network_congestion_level = 'HIGH' THEN 1 ELSE 0 END) as high_congestion_events,\n",
    "            \n",
    "            -- Load metrics\n",
    "            COUNT(DISTINCT subscriber_id) as unique_users,\n",
    "            SUM(data_volume_mb) as total_data_load_mb,\n",
    "            SUM(duration) / 3600.0 as total_voice_hours\n",
    "            \n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month = 1\n",
    "        GROUP BY location_area, operator, network_type, cell_id, HOUR(start_time)\n",
    "    )\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Calculated KPIs\n",
    "        (dropped_calls + failed_transactions) / NULLIF(total_transactions, 0) * 100 as failure_rate,\n",
    "        high_congestion_events / NULLIF(total_transactions, 0) * 100 as congestion_rate,\n",
    "        total_data_load_mb / NULLIF(unique_users, 0) as data_per_user,\n",
    "        \n",
    "        -- Performance score (composite metric)\n",
    "        (avg_quality_score * 0.4 + \n",
    "         (100 - (dropped_calls + failed_transactions) / NULLIF(total_transactions, 0) * 100) / 100 * 0.3 +\n",
    "         avg_signal_strength / 100.0 * 0.3) as network_performance_score,\n",
    "         \n",
    "        -- Congestion classification\n",
    "        CASE \n",
    "            WHEN high_congestion_events / NULLIF(total_transactions, 0) > 0.2 THEN 'Critical'\n",
    "            WHEN high_congestion_events / NULLIF(total_transactions, 0) > 0.1 THEN 'High'\n",
    "            WHEN high_congestion_events / NULLIF(total_transactions, 0) > 0.05 THEN 'Medium'\n",
    "            ELSE 'Low'\n",
    "        END as congestion_severity\n",
    "        \n",
    "    FROM network_metrics\n",
    "\"\"\")\n",
    "\n",
    "network_performance.write.mode(\"overwrite\").saveAsTable(\"network_performance_features\")\n",
    "print(\"âœ… Network performance features engineered\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. REVENUE OPTIMIZATION FEATURES\n",
    "# =====================================================\n",
    "print(\"\\nðŸ’° CREATING REVENUE OPTIMIZATION FEATURES...\")\n",
    "\n",
    "revenue_features = spark.sql(\"\"\"\n",
    "    WITH revenue_base AS (\n",
    "        SELECT \n",
    "            subscriber_id,\n",
    "            customer_segment,\n",
    "            operator,\n",
    "            service_type,\n",
    "            special_offer_applied,\n",
    "            \n",
    "            -- Revenue metrics\n",
    "            SUM(charging_amount) as total_revenue,\n",
    "            AVG(charging_amount) as avg_revenue_per_transaction,\n",
    "            SUM(tax_amount) as total_tax,\n",
    "            \n",
    "            -- Discount analysis\n",
    "            AVG(promotional_discount) as avg_discount,\n",
    "            MAX(promotional_discount) as max_discount,\n",
    "            COUNT(CASE WHEN promotional_discount > 0 THEN 1 END) as discounted_transactions,\n",
    "            \n",
    "            -- Service-specific metrics\n",
    "            SUM(CASE WHEN service_type = 'DATA' THEN data_volume_mb ELSE 0 END) as total_data_mb,\n",
    "            SUM(CASE WHEN service_type = 'DATA' THEN charging_amount ELSE 0 END) as data_revenue,\n",
    "            SUM(CASE WHEN service_type = 'VOICE' THEN duration ELSE 0 END) as total_voice_seconds,\n",
    "            SUM(CASE WHEN service_type = 'VOICE' THEN charging_amount ELSE 0 END) as voice_revenue,\n",
    "            \n",
    "            COUNT(*) as transaction_count\n",
    "            \n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month IN (1, 2, 3)\n",
    "        GROUP BY subscriber_id, customer_segment, operator, service_type, special_offer_applied\n",
    "    )\n",
    "    SELECT \n",
    "        subscriber_id,\n",
    "        customer_segment,\n",
    "        operator,\n",
    "        \n",
    "        -- Aggregate across services\n",
    "        SUM(total_revenue) as total_customer_revenue,\n",
    "        SUM(total_tax) as total_customer_tax,\n",
    "        AVG(avg_revenue_per_transaction) as avg_transaction_value,\n",
    "        \n",
    "        -- Service mix\n",
    "        SUM(CASE WHEN service_type = 'VOICE' THEN total_revenue ELSE 0 END) / NULLIF(SUM(total_revenue), 0) as voice_revenue_ratio,\n",
    "        SUM(CASE WHEN service_type = 'DATA' THEN total_revenue ELSE 0 END) / NULLIF(SUM(total_revenue), 0) as data_revenue_ratio,\n",
    "        SUM(CASE WHEN service_type = 'SMS' THEN total_revenue ELSE 0 END) / NULLIF(SUM(total_revenue), 0) as sms_revenue_ratio,\n",
    "        \n",
    "        -- Discount effectiveness\n",
    "        SUM(discounted_transactions) / SUM(transaction_count) as discount_usage_rate,\n",
    "        AVG(avg_discount) as overall_avg_discount,\n",
    "        \n",
    "        -- ARPU components\n",
    "        SUM(data_revenue) / NULLIF(SUM(total_data_mb), 0) as revenue_per_mb,\n",
    "        SUM(voice_revenue) / NULLIF(SUM(total_voice_seconds), 0) * 60 as revenue_per_minute,\n",
    "        \n",
    "        -- Offer adoption\n",
    "        COUNT(DISTINCT CASE WHEN special_offer_applied != 'None' THEN special_offer_applied END) as unique_offers_used\n",
    "        \n",
    "    FROM revenue_base\n",
    "    GROUP BY subscriber_id, customer_segment, operator\n",
    "\"\"\")\n",
    "\n",
    "revenue_features.write.mode(\"overwrite\").saveAsTable(\"revenue_optimization_features\")\n",
    "print(\"âœ… Revenue optimization features created\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. TIME SERIES FEATURE ENGINEERING\n",
    "# =====================================================\n",
    "print(\"\\nðŸ“ˆ ENGINEERING TIME SERIES FEATURES...\")\n",
    "\n",
    "# Daily aggregations with lag features\n",
    "time_series_features = spark.sql(\"\"\"\n",
    "    WITH daily_metrics AS (\n",
    "        SELECT \n",
    "            DATE(start_time) as activity_date,\n",
    "            operator,\n",
    "            service_type,\n",
    "            COUNT(*) as daily_transactions,\n",
    "            COUNT(DISTINCT subscriber_id) as daily_active_users,\n",
    "            SUM(charging_amount) as daily_revenue,\n",
    "            AVG(quality_score) as daily_avg_quality,\n",
    "            SUM(CASE WHEN fraud_indicator THEN 1 ELSE 0 END) as daily_fraud_cases,\n",
    "            SUM(data_volume_mb) as daily_data_volume,\n",
    "            SUM(duration) / 3600.0 as daily_voice_hours\n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month IN (1, 2, 3)\n",
    "        GROUP BY DATE(start_time), operator, service_type\n",
    "    ),\n",
    "    with_lag_features AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            -- Lag features for trend analysis\n",
    "            LAG(daily_transactions, 1) OVER (PARTITION BY operator, service_type ORDER BY activity_date) as prev_day_transactions,\n",
    "            LAG(daily_transactions, 7) OVER (PARTITION BY operator, service_type ORDER BY activity_date) as prev_week_transactions,\n",
    "            LAG(daily_revenue, 1) OVER (PARTITION BY operator, service_type ORDER BY activity_date) as prev_day_revenue,\n",
    "            LAG(daily_revenue, 7) OVER (PARTITION BY operator, service_type ORDER BY activity_date) as prev_week_revenue,\n",
    "            \n",
    "            -- Moving averages\n",
    "            AVG(daily_transactions) OVER (\n",
    "                PARTITION BY operator, service_type \n",
    "                ORDER BY activity_date \n",
    "                ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "            ) as ma7_transactions,\n",
    "            \n",
    "            AVG(daily_revenue) OVER (\n",
    "                PARTITION BY operator, service_type \n",
    "                ORDER BY activity_date \n",
    "                ROWS BETWEEN 6 PRECEDING AND CURRENT ROW\n",
    "            ) as ma7_revenue\n",
    "            \n",
    "        FROM daily_metrics\n",
    "    )\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Growth rates\n",
    "        (daily_transactions - prev_day_transactions) / NULLIF(prev_day_transactions, 0) * 100 as daily_growth_rate,\n",
    "        (daily_transactions - prev_week_transactions) / NULLIF(prev_week_transactions, 0) * 100 as weekly_growth_rate,\n",
    "        \n",
    "        -- Trend indicators\n",
    "        CASE \n",
    "            WHEN daily_transactions > ma7_transactions * 1.2 THEN 'Spike'\n",
    "            WHEN daily_transactions < ma7_transactions * 0.8 THEN 'Drop'\n",
    "            ELSE 'Normal'\n",
    "        END as traffic_trend,\n",
    "        \n",
    "        -- Revenue per user\n",
    "        daily_revenue / NULLIF(daily_active_users, 0) as daily_arpu\n",
    "        \n",
    "    FROM with_lag_features\n",
    "\"\"\")\n",
    "\n",
    "time_series_features.write.mode(\"overwrite\").saveAsTable(\"time_series_features\")\n",
    "print(\"âœ… Time series features engineered\")\n",
    "\n",
    "# =====================================================\n",
    "# 5. CUSTOMER CHURN RISK INDICATORS\n",
    "# =====================================================\n",
    "print(\"\\nðŸš¨ BUILDING CHURN RISK INDICATORS...\")\n",
    "\n",
    "churn_indicators = spark.sql(\"\"\"\n",
    "    WITH activity_timeline AS (\n",
    "        SELECT \n",
    "            subscriber_id,\n",
    "            DATE(start_time) as activity_date,\n",
    "            COUNT(*) as daily_activities\n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month IN (1, 2, 3)\n",
    "        GROUP BY subscriber_id, DATE(start_time)\n",
    "    ),\n",
    "    activity_gaps AS (\n",
    "        SELECT \n",
    "            subscriber_id,\n",
    "            activity_date,\n",
    "            LAG(activity_date, 1) OVER (PARTITION BY subscriber_id ORDER BY activity_date) as prev_activity_date,\n",
    "            DATEDIFF(activity_date, LAG(activity_date, 1) OVER (PARTITION BY subscriber_id ORDER BY activity_date)) as days_since_last_activity\n",
    "        FROM activity_timeline\n",
    "    ),\n",
    "    churn_metrics AS (\n",
    "        SELECT \n",
    "            ag.subscriber_id,\n",
    "            COUNT(*) as total_active_days,\n",
    "            MAX(days_since_last_activity) as max_inactivity_gap,\n",
    "            AVG(days_since_last_activity) as avg_days_between_activities,\n",
    "            STDDEV(days_since_last_activity) as activity_regularity,\n",
    "            MAX(activity_date) as last_activity_date,\n",
    "            DATEDIFF('2025-03-31', MAX(activity_date)) as days_since_last_seen\n",
    "        FROM activity_gaps ag\n",
    "        GROUP BY ag.subscriber_id\n",
    "    )\n",
    "    SELECT \n",
    "        cm.*,\n",
    "        cp.customer_segment,\n",
    "        cp.total_spending,\n",
    "        cp.avg_transaction_value,\n",
    "        cp.active_days,\n",
    "        \n",
    "        -- Churn risk score components\n",
    "        CASE \n",
    "            WHEN days_since_last_seen > 30 THEN 1.0\n",
    "            WHEN days_since_last_seen > 14 THEN 0.7\n",
    "            WHEN days_since_last_seen > 7 THEN 0.4\n",
    "            ELSE 0.1\n",
    "        END as recency_risk,\n",
    "        \n",
    "        CASE \n",
    "            WHEN total_active_days < 10 THEN 0.8\n",
    "            WHEN total_active_days < 30 THEN 0.5\n",
    "            ELSE 0.2\n",
    "        END as frequency_risk,\n",
    "        \n",
    "        CASE \n",
    "            WHEN total_spending < 100 THEN 0.8\n",
    "            WHEN total_spending < 500 THEN 0.5\n",
    "            ELSE 0.2\n",
    "        END as monetary_risk,\n",
    "        \n",
    "        -- Overall churn risk\n",
    "        (CASE \n",
    "            WHEN days_since_last_seen > 30 THEN 1.0\n",
    "            WHEN days_since_last_seen > 14 THEN 0.7\n",
    "            WHEN days_since_last_seen > 7 THEN 0.4\n",
    "            ELSE 0.1\n",
    "        END * 0.5 +\n",
    "        CASE \n",
    "            WHEN total_active_days < 10 THEN 0.8\n",
    "            WHEN total_active_days < 30 THEN 0.5\n",
    "            ELSE 0.2\n",
    "        END * 0.3 +\n",
    "        CASE \n",
    "            WHEN total_spending < 100 THEN 0.8\n",
    "            WHEN total_spending < 500 THEN 0.5\n",
    "            ELSE 0.2\n",
    "        END * 0.2) as churn_risk_score\n",
    "        \n",
    "    FROM churn_metrics cm\n",
    "    JOIN customer_behavior_profiles cp ON cm.subscriber_id = cp.subscriber_id\n",
    "\"\"\")\n",
    "\n",
    "churn_indicators.write.mode(\"overwrite\").saveAsTable(\"customer_churn_indicators\")\n",
    "print(\"âœ… Churn risk indicators calculated\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. FRAUD PATTERN DETECTION FEATURES\n",
    "# =====================================================\n",
    "print(\"\\nðŸ” ENGINEERING FRAUD DETECTION FEATURES...\")\n",
    "\n",
    "fraud_features = spark.sql(\"\"\"\n",
    "    WITH subscriber_patterns AS (\n",
    "        SELECT \n",
    "            subscriber_id,\n",
    "            -- Normal behavior baseline\n",
    "            AVG(CASE WHEN fraud_indicator = false THEN charging_amount END) as normal_avg_charge,\n",
    "            STDDEV(CASE WHEN fraud_indicator = false THEN charging_amount END) as normal_charge_stddev,\n",
    "            AVG(CASE WHEN fraud_indicator = false AND service_type = 'VOICE' THEN duration END) as normal_avg_duration,\n",
    "            \n",
    "            -- Anomaly counts\n",
    "            SUM(CASE WHEN fraud_indicator THEN 1 ELSE 0 END) as fraud_flag_count,\n",
    "            SUM(CASE WHEN unusual_pattern_flag THEN 1 ELSE 0 END) as unusual_pattern_count,\n",
    "            \n",
    "            -- High-risk behaviors\n",
    "            COUNT(CASE WHEN roaming_flag AND charging_amount > 500 THEN 1 END) as high_value_roaming,\n",
    "            COUNT(CASE WHEN HOUR(start_time) BETWEEN 2 AND 5 THEN 1 END) as night_activities,\n",
    "            COUNT(CASE WHEN service_subtype = 'INTERNATIONAL_CALL' THEN 1 END) as international_calls,\n",
    "            \n",
    "            -- Velocity features\n",
    "            COUNT(DISTINCT DATE(start_time)) as active_days,\n",
    "            COUNT(*) as total_transactions,\n",
    "            MAX(charging_amount) as max_single_charge\n",
    "            \n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month IN (1, 2, 3)\n",
    "        GROUP BY subscriber_id\n",
    "    ),\n",
    "    hourly_velocity AS (\n",
    "        SELECT \n",
    "            subscriber_id,\n",
    "            DATE(start_time) as activity_date,\n",
    "            HOUR(start_time) as activity_hour,\n",
    "            COUNT(*) as hourly_transactions,\n",
    "            SUM(charging_amount) as hourly_spending\n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month IN (1, 2, 3)\n",
    "        GROUP BY subscriber_id, DATE(start_time), HOUR(start_time)\n",
    "    ),\n",
    "    velocity_features AS (\n",
    "        SELECT \n",
    "            subscriber_id,\n",
    "            MAX(hourly_transactions) as max_hourly_transactions,\n",
    "            MAX(hourly_spending) as max_hourly_spending,\n",
    "            AVG(hourly_transactions) as avg_hourly_transactions\n",
    "        FROM hourly_velocity\n",
    "        GROUP BY subscriber_id\n",
    "    )\n",
    "    SELECT \n",
    "        sp.*,\n",
    "        vf.max_hourly_transactions,\n",
    "        vf.max_hourly_spending,\n",
    "        vf.avg_hourly_transactions,\n",
    "        \n",
    "        -- Risk indicators\n",
    "        CASE \n",
    "            WHEN max_single_charge > normal_avg_charge + 3 * normal_charge_stddev THEN 1\n",
    "            ELSE 0\n",
    "        END as charge_anomaly_flag,\n",
    "        \n",
    "        CASE \n",
    "            WHEN max_hourly_transactions > 50 THEN 1\n",
    "            ELSE 0\n",
    "        END as velocity_anomaly_flag,\n",
    "        \n",
    "        -- Fraud risk score\n",
    "        (fraud_flag_count * 0.4 +\n",
    "         unusual_pattern_count * 0.2 +\n",
    "         CASE WHEN max_single_charge > normal_avg_charge + 3 * normal_charge_stddev THEN 10 ELSE 0 END * 0.2 +\n",
    "         CASE WHEN max_hourly_transactions > 50 THEN 10 ELSE 0 END * 0.1 +\n",
    "         CASE WHEN night_activities > total_transactions * 0.3 THEN 10 ELSE 0 END * 0.1\n",
    "        ) as fraud_risk_score\n",
    "        \n",
    "    FROM subscriber_patterns sp\n",
    "    JOIN velocity_features vf ON sp.subscriber_id = vf.subscriber_id\n",
    "\"\"\")\n",
    "\n",
    "fraud_features.write.mode(\"overwrite\").saveAsTable(\"fraud_detection_features\")\n",
    "print(\"âœ… Fraud detection features created\")\n",
    "\n",
    "# =====================================================\n",
    "# 7. CAMPAIGN EFFECTIVENESS FEATURES\n",
    "# =====================================================\n",
    "print(\"\\nðŸ“¢ ANALYZING CAMPAIGN EFFECTIVENESS...\")\n",
    "\n",
    "campaign_features = spark.sql(\"\"\"\n",
    "    WITH offer_adoption AS (\n",
    "        SELECT \n",
    "            special_offer_applied,\n",
    "            customer_segment,\n",
    "            age_group,\n",
    "            operator,\n",
    "            COUNT(DISTINCT subscriber_id) as unique_adopters,\n",
    "            COUNT(*) as total_usage_count,\n",
    "            SUM(charging_amount) as revenue_with_offer,\n",
    "            AVG(promotional_discount) as avg_discount_given,\n",
    "            SUM(charging_amount * promotional_discount / 100) as total_discount_amount\n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month IN (1, 2, 3)\n",
    "          AND special_offer_applied != 'None'\n",
    "        GROUP BY special_offer_applied, customer_segment, age_group, operator\n",
    "    ),\n",
    "    baseline_revenue AS (\n",
    "        SELECT \n",
    "            customer_segment,\n",
    "            age_group,\n",
    "            operator,\n",
    "            AVG(charging_amount) as baseline_avg_charge\n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month IN (1, 2, 3)\n",
    "          AND special_offer_applied = 'None'\n",
    "        GROUP BY customer_segment, age_group, operator\n",
    "    )\n",
    "    SELECT \n",
    "        oa.*,\n",
    "        br.baseline_avg_charge,\n",
    "        \n",
    "        -- Campaign metrics\n",
    "        unique_adopters / total_usage_count as adoption_rate,\n",
    "        revenue_with_offer / NULLIF(unique_adopters, 0) as revenue_per_adopter,\n",
    "        total_discount_amount / NULLIF(revenue_with_offer, 0) * 100 as discount_cost_ratio,\n",
    "        \n",
    "        -- Effectiveness score\n",
    "        CASE \n",
    "            WHEN revenue_with_offer / NULLIF(unique_adopters, 0) > br.baseline_avg_charge * 1.2 THEN 'Highly Effective'\n",
    "            WHEN revenue_with_offer / NULLIF(unique_adopters, 0) > br.baseline_avg_charge THEN 'Effective'\n",
    "            ELSE 'Needs Improvement'\n",
    "        END as campaign_effectiveness\n",
    "        \n",
    "    FROM offer_adoption oa\n",
    "    LEFT JOIN baseline_revenue br \n",
    "        ON oa.customer_segment = br.customer_segment \n",
    "        AND oa.age_group = br.age_group\n",
    "        AND oa.operator = br.operator\n",
    "\"\"\")\n",
    "\n",
    "campaign_features.write.mode(\"overwrite\").saveAsTable(\"campaign_effectiveness_features\")\n",
    "print(\"âœ… Campaign effectiveness features analyzed\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. NETWORK CAPACITY PLANNING FEATURES\n",
    "# =====================================================\n",
    "print(\"\\nðŸ“Š CREATING NETWORK CAPACITY PLANNING FEATURES...\")\n",
    "\n",
    "capacity_planning = spark.sql(\"\"\"\n",
    "    WITH hourly_load AS (\n",
    "        SELECT \n",
    "            location_area,\n",
    "            cell_id,\n",
    "            operator,\n",
    "            network_type,\n",
    "            DATE(start_time) as load_date,\n",
    "            HOUR(start_time) as load_hour,\n",
    "            \n",
    "            COUNT(*) as concurrent_sessions,\n",
    "            COUNT(DISTINCT subscriber_id) as concurrent_users,\n",
    "            SUM(CASE WHEN service_type = 'VOICE' THEN 1 ELSE 0 END) as voice_sessions,\n",
    "            SUM(CASE WHEN service_type = 'DATA' THEN 1 ELSE 0 END) as data_sessions,\n",
    "            SUM(data_volume_mb) as hourly_data_mb,\n",
    "            AVG(quality_score) as avg_quality,\n",
    "            SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_sessions\n",
    "            \n",
    "        FROM cdr_partitioned\n",
    "        WHERE year = 2025 AND month = 1\n",
    "        GROUP BY location_area, cell_id, operator, network_type, DATE(start_time), HOUR(start_time)\n",
    "    ),\n",
    "    cell_capacity_metrics AS (\n",
    "        SELECT \n",
    "            location_area,\n",
    "            cell_id,\n",
    "            operator,\n",
    "            network_type,\n",
    "            \n",
    "            -- Peak load metrics\n",
    "            MAX(concurrent_sessions) as peak_concurrent_sessions,\n",
    "            MAX(concurrent_users) as peak_concurrent_users,\n",
    "            MAX(hourly_data_mb) as peak_hourly_data_mb,\n",
    "            \n",
    "            -- Average load\n",
    "            AVG(concurrent_sessions) as avg_concurrent_sessions,\n",
    "            AVG(hourly_data_mb) as avg_hourly_data_mb,\n",
    "            \n",
    "            -- Quality under load\n",
    "            MIN(avg_quality) as min_quality_score,\n",
    "            AVG(CASE WHEN concurrent_sessions > AVG(concurrent_sessions) * 1.5 THEN avg_quality END) as quality_under_peak_load,\n",
    "            \n",
    "            -- Capacity utilization (assuming theoretical max)\n",
    "            MAX(concurrent_sessions) / 1000.0 * 100 as estimated_capacity_utilization,\n",
    "            \n",
    "            -- Failure rate under load\n",
    "            SUM(dropped_sessions) / SUM(concurrent_sessions) * 100 as overall_drop_rate\n",
    "            \n",
    "        FROM hourly_load\n",
    "        GROUP BY location_area, cell_id, operator, network_type\n",
    "    )\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Capacity planning recommendations\n",
    "        CASE \n",
    "            WHEN estimated_capacity_utilization > 80 THEN 'Urgent Upgrade Required'\n",
    "            WHEN estimated_capacity_utilization > 60 THEN 'Plan Upgrade'\n",
    "            WHEN estimated_capacity_utilization > 40 THEN 'Monitor'\n",
    "            ELSE 'Adequate'\n",
    "        END as capacity_recommendation,\n",
    "        \n",
    "        -- Load balancing score\n",
    "        (100 - estimated_capacity_utilization) * 0.4 +\n",
    "        min_quality_score * 100 * 0.3 +\n",
    "        (100 - overall_drop_rate) * 0.3 as cell_health_score\n",
    "        \n",
    "    FROM cell_capacity_metrics\n",
    "\"\"\")\n",
    "\n",
    "capacity_planning.write.mode(\"overwrite\").saveAsTable(\"network_capacity_planning\")\n",
    "print(\"âœ… Network capacity planning features created\")\n",
    "\n",
    "# =====================================================\n",
    "# 9. FEATURE STORE SUMMARY\n",
    "# =====================================================\n",
    "print(\"\\nðŸ“š CREATING UNIFIED FEATURE STORE...\")\n",
    "\n",
    "# Create a master feature table joining key features\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TABLE feature_store AS\n",
    "    SELECT \n",
    "        cbp.subscriber_id,\n",
    "        cbp.customer_segment,\n",
    "        cbp.age_group,\n",
    "        cbp.gender,\n",
    "        cbp.operator,\n",
    "        cbp.payment_type,\n",
    "        cbp.value_category,\n",
    "        cbp.usage_pattern,\n",
    "        cbp.total_spending,\n",
    "        cbp.active_days,\n",
    "        cbp.daily_avg_spending,\n",
    "        \n",
    "        -- Revenue features\n",
    "        rof.total_customer_revenue,\n",
    "        rof.voice_revenue_ratio,\n",
    "        rof.data_revenue_ratio,\n",
    "        rof.discount_usage_rate,\n",
    "        \n",
    "        -- Churn indicators\n",
    "        cci.days_since_last_seen,\n",
    "        cci.churn_risk_score,\n",
    "        \n",
    "        -- Fraud indicators\n",
    "        fdf.fraud_risk_score,\n",
    "        fdf.fraud_flag_count,\n",
    "        fdf.unusual_pattern_count,\n",
    "        \n",
    "        -- Current timestamp for versioning\n",
    "        CURRENT_TIMESTAMP() as feature_timestamp\n",
    "        \n",
    "    FROM customer_behavior_profiles cbp\n",
    "    LEFT JOIN revenue_optimization_features rof ON cbp.subscriber_id = rof.subscriber_id\n",
    "    LEFT JOIN customer_churn_indicators cci ON cbp.subscriber_id = cci.subscriber_id\n",
    "    LEFT JOIN fraud_detection_features fdf ON cbp.subscriber_id = fdf.subscriber_id\n",
    "\"\"\")\n",
    "\n",
    "print(\"âœ… Unified feature store created\")\n",
    "\n",
    "# =====================================================\n",
    "# 10. FEATURE STATISTICS & QUALITY CHECK\n",
    "# =====================================================\n",
    "print(\"\\nðŸ“Š FEATURE STATISTICS AND QUALITY REPORT...\")\n",
    "\n",
    "# Feature completeness check\n",
    "feature_quality = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_records,\n",
    "        COUNT(subscriber_id) / COUNT(*) * 100 as subscriber_id_completeness,\n",
    "        COUNT(total_spending) / COUNT(*) * 100 as spending_completeness,\n",
    "        COUNT(churn_risk_score) / COUNT(*) * 100 as churn_score_completeness,\n",
    "        COUNT(fraud_risk_score) / COUNT(*) * 100 as fraud_score_completeness,\n",
    "        \n",
    "        AVG(total_spending) as avg_customer_spending,\n",
    "        STDDEV(total_spending) as stddev_customer_spending,\n",
    "        \n",
    "        AVG(churn_risk_score) as avg_churn_risk,\n",
    "        AVG(fraud_risk_score) as avg_fraud_risk,\n",
    "        \n",
    "        COUNT(DISTINCT customer_segment) as unique_segments,\n",
    "        COUNT(DISTINCT operator) as unique_operators\n",
    "        \n",
    "    FROM feature_store\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(\"\\nðŸ“ˆ Feature Store Quality Report:\")\n",
    "print(f\"   Total customer records: {feature_quality['total_records']:,}\")\n",
    "print(f\"   Average customer spending: {feature_quality['avg_customer_spending']:.2f} DZD\")\n",
    "print(f\"   Average churn risk: {feature_quality['avg_churn_risk']:.2%}\")\n",
    "print(f\"   Average fraud risk: {feature_quality['avg_fraud_risk']:.2f}\")\n",
    "\n",
    "# Show feature distributions\n",
    "print(\"\\nðŸ“Š Customer Segment Distribution:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        customer_segment,\n",
    "        COUNT(*) as customer_count,\n",
    "        ROUND(AVG(total_spending), 2) as avg_spending,\n",
    "        ROUND(AVG(churn_risk_score), 3) as avg_churn_risk\n",
    "    FROM feature_store\n",
    "    GROUP BY customer_segment\n",
    "    ORDER BY customer_count DESC\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\nðŸ“Š Value Category Distribution:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        value_category,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER(), 2) as percentage\n",
    "    FROM feature_store\n",
    "    GROUP BY value_category\n",
    "    ORDER BY \n",
    "        CASE value_category \n",
    "            WHEN 'Premium' THEN 1 \n",
    "            WHEN 'High' THEN 2 \n",
    "            WHEN 'Medium' THEN 3 \n",
    "            ELSE 4 \n",
    "        END\n",
    "\"\"\").show()\n",
    "\n",
    "# =====================================================\n",
    "# 11. EXPORT KEY METRICS FOR VISUALIZATION\n",
    "# =====================================================\n",
    "print(\"\\nðŸ“¤ EXPORTING KEY METRICS FOR VISUALIZATION...\")\n",
    "\n",
    "# Daily trends for visualization\n",
    "daily_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE(start_time) as date,\n",
    "        operator,\n",
    "        COUNT(*) as transactions,\n",
    "        COUNT(DISTINCT subscriber_id) as active_users,\n",
    "        SUM(charging_amount) as revenue,\n",
    "        AVG(quality_score) as avg_quality\n",
    "    FROM cdr_partitioned\n",
    "    WHERE year = 2025 AND month IN (1, 2)\n",
    "    GROUP BY DATE(start_time), operator\n",
    "    ORDER BY date, operator\n",
    "\"\"\").toPandas()\n",
    "\n",
    "# Save for later visualization\n",
    "daily_trends.to_csv(\"/tmp/daily_trends.csv\", index=False)\n",
    "print(\"âœ… Daily trends exported\")\n",
    "\n",
    "# =====================================================\n",
    "# SUMMARY\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… DATA ENGINEERING PIPELINE COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Feature Engineering Summary:\")\n",
    "print(f\"   â€¢ Customer Behavior Profiles: {customer_profile.count():,} profiles\")\n",
    "print(f\"   â€¢ Network Performance Features: Analyzed across {network_performance.select('location_area').distinct().count()} locations\")\n",
    "print(f\"   â€¢ Revenue Optimization Features: Created for {revenue_features.count():,} customers\")\n",
    "print(f\"   â€¢ Time Series Features: {time_series_features.count():,} daily records\")\n",
    "print(f\"   â€¢ Churn Risk Indicators: Calculated for {churn_indicators.count():,} subscribers\")\n",
    "print(f\"   â€¢ Fraud Detection Features: {fraud_features.count():,} subscriber patterns analyzed\")\n",
    "print(f\"   â€¢ Campaign Effectiveness: Analyzed {campaign_features.select('special_offer_applied').distinct().count()} campaigns\")\n",
    "print(f\"   â€¢ Network Capacity Planning: {capacity_planning.count():,} cell tower metrics\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Next Steps:\")\n",
    "print(\"   â†’ Run Notebook 04 for Advanced Analytics & Anomaly Detection\")\n",
    "print(\"   â†’ Run Notebook 05 for Business Intelligence Dashboards\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nðŸ”š Spark session closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
