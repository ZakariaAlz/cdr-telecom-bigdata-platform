{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546f2a51-8ae8-46d1-bca2-20117ce5cec7",
   "metadata": {},
   "source": [
    "# Notebook 02: Complete Hive Hourly Table & Feature Engineering\n",
    "## CDR Telecom - New Year's Eve (Transition from 31 December to 01 January) Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5051d0-2329-47d3-a047-67651386ad74",
   "metadata": {},
   "source": [
    "# Focus: Creating hourly aggregations and features for Dec 31 - Jan 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c5406c-e782-4332-8b65-7d75250016e5",
   "metadata": {},
   "source": [
    "\n",
    "# Cell 1: Setup and Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfe5f12f-2ddf-4b50-856c-8af060d0cd6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/29 04:18:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SparkSession initialized (App: CDR Hourly Tables & Feature Engineering, Spark: 3.5.1)\n",
      "âœ… Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "âœ… Hive Metastore URI: thrift://hive-metastore:9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 04:18:36 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Using database: algerie_telecom_cdr\n",
      "ðŸ“Š Creating advanced hourly analytics tables\n",
      "\n",
      "â° CREATING COMPREHENSIVE HOURLY AGGREGATION TABLE\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 04:18:37 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "25/06/29 04:18:44 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created hourly aggregation table: cdr_hourly_aggregated\n",
      "\n",
      "ðŸ“Š Sample Hourly Data:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+-------------+-----------+------------+------------+---------------+----------------+------------+----------------------+-----------------+------------------+------------+------------+---------------+------------+---------------+-----------------+-----------------+-------------------+-------------+--------------------+----------+----------+-----------------+-----------+---------+-------------+-----------+--------------+-------------------+-------------+---------------+-------------------+-------------------+------------+------------+------------------+-----------+---------------+\n",
      "|CDR_DAY   |call_hour|hour_key     |total_calls|unique_users|active_cells|unique_sessions|successful_calls|failed_calls|total_duration_seconds|avg_duration     |stddev_duration   |min_duration|max_duration|median_duration|p95_duration|short_calls_30s|medium_calls_2min|normal_calls_5min|long_calls_over5min|total_revenue|avg_revenue_per_call|paid_calls|free_calls|max_single_charge|voice_calls|sms_count|data_sessions|local_calls|national_calls|international_calls|roaming_calls|forwarded_calls|first_call_time    |last_call_time     |success_rate|failure_rate|avg_calls_per_user|hourly_arpu|paid_call_ratio|\n",
      "+----------+---------+-------------+-----------+------------+------------+---------------+----------------+------------+----------------------+-----------------+------------------+------------+------------+---------------+------------+---------------+-----------------+-----------------+-------------------+-------------+--------------------+----------+----------+-----------------+-----------+---------+-------------+-----------+--------------+-------------------+-------------+---------------+-------------------+-------------------+------------+------------+------------------+-----------+---------------+\n",
      "|2024-12-31|21       |2024-12-31_21|30         |30          |0           |30             |30              |0           |87210.0               |2907.0           |927.644291220769  |614.0       |3601.0      |3443.0         |3601.0      |0              |0                |0                |30                 |216000.0     |7200.0              |8         |22        |54900.0          |30         |0        |0            |25         |5             |0                  |0            |0              |2024-12-31 21:19:09|2024-12-31 21:58:48|100.0       |0.0         |1.0               |7200.0     |26.67          |\n",
      "|2024-12-31|22       |2024-12-31_22|1880       |1076        |1           |1880           |1877            |3           |276278.0              |146.9563829787234|415.99253024225317|0.0         |3601.0      |48.0           |544.0       |584            |984              |172              |140                |1928527.0    |1025.8122340425532  |1334      |546       |44280.0          |1880       |0        |0            |920        |932           |0                  |0            |0              |2024-12-31 22:00:06|2024-12-31 22:59:57|99.84       |0.16        |1.75              |1792.31    |70.96          |\n",
      "|2024-12-31|23       |2024-12-31_23|5271       |3116        |1           |5271           |5261            |10          |390561.0              |74.09618668184405|179.14410799543103|0.0         |3601.0      |53.0           |141.0       |1263           |3654             |251              |103                |1589915.0    |301.63441472206415  |1533      |3738      |52200.0          |5271       |0        |0            |1108       |4049          |0                  |0            |0              |2024-12-31 23:00:00|2024-12-31 23:59:59|99.81       |0.19        |1.69              |510.24     |29.08          |\n",
      "|2025-01-01|0        |2025-01-01_00|2032       |982         |1           |2032           |2026            |6           |130308.0              |64.12795275590551|168.43882854682647|0.0         |3601.0      |43.0           |153.0       |765            |1113             |121              |33                 |810174.0     |398.7076771653543   |881       |1151      |24400.0          |2032       |0        |0            |575        |1355          |0                  |0            |0              |2025-01-01 00:00:00|2025-01-01 00:59:53|99.7        |0.3         |2.07              |825.02     |43.36          |\n",
      "|2025-01-01|1        |2025-01-01_01|881        |440         |1           |881            |881             |0           |60363.0               |68.51645856980704|219.16665517402808|1.0         |3600.0      |29.0           |163.0       |451            |364              |43               |23                 |612753.0     |695.5198637911465   |498       |383       |54000.0          |881        |0        |0            |272        |503           |0                  |0            |0              |2025-01-01 01:00:02|2025-01-01 01:59:59|100.0       |0.0         |2.0               |1392.62    |56.53          |\n",
      "+----------+---------+-------------+-----------+------------+------------+---------------+----------------+------------+----------------------+-----------------+------------------+------------+------------+---------------+------------+---------------+-----------------+-----------------+-------------------+-------------+--------------------+----------+----------+-----------------+-----------+---------+-------------+-----------+--------------+-------------------+-------------+---------------+-------------------+-------------------+------------+------------+------------------+-----------+---------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "ðŸ• CREATING MINUTE-LEVEL TABLE FOR MIDNIGHT ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âœ… Created minute-level table: cdr_minute_aggregated\n",
      "\n",
      "ðŸŽŠ Midnight Spike Analysis:\n",
      "+----------+---------+-----------+----------------+----------------+--------------+----------------+------------+------------------+--------------+-------------------+\n",
      "|   CDR_DAY|call_hour|call_minute|      minute_key|calls_per_minute|unique_callers|successful_calls|failed_calls|      avg_duration|minute_revenue|          timestamp|\n",
      "+----------+---------+-----------+----------------+----------------+--------------+----------------+------------+------------------+--------------+-------------------+\n",
      "|2025-01-01|        0|          0|2025-01-01_00_00|              56|            55|              56|           0|           114.625|       31350.0|2025-01-01 00:00:00|\n",
      "|2025-01-01|        0|          1|2025-01-01_00_01|              60|            57|              60|           0|              59.4|       18300.0|2025-01-01 00:01:00|\n",
      "|2025-01-01|        0|          2|2025-01-01_00_02|              59|            56|              59|           0|114.89830508474576|       27440.0|2025-01-01 00:02:00|\n",
      "|2025-01-01|        0|          3|2025-01-01_00_03|              49|            46|              49|           0| 56.30612244897959|        4950.0|2025-01-01 00:03:00|\n",
      "|2025-01-01|        0|          4|2025-01-01_00_04|              58|            53|              58|           0| 77.70689655172414|       11680.0|2025-01-01 00:04:00|\n",
      "+----------+---------+-----------+----------------+----------------+--------------+----------------+------------+------------------+--------------+-------------------+\n",
      "\n",
      "\n",
      "ðŸ”§ ADVANCED FEATURE ENGINEERING FOR 2-DAY ANALYSIS\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(((call_hour >= 0) AND (call_hour <= 5)) OR call_hour)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"BOOLEAN\" and \"INT\").;\n'Project [CDR_DAY#2663, call_hour#2664, hour_key#2665, total_calls#2666L, unique_users#2667L, active_cells#2668L, unique_sessions#2669L, successful_calls#2670L, failed_calls#2671L, total_duration_seconds#2672, avg_duration#2673, stddev_duration#2674, min_duration#2675, max_duration#2676, median_duration#2677, p95_duration#2678, short_calls_30s#2679L, medium_calls_2min#2680L, normal_calls_5min#2681L, long_calls_over5min#2682L, total_revenue#2683, avg_revenue_per_call#2684, paid_calls#2685L, free_calls#2686L, ... 18 more fields]\n+- Project [CDR_DAY#2663, call_hour#2664, hour_key#2665, total_calls#2666L, unique_users#2667L, active_cells#2668L, unique_sessions#2669L, successful_calls#2670L, failed_calls#2671L, total_duration_seconds#2672, avg_duration#2673, stddev_duration#2674, min_duration#2675, max_duration#2676, median_duration#2677, p95_duration#2678, short_calls_30s#2679L, medium_calls_2min#2680L, normal_calls_5min#2681L, long_calls_over5min#2682L, total_revenue#2683, avg_revenue_per_call#2684, paid_calls#2685L, free_calls#2686L, ... 17 more fields]\n   +- SubqueryAlias spark_catalog.algerie_telecom_cdr.cdr_hourly_aggregated\n      +- Relation spark_catalog.algerie_telecom_cdr.cdr_hourly_aggregated[CDR_DAY#2663,call_hour#2664,hour_key#2665,total_calls#2666L,unique_users#2667L,active_cells#2668L,unique_sessions#2669L,successful_calls#2670L,failed_calls#2671L,total_duration_seconds#2672,avg_duration#2673,stddev_duration#2674,min_duration#2675,max_duration#2676,median_duration#2677,p95_duration#2678,short_calls_30s#2679L,medium_calls_2min#2680L,normal_calls_5min#2681L,long_calls_over5min#2682L,total_revenue#2683,avg_revenue_per_call#2684,paid_calls#2685L,free_calls#2686L,... 16 more fields] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 190\u001b[0m\n\u001b[1;32m    184\u001b[0m hourly_df \u001b[38;5;241m=\u001b[39m spark\u001b[38;5;241m.\u001b[39mtable(HOURLY_TABLE)\n\u001b[1;32m    186\u001b[0m \u001b[38;5;66;03m# 1. Time-based features\u001b[39;00m\n\u001b[1;32m    187\u001b[0m hourly_features \u001b[38;5;241m=\u001b[39m \u001b[43mhourly_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    188\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_peak_hour\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcall_hour\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbetween\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m11\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcall_hour\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbetween\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m18\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 190\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mis_night_hour\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwhen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcall_hour\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbetween\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m|\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcall_hour\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m22\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43motherwise\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m    194\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_business_hour\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    195\u001b[0m     F\u001b[38;5;241m.\u001b[39mwhen(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_hour\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mbetween(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m17\u001b[39m), \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    196\u001b[0m )\u001b[38;5;241m.\u001b[39mwithColumn(\n\u001b[1;32m    197\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhour_of_week\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    198\u001b[0m     F\u001b[38;5;241m.\u001b[39mwhen(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDR_DAY\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2024-12-31\u001b[39m\u001b[38;5;124m\"\u001b[39m, F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_hour\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    199\u001b[0m      \u001b[38;5;241m.\u001b[39motherwise(F\u001b[38;5;241m.\u001b[39mcol(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_hour\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m24\u001b[39m)  \u001b[38;5;66;03m# Continue counting for Jan 1\u001b[39;00m\n\u001b[1;32m    200\u001b[0m )\n\u001b[1;32m    202\u001b[0m \u001b[38;5;66;03m# 2. Relative metrics (compare to daily average)\u001b[39;00m\n\u001b[1;32m    203\u001b[0m daily_avg_window \u001b[38;5;241m=\u001b[39m Window\u001b[38;5;241m.\u001b[39mpartitionBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDR_DAY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:5174\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[0;34m(self, colName, col)\u001b[0m\n\u001b[1;32m   5169\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, Column):\n\u001b[1;32m   5170\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m   5171\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_COLUMN\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5172\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcol\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marg_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(col)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m},\n\u001b[1;32m   5173\u001b[0m     )\n\u001b[0;32m-> 5174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwithColumn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolName\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jc\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.BINARY_OP_DIFF_TYPES] Cannot resolve \"(((call_hour >= 0) AND (call_hour <= 5)) OR call_hour)\" due to data type mismatch: the left and right operands of the binary operator have incompatible types (\"BOOLEAN\" and \"INT\").;\n'Project [CDR_DAY#2663, call_hour#2664, hour_key#2665, total_calls#2666L, unique_users#2667L, active_cells#2668L, unique_sessions#2669L, successful_calls#2670L, failed_calls#2671L, total_duration_seconds#2672, avg_duration#2673, stddev_duration#2674, min_duration#2675, max_duration#2676, median_duration#2677, p95_duration#2678, short_calls_30s#2679L, medium_calls_2min#2680L, normal_calls_5min#2681L, long_calls_over5min#2682L, total_revenue#2683, avg_revenue_per_call#2684, paid_calls#2685L, free_calls#2686L, ... 18 more fields]\n+- Project [CDR_DAY#2663, call_hour#2664, hour_key#2665, total_calls#2666L, unique_users#2667L, active_cells#2668L, unique_sessions#2669L, successful_calls#2670L, failed_calls#2671L, total_duration_seconds#2672, avg_duration#2673, stddev_duration#2674, min_duration#2675, max_duration#2676, median_duration#2677, p95_duration#2678, short_calls_30s#2679L, medium_calls_2min#2680L, normal_calls_5min#2681L, long_calls_over5min#2682L, total_revenue#2683, avg_revenue_per_call#2684, paid_calls#2685L, free_calls#2686L, ... 17 more fields]\n   +- SubqueryAlias spark_catalog.algerie_telecom_cdr.cdr_hourly_aggregated\n      +- Relation spark_catalog.algerie_telecom_cdr.cdr_hourly_aggregated[CDR_DAY#2663,call_hour#2664,hour_key#2665,total_calls#2666L,unique_users#2667L,active_cells#2668L,unique_sessions#2669L,successful_calls#2670L,failed_calls#2671L,total_duration_seconds#2672,avg_duration#2673,stddev_duration#2674,min_duration#2675,max_duration#2676,median_duration#2677,p95_duration#2678,short_calls_30s#2679L,medium_calls_2min#2680L,normal_calls_5min#2681L,long_calls_over5min#2682L,total_revenue#2683,avg_revenue_per_call#2684,paid_calls#2685L,free_calls#2686L,... 16 more fields] parquet\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "spark = init_spark(\"CDR Hourly Tables & Feature Engineering\")\n",
    "\n",
    "# Configuration\n",
    "DATABASE_NAME = \"algerie_telecom_cdr\"\n",
    "MAIN_TABLE = \"cdr_anonymized\"\n",
    "HOURLY_TABLE = \"cdr_hourly_aggregated\"\n",
    "HOURLY_FEATURES = \"cdr_hourly_features\"\n",
    "MINUTE_LEVEL = \"cdr_minute_aggregated\"  # For midnight analysis\n",
    "HOURLY_USER_BEHAVIOR = \"cdr_hourly_user_behavior\"\n",
    "\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "print(f\"âœ… Using database: {DATABASE_NAME}\")\n",
    "print(f\"ðŸ“Š Creating advanced hourly analytics tables\")\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 2: Create Comprehensive Hourly Table\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nâ° CREATING COMPREHENSIVE HOURLY AGGREGATION TABLE\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load main data with temporal parsing\n",
    "df = spark.table(MAIN_TABLE)\n",
    "\n",
    "# Add detailed temporal features\n",
    "df_temporal = df.withColumn(\n",
    "    \"call_timestamp\", F.to_timestamp(F.col(\"START_DATE\"), \"yyyyMMddHHmmss\")\n",
    ").withColumn(\n",
    "    \"call_hour\", F.hour(\"call_timestamp\")\n",
    ").withColumn(\n",
    "    \"call_minute\", F.minute(\"call_timestamp\")\n",
    ").withColumn(\n",
    "    \"call_date\", F.to_date(\"call_timestamp\")\n",
    ").withColumn(\n",
    "    \"hour_key\", F.concat_ws(\"_\", F.col(\"CDR_DAY\"), F.lpad(F.col(\"call_hour\"), 2, \"0\"))\n",
    ")\n",
    "\n",
    "# Create comprehensive hourly aggregation\n",
    "hourly_agg = df_temporal.groupBy(\"CDR_DAY\", \"call_hour\", \"hour_key\").agg(\n",
    "    # Volume metrics\n",
    "    F.count(\"*\").alias(\"total_calls\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_users\"),\n",
    "    F.countDistinct(\"CallingCellID\").alias(\"active_cells\"),\n",
    "    F.countDistinct(\"SESSION_ID\").alias(\"unique_sessions\"),\n",
    "    \n",
    "    # Success metrics\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\") > 0, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\") == 0, 1).otherwise(0)).alias(\"failed_calls\"),\n",
    "    \n",
    "    # Duration metrics (in seconds)\n",
    "    F.sum(\"ACTUAL_USAGE\").alias(\"total_duration_seconds\"),\n",
    "    F.avg(\"ACTUAL_USAGE\").alias(\"avg_duration\"),\n",
    "    F.stddev(\"ACTUAL_USAGE\").alias(\"stddev_duration\"),\n",
    "    F.min(\"ACTUAL_USAGE\").alias(\"min_duration\"),\n",
    "    F.max(\"ACTUAL_USAGE\").alias(\"max_duration\"),\n",
    "    F.expr(\"percentile_approx(ACTUAL_USAGE, 0.5)\").alias(\"median_duration\"),\n",
    "    F.expr(\"percentile_approx(ACTUAL_USAGE, 0.95)\").alias(\"p95_duration\"),\n",
    "    \n",
    "    # Duration categories\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\") <= 30, 1).otherwise(0)).alias(\"short_calls_30s\"),\n",
    "    F.sum(F.when((F.col(\"ACTUAL_USAGE\") > 30) & (F.col(\"ACTUAL_USAGE\") <= 120), 1).otherwise(0)).alias(\"medium_calls_2min\"),\n",
    "    F.sum(F.when((F.col(\"ACTUAL_USAGE\") > 120) & (F.col(\"ACTUAL_USAGE\") <= 300), 1).otherwise(0)).alias(\"normal_calls_5min\"),\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\") > 300, 1).otherwise(0)).alias(\"long_calls_over5min\"),\n",
    "    \n",
    "    # Revenue metrics\n",
    "    F.sum(\"DEBIT_AMOUNT\").alias(\"total_revenue\"),\n",
    "    F.avg(\"DEBIT_AMOUNT\").alias(\"avg_revenue_per_call\"),\n",
    "    F.sum(F.when(F.col(\"DEBIT_AMOUNT\") > 0, 1).otherwise(0)).alias(\"paid_calls\"),\n",
    "    F.sum(F.when(F.col(\"DEBIT_AMOUNT\") == 0, 1).otherwise(0)).alias(\"free_calls\"),\n",
    "    F.max(\"DEBIT_AMOUNT\").alias(\"max_single_charge\"),\n",
    "    \n",
    "    # Service breakdown\n",
    "    F.sum(F.when(F.col(\"SERVICE_CATEGORY\") == \"1\", 1).otherwise(0)).alias(\"voice_calls\"),\n",
    "    F.sum(F.when(F.col(\"SERVICE_CATEGORY\") == \"2\", 1).otherwise(0)).alias(\"sms_count\"),\n",
    "    F.sum(F.when(F.col(\"SERVICE_CATEGORY\") == \"3\", 1).otherwise(0)).alias(\"data_sessions\"),\n",
    "    \n",
    "    # Call types\n",
    "    F.sum(F.when(F.col(\"CallType\") == \"0\", 1).otherwise(0)).alias(\"local_calls\"),\n",
    "    F.sum(F.when(F.col(\"CallType\") == \"1\", 1).otherwise(0)).alias(\"national_calls\"),\n",
    "    F.sum(F.when(F.col(\"CallType\") == \"2\", 1).otherwise(0)).alias(\"international_calls\"),\n",
    "    \n",
    "    # Network indicators\n",
    "    F.sum(F.when(F.col(\"RoamState\") == \"1\", 1).otherwise(0)).alias(\"roaming_calls\"),\n",
    "    F.sum(F.when(F.col(\"CallForwardIndicator\") == \"1\", 1).otherwise(0)).alias(\"forwarded_calls\"),\n",
    "    \n",
    "    # Time markers\n",
    "    F.min(\"call_timestamp\").alias(\"first_call_time\"),\n",
    "    F.max(\"call_timestamp\").alias(\"last_call_time\")\n",
    ").withColumn(\n",
    "    \"success_rate\", F.round(F.col(\"successful_calls\") / F.col(\"total_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"failure_rate\", F.round(F.col(\"failed_calls\") / F.col(\"total_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"avg_calls_per_user\", F.round(F.col(\"total_calls\") / F.col(\"unique_users\"), 2)\n",
    ").withColumn(\n",
    "    \"hourly_arpu\", F.round(F.col(\"total_revenue\") / F.col(\"unique_users\"), 2)\n",
    ").withColumn(\n",
    "    \"paid_call_ratio\", F.round(F.col(\"paid_calls\") / F.col(\"total_calls\") * 100, 2)\n",
    ").orderBy(\"CDR_DAY\", \"call_hour\")\n",
    "\n",
    "# Save the hourly table\n",
    "hourly_agg.write.mode(\"overwrite\").saveAsTable(HOURLY_TABLE)\n",
    "print(f\"âœ… Created hourly aggregation table: {HOURLY_TABLE}\")\n",
    "\n",
    "# Show sample\n",
    "print(\"\\nðŸ“Š Sample Hourly Data:\")\n",
    "hourly_agg.show(5, truncate=False)\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 3: Create Minute-Level Table for Midnight Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ• CREATING MINUTE-LEVEL TABLE FOR MIDNIGHT ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Focus on celebration window: Dec 31 22:00 - Jan 1 02:00\n",
    "midnight_window = df_temporal.filter(\n",
    "    ((F.col(\"CDR_DAY\") == \"2024-12-31\") & (F.col(\"call_hour\") >= 22)) |\n",
    "    ((F.col(\"CDR_DAY\") == \"2025-01-01\") & (F.col(\"call_hour\") <= 2))\n",
    ").withColumn(\n",
    "    \"minute_key\", \n",
    "    F.concat_ws(\"_\", \n",
    "        F.col(\"CDR_DAY\"), \n",
    "        F.lpad(F.col(\"call_hour\"), 2, \"0\"),\n",
    "        F.lpad(F.col(\"call_minute\"), 2, \"0\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# Minute-level aggregation\n",
    "minute_agg = midnight_window.groupBy(\n",
    "    \"CDR_DAY\", \"call_hour\", \"call_minute\", \"minute_key\"\n",
    ").agg(\n",
    "    F.count(\"*\").alias(\"calls_per_minute\"),\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"unique_callers\"),\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\") > 0, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\") == 0, 1).otherwise(0)).alias(\"failed_calls\"),\n",
    "    F.avg(\"ACTUAL_USAGE\").alias(\"avg_duration\"),\n",
    "    F.sum(\"DEBIT_AMOUNT\").alias(\"minute_revenue\")\n",
    ").withColumn(\n",
    "    \"timestamp\", \n",
    "    F.to_timestamp(\n",
    "        F.concat(F.col(\"CDR_DAY\"), F.lit(\" \"), \n",
    "                F.lpad(F.col(\"call_hour\"), 2, \"0\"), F.lit(\":\"),\n",
    "                F.lpad(F.col(\"call_minute\"), 2, \"0\"), F.lit(\":00\"))\n",
    "    )\n",
    ").orderBy(\"timestamp\")\n",
    "\n",
    "minute_agg.write.mode(\"overwrite\").saveAsTable(MINUTE_LEVEL)\n",
    "print(f\"âœ… Created minute-level table: {MINUTE_LEVEL}\")\n",
    "\n",
    "# Find the exact midnight spike\n",
    "print(\"\\nðŸŽŠ Midnight Spike Analysis:\")\n",
    "midnight_spike = minute_agg.filter(\n",
    "    (F.col(\"call_hour\") == 0) & (F.col(\"call_minute\") < 5)\n",
    ").orderBy(\"call_minute\")\n",
    "midnight_spike.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bd2110-9522-437b-b912-c954a23b028c",
   "metadata": {},
   "source": [
    "### Advanced Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec691901-c43f-438b-886b-d009e5c67be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ ADVANCED FEATURE ENGINEERING FOR 2-DAY ANALYSIS\n",
      "------------------------------------------------------------\n",
      "âœ… Created hourly features table: cdr_hourly_features\n",
      "\n",
      "ðŸ“Š Feature Engineering Summary:\n",
      "+-------------+-----------+------------+------------------+-------------+--------------------+-------------------+\n",
      "|     hour_key|total_calls|success_rate|calls_vs_daily_avg|is_spike_hour|network_stress_level|is_celebration_hour|\n",
      "+-------------+-----------+------------+------------------+-------------+--------------------+-------------------+\n",
      "|2024-12-31_22|       1880|       99.84|            -21.46|       Normal|                 Low|                  1|\n",
      "|2024-12-31_23|       5271|       99.81|            120.21|  Minor Spike|              Medium|                  1|\n",
      "|2025-01-01_00|       2032|        99.7|            -65.61|       Normal|                 Low|                  1|\n",
      "|2025-01-01_01|        881|       100.0|            -85.09|       Normal|                 Low|                  1|\n",
      "|2025-01-01_02|        643|       100.0|            -89.12|       Normal|                 Low|                  1|\n",
      "+-------------+-----------+------------+------------------+-------------+--------------------+-------------------+\n",
      "\n",
      "\n",
      "âœ… Feature validation:\n",
      "   Total hours processed: 17\n",
      "   Celebration hours: 5\n",
      "   Spike hours detected: 4\n",
      "\n",
      "ðŸ‘¥ HOURLY USER BEHAVIOR ANALYSIS\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'pivot'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 169\u001b[0m\n\u001b[1;32m    145\u001b[0m user_patterns \u001b[38;5;241m=\u001b[39m user_hourly\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRI_IDENTITY_HASH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m    146\u001b[0m     F\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactive_hours\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m    147\u001b[0m     F\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_hourly_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m      \u001b[38;5;241m.\u001b[39motherwise(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLow Activity\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    161\u001b[0m )\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Aggregate user patterns by hour\u001b[39;00m\n\u001b[1;32m    164\u001b[0m hourly_user_behavior \u001b[38;5;241m=\u001b[39m \u001b[43muser_patterns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    165\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_hourly\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPRI_IDENTITY_HASH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\n\u001b[1;32m    166\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupBy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCDR_DAY\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcall_hour\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcountDistinct\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mPRI_IDENTITY_HASH\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43musers_by_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser_hourly_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcalls_by_type\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m--> 169\u001b[0m \u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpivot\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musers_by_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    171\u001b[0m hourly_user_behavior\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(HOURLY_USER_BEHAVIOR)\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Created hourly user behavior table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOURLY_USER_BEHAVIOR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:3127\u001b[0m, in \u001b[0;36mDataFrame.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   3094\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the :class:`Column` denoted by ``name``.\u001b[39;00m\n\u001b[1;32m   3095\u001b[0m \n\u001b[1;32m   3096\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3124\u001b[0m \u001b[38;5;124;03m+---+\u001b[39;00m\n\u001b[1;32m   3125\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3126\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 3127\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   3128\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name)\n\u001b[1;32m   3129\u001b[0m     )\n\u001b[1;32m   3130\u001b[0m jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mapply(name)\n\u001b[1;32m   3131\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'pivot'"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nðŸ”§ ADVANCED FEATURE ENGINEERING FOR 2-DAY ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Load hourly data for feature engineering\n",
    "hourly_df = spark.table(HOURLY_TABLE)\n",
    "\n",
    "# 1. Time-based features (FIXED: Added proper parentheses)\n",
    "hourly_features = hourly_df.withColumn(\n",
    "    \"is_peak_hour\", \n",
    "    F.when((F.col(\"call_hour\").between(9, 11)) | (F.col(\"call_hour\").between(18, 22)), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_night_hour\",\n",
    "    F.when((F.col(\"call_hour\").between(0, 5)) | (F.col(\"call_hour\") >= 22), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_business_hour\",\n",
    "    F.when(F.col(\"call_hour\").between(8, 17), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"hour_of_week\",\n",
    "    F.when(F.col(\"CDR_DAY\") == \"2024-12-31\", F.col(\"call_hour\"))\n",
    "     .otherwise(F.col(\"call_hour\") + 24)  # Continue counting for Jan 1\n",
    ")\n",
    "\n",
    "# 2. Relative metrics (compare to daily average)\n",
    "daily_avg_window = Window.partitionBy(\"CDR_DAY\")\n",
    "\n",
    "hourly_features = hourly_features.withColumn(\n",
    "    \"daily_avg_calls\", F.avg(\"total_calls\").over(daily_avg_window)\n",
    ").withColumn(\n",
    "    \"daily_avg_revenue\", F.avg(\"total_revenue\").over(daily_avg_window)\n",
    ").withColumn(\n",
    "    \"calls_vs_daily_avg\", \n",
    "    F.round((F.col(\"total_calls\") - F.col(\"daily_avg_calls\")) / F.col(\"daily_avg_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"revenue_vs_daily_avg\",\n",
    "    F.round((F.col(\"total_revenue\") - F.col(\"daily_avg_revenue\")) / F.col(\"daily_avg_revenue\") * 100, 2)\n",
    ")\n",
    "\n",
    "# 3. Spike detection using z-scores\n",
    "hourly_features = hourly_features.withColumn(\n",
    "    \"daily_stddev_calls\", F.stddev(\"total_calls\").over(daily_avg_window)\n",
    ").withColumn(\n",
    "    \"call_volume_zscore\",\n",
    "    F.when(F.col(\"daily_stddev_calls\") > 0,\n",
    "        (F.col(\"total_calls\") - F.col(\"daily_avg_calls\")) / F.col(\"daily_stddev_calls\")\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_spike_hour\",\n",
    "    F.when(F.col(\"call_volume_zscore\") > 2, \"Major Spike\")\n",
    "     .when(F.col(\"call_volume_zscore\") > 1, \"Minor Spike\")\n",
    "     .when(F.col(\"call_volume_zscore\") < -1, \"Low Activity\")\n",
    "     .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# 4. Network stress indicators\n",
    "hourly_features = hourly_features.withColumn(\n",
    "    \"network_stress_score\",\n",
    "    (F.col(\"failure_rate\") * 0.4 + \n",
    "     (100 - F.col(\"success_rate\")) * 0.3 +\n",
    "     F.when(F.col(\"total_calls\") > F.col(\"daily_avg_calls\") * 2, 30).otherwise(0))\n",
    ").withColumn(\n",
    "    \"network_stress_level\",\n",
    "    F.when(F.col(\"network_stress_score\") > 60, \"Critical\")\n",
    "     .when(F.col(\"network_stress_score\") > 40, \"High\")\n",
    "     .when(F.col(\"network_stress_score\") > 20, \"Medium\")\n",
    "     .otherwise(\"Low\")\n",
    ")\n",
    "\n",
    "# 5. User behavior features\n",
    "hourly_features = hourly_features.withColumn(\n",
    "    \"user_concentration\",\n",
    "    F.round(F.col(\"unique_users\") / F.col(\"total_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"avg_user_activity\",\n",
    "    F.round(F.col(\"total_calls\") / F.col(\"unique_users\"), 2)\n",
    ").withColumn(\n",
    "    \"revenue_concentration\",\n",
    "    F.when(F.col(\"paid_calls\") > 0,\n",
    "        F.round(F.col(\"total_revenue\") / F.col(\"paid_calls\"), 2)\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 6. Service mix features\n",
    "hourly_features = hourly_features.withColumn(\n",
    "    \"voice_dominance\",\n",
    "    F.when(F.col(\"total_calls\") > 0,\n",
    "        F.round(F.col(\"voice_calls\") / F.col(\"total_calls\") * 100, 2)\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"sms_ratio\",\n",
    "    F.when(F.col(\"total_calls\") > 0,\n",
    "        F.round(F.col(\"sms_count\") / F.col(\"total_calls\") * 100, 2)\n",
    "    ).otherwise(0)\n",
    ")\n",
    "\n",
    "# 7. New Year specific features\n",
    "hourly_features = hourly_features.withColumn(\n",
    "    \"is_celebration_hour\",\n",
    "    F.when(\n",
    "        ((F.col(\"CDR_DAY\") == \"2024-12-31\") & (F.col(\"call_hour\") >= 22)) |\n",
    "        ((F.col(\"CDR_DAY\") == \"2025-01-01\") & (F.col(\"call_hour\") <= 2)), 1\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    \"hours_from_midnight\",\n",
    "    F.when(F.col(\"CDR_DAY\") == \"2024-12-31\", F.col(\"call_hour\") - 24)\n",
    "     .otherwise(F.col(\"call_hour\"))\n",
    ")\n",
    "\n",
    "# Save enriched hourly features\n",
    "hourly_features.write.mode(\"overwrite\").saveAsTable(HOURLY_FEATURES)\n",
    "print(f\"âœ… Created hourly features table: {HOURLY_FEATURES}\")\n",
    "\n",
    "# Show feature summary\n",
    "print(\"\\nðŸ“Š Feature Engineering Summary:\")\n",
    "feature_cols = [\"hour_key\", \"total_calls\", \"success_rate\", \"calls_vs_daily_avg\", \n",
    "                \"is_spike_hour\", \"network_stress_level\", \"is_celebration_hour\"]\n",
    "hourly_features.select(*feature_cols).filter(\n",
    "    F.col(\"is_celebration_hour\") == 1\n",
    ").show()\n",
    "\n",
    "# Additional validation\n",
    "print(\"\\nâœ… Feature validation:\")\n",
    "print(f\"   Total hours processed: {hourly_features.count()}\")\n",
    "print(f\"   Celebration hours: {hourly_features.filter(F.col('is_celebration_hour') == 1).count()}\")\n",
    "print(f\"   Spike hours detected: {hourly_features.filter(F.col('is_spike_hour') != 'Normal').count()}\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 5: User Behavior Patterns by Hour\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ‘¥ HOURLY USER BEHAVIOR ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create user-hour level aggregations\n",
    "user_hourly = df_temporal.groupBy(\"PRI_IDENTITY_HASH\", \"CDR_DAY\", \"call_hour\").agg(\n",
    "    F.count(\"*\").alias(\"user_hourly_calls\"),\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\") > 0, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(\"ACTUAL_USAGE\").alias(\"total_duration\"),\n",
    "    F.sum(\"DEBIT_AMOUNT\").alias(\"total_spend\"),\n",
    "    F.countDistinct(\"CallingCellID\").alias(\"cells_used\")\n",
    ")\n",
    "\n",
    "# Identify user patterns\n",
    "user_patterns = user_hourly.groupBy(\"PRI_IDENTITY_HASH\").agg(\n",
    "    F.count(\"*\").alias(\"active_hours\"),\n",
    "    F.sum(\"user_hourly_calls\").alias(\"total_calls\"),\n",
    "    F.max(\"user_hourly_calls\").alias(\"max_hourly_calls\"),\n",
    "    F.collect_list(\"call_hour\").alias(\"active_hour_list\"),\n",
    "    F.sum(F.when(F.col(\"call_hour\").between(22, 23), F.col(\"user_hourly_calls\")).otherwise(0)).alias(\"late_night_calls\"),\n",
    "    F.sum(F.when(F.col(\"call_hour\").between(0, 2), F.col(\"user_hourly_calls\")).otherwise(0)).alias(\"early_morning_calls\")\n",
    ").withColumn(\n",
    "    \"is_midnight_caller\",\n",
    "    F.when((F.col(\"late_night_calls\") > 0) & (F.col(\"early_morning_calls\") > 0), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"user_type\",\n",
    "    F.when(F.col(\"active_hours\") >= 20, \"Always Active\")\n",
    "     .when(F.col(\"active_hours\") >= 10, \"Highly Active\")\n",
    "     .when(F.col(\"active_hours\") >= 5, \"Moderately Active\")\n",
    "     .otherwise(\"Low Activity\")\n",
    ")\n",
    "\n",
    "# Aggregate user patterns by hour\n",
    "hourly_user_behavior = user_patterns.join(\n",
    "    user_hourly, on=\"PRI_IDENTITY_HASH\"\n",
    ").groupBy(\"CDR_DAY\", \"call_hour\", \"user_type\").agg(\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"users_by_type\"),\n",
    "    F.sum(\"user_hourly_calls\").alias(\"calls_by_type\")\n",
    ").pivot(\"user_type\").sum(\"users_by_type\")\n",
    "\n",
    "hourly_user_behavior.write.mode(\"overwrite\").saveAsTable(HOURLY_USER_BEHAVIOR)\n",
    "print(f\"âœ… Created hourly user behavior table: {HOURLY_USER_BEHAVIOR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6855ee20-98b8-4459-a330-d6683f034eec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¥ HOURLY USER BEHAVIOR ANALYSIS\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o827.saveAsTable.\n: java.lang.NullPointerException\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$getChunks$1(ChunkedByteBuffer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.getChunks(ChunkedByteBuffer.scala:181)\n\tat org.apache.spark.util.io.ChunkedByteBufferInputStream.<init>(ChunkedByteBuffer.scala:278)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.toInputStream(ChunkedByteBuffer.scala:174)\n\tat org.apache.spark.sql.execution.SparkPlan.decodeUnsafeRows(SparkPlan.scala:409)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectIterator$2(SparkPlan.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:152)\n\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1162)\n\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1150)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m\n\u001b[1;32m     36\u001b[0m hourly_user_behavior \u001b[38;5;241m=\u001b[39m user_patterns\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m     37\u001b[0m     user_hourly, on\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRI_IDENTITY_HASH\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m )\u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDR_DAY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_hour\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m     39\u001b[0m     F\u001b[38;5;241m.\u001b[39mcountDistinct(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPRI_IDENTITY_HASH\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124musers_count\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     40\u001b[0m     F\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_hourly_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     41\u001b[0m )\u001b[38;5;241m.\u001b[39morderBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDR_DAY\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcall_hour\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser_type\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m# Save the table\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m \u001b[43mhourly_user_behavior\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moverwrite\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHOURLY_USER_BEHAVIOR\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mâœ… Created hourly user behavior table: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHOURLY_USER_BEHAVIOR\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Show sample of user behavior patterns\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/readwriter.py:1586\u001b[0m, in \u001b[0;36mDataFrameWriter.saveAsTable\u001b[0;34m(self, name, format, mode, partitionBy, **options)\u001b[0m\n\u001b[1;32m   1584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1585\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mformat\u001b[39m)\n\u001b[0;32m-> 1586\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jwrite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msaveAsTable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o827.saveAsTable.\n: java.lang.NullPointerException\n\tat org.apache.spark.util.io.ChunkedByteBuffer.$anonfun$getChunks$1(ChunkedByteBuffer.scala:181)\n\tat scala.collection.TraversableLike.$anonfun$map$1(TraversableLike.scala:286)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat scala.collection.TraversableLike.map(TraversableLike.scala:286)\n\tat scala.collection.TraversableLike.map$(TraversableLike.scala:279)\n\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:198)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.getChunks(ChunkedByteBuffer.scala:181)\n\tat org.apache.spark.util.io.ChunkedByteBufferInputStream.<init>(ChunkedByteBuffer.scala:278)\n\tat org.apache.spark.util.io.ChunkedByteBuffer.toInputStream(ChunkedByteBuffer.scala:174)\n\tat org.apache.spark.sql.execution.SparkPlan.decodeUnsafeRows(SparkPlan.scala:409)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$executeCollectIterator$2(SparkPlan.scala:457)\n\tat scala.collection.Iterator$$anon$11.nextCur(Iterator.scala:486)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:492)\n\tat org.apache.spark.sql.execution.joins.HashedRelation$.apply(HashedRelation.scala:152)\n\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1162)\n\tat org.apache.spark.sql.execution.joins.HashedRelationBroadcastMode.transform(HashedRelation.scala:1150)\n\tat org.apache.spark.sql.execution.exchange.BroadcastExchangeExec.$anonfun$relationFuture$1(BroadcastExchangeExec.scala:151)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$2(SQLExecution.scala:224)\n\tat org.apache.spark.JobArtifactSet$.withActiveJobArtifactState(JobArtifactSet.scala:94)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withThreadLocalCaptured$1(SQLExecution.scala:219)\n\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 5: User Behavior Patterns by Hour (FIXED)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ‘¥ HOURLY USER BEHAVIOR ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create user-hour level aggregations\n",
    "user_hourly = df_temporal.groupBy(\"PRI_IDENTITY_HASH\", \"CDR_DAY\", \"call_hour\").agg(\n",
    "    F.count(\"*\").alias(\"user_hourly_calls\"),\n",
    "    F.sum(F.when(F.col(\"ACTUAL_USAGE\") > 0, 1).otherwise(0)).alias(\"successful_calls\"),\n",
    "    F.sum(\"ACTUAL_USAGE\").alias(\"total_duration\"),\n",
    "    F.sum(\"DEBIT_AMOUNT\").alias(\"total_spend\"),\n",
    "    F.countDistinct(\"CallingCellID\").alias(\"cells_used\")\n",
    ")\n",
    "\n",
    "# Identify user patterns\n",
    "user_patterns = user_hourly.groupBy(\"PRI_IDENTITY_HASH\").agg(\n",
    "    F.count(\"*\").alias(\"active_hours\"),\n",
    "    F.sum(\"user_hourly_calls\").alias(\"total_calls\"),\n",
    "    F.max(\"user_hourly_calls\").alias(\"max_hourly_calls\"),\n",
    "    F.collect_list(\"call_hour\").alias(\"active_hour_list\"),\n",
    "    F.sum(F.when(F.col(\"call_hour\").between(22, 23), F.col(\"user_hourly_calls\")).otherwise(0)).alias(\"late_night_calls\"),\n",
    "    F.sum(F.when(F.col(\"call_hour\").between(0, 2), F.col(\"user_hourly_calls\")).otherwise(0)).alias(\"early_morning_calls\")\n",
    ").withColumn(\n",
    "    \"is_midnight_caller\",\n",
    "    F.when((F.col(\"late_night_calls\") > 0) & (F.col(\"early_morning_calls\") > 0), 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"user_type\",\n",
    "    F.when(F.col(\"active_hours\") >= 20, \"Always Active\")\n",
    "     .when(F.col(\"active_hours\") >= 10, \"Highly Active\")\n",
    "     .when(F.col(\"active_hours\") >= 5, \"Moderately Active\")\n",
    "     .otherwise(\"Low Activity\")\n",
    ")\n",
    "\n",
    "# Method 1: Simpler approach without pivot\n",
    "hourly_user_behavior = user_patterns.join(\n",
    "    user_hourly, on=\"PRI_IDENTITY_HASH\"\n",
    ").groupBy(\"CDR_DAY\", \"call_hour\", \"user_type\").agg(\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"users_count\"),\n",
    "    F.sum(\"user_hourly_calls\").alias(\"total_calls\")\n",
    ").orderBy(\"CDR_DAY\", \"call_hour\", \"user_type\")\n",
    "\n",
    "# Save the table\n",
    "hourly_user_behavior.write.mode(\"overwrite\").saveAsTable(HOURLY_USER_BEHAVIOR)\n",
    "print(f\"âœ… Created hourly user behavior table: {HOURLY_USER_BEHAVIOR}\")\n",
    "\n",
    "# Show sample of user behavior patterns\n",
    "print(\"\\nðŸ“Š User Behavior Summary:\")\n",
    "user_type_summary = user_patterns.groupBy(\"user_type\").agg(\n",
    "    F.count(\"*\").alias(\"user_count\"),\n",
    "    F.avg(\"active_hours\").alias(\"avg_active_hours\"),\n",
    "    F.sum(\"total_calls\").alias(\"total_calls_by_type\"),\n",
    "    F.sum(\"is_midnight_caller\").alias(\"midnight_callers\")\n",
    ").orderBy(\"user_count\", ascending=False)\n",
    "user_type_summary.show()\n",
    "\n",
    "# Alternative: Create a pivot table separately if needed\n",
    "print(\"\\nðŸ“Š Creating User Activity Pivot Table:\")\n",
    "user_pivot = user_hourly.join(\n",
    "    user_patterns.select(\"PRI_IDENTITY_HASH\", \"user_type\"), \n",
    "    on=\"PRI_IDENTITY_HASH\"\n",
    ").groupBy(\"CDR_DAY\", \"call_hour\").pivot(\"user_type\", \n",
    "    [\"Always Active\", \"Highly Active\", \"Moderately Active\", \"Low Activity\"]\n",
    ").agg(\n",
    "    F.countDistinct(\"PRI_IDENTITY_HASH\").alias(\"users\")\n",
    ").fillna(0).orderBy(\"CDR_DAY\", \"call_hour\")\n",
    "\n",
    "# Save pivot table separately\n",
    "user_pivot.write.mode(\"overwrite\").saveAsTable(\"cdr_user_activity_pivot\")\n",
    "print(\"âœ… Created user activity pivot table: cdr_user_activity_pivot\")\n",
    "\n",
    "# Show pivot sample\n",
    "print(\"\\nUser Distribution by Hour and Activity Level:\")\n",
    "user_pivot.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f354262-e22d-404f-84e1-52ef60eb5795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ‘¥ DETAILED USER BEHAVIOR PATTERNS\n",
      "------------------------------------------------------------\n",
      "âœ… Cached temporal data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created user time patterns table\n",
      "\n",
      "Sample User Time Patterns:\n",
      "+--------------------+---------+-------------+-------+----------+-------+\n",
      "|   PRI_IDENTITY_HASH|Afternoon|Early Morning|Evening|Late Night|Morning|\n",
      "+--------------------+---------+-------------+-------+----------+-------+\n",
      "|17f61fc2ffe59ef59...|        0|            0|      0|         0|      2|\n",
      "|8b415d740322d385d...|        0|            1|      0|         0|      4|\n",
      "|7d7c539fecce2c6dc...|        1|            1|      0|         0|      2|\n",
      "|dbb0c0db7d216b1ca...|        0|            0|      0|         0|      1|\n",
      "|cc21d91eec7bb8488...|        0|            0|      0|         0|      1|\n",
      "+--------------------+---------+-------------+-------+----------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[CDR_ID: string, CDR_SUB_ID: string, CDR_TYPE: string, CDR_BATCH_ID: string, SRC_CDR_ID: string, START_DATE: string, END_DATE: string, CREATE_DATE: string, CUST_LOCAL_START_DATE: string, CUST_LOCAL_END_DATE: string, OBJ_ID: string, ACTUAL_USAGE: double, RATE_USAGE: double, SERVICE_UNIT_TYPE: string, SERVICE_CATEGORY: string, USAGE_SERVICE_TYPE: string, STD_EVT_TYPE_ID: string, SESSION_ID: string, DEBIT_AMOUNT: double, UN_DEBIT_AMOUNT: double, TOTAL_TAX: double, ServiceFlow: string, CallForwardIndicator: string, ChargingTime: double, CallType: string, RoamState: string, CallingRoamInfo: string, CalledRoamInfo: string, CallingCellID: string, CalledCellID: string, MSCAddress: string, BrandID: string, PRI_IDENTITY_HASH: string, CallingPartyNumber_HASH: string, CalledPartyNumber_HASH: string, CallingPartyIMSI_HASH: string, CalledPartyIMSI_HASH: string, IMEI_HASH: string, CDR_DAY: date, call_timestamp: timestamp, call_hour: int, call_minute: int, call_date: date, hour_key: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 5B: Complex User Behavior (Optional - Run if needed)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ‘¥ DETAILED USER BEHAVIOR PATTERNS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Cache the temporal data first\n",
    "df_temporal.cache()\n",
    "print(\"âœ… Cached temporal data\")\n",
    "\n",
    "# Create user profiles with error handling\n",
    "try:\n",
    "    # User activity by time period\n",
    "    user_time_patterns = df_temporal.withColumn(\n",
    "        \"time_period\",\n",
    "        F.when(F.col(\"call_hour\").between(6, 11), \"Morning\")\n",
    "         .when(F.col(\"call_hour\").between(12, 17), \"Afternoon\")\n",
    "         .when(F.col(\"call_hour\").between(18, 21), \"Evening\")\n",
    "         .when(F.col(\"call_hour\").between(22, 23), \"Late Night\")\n",
    "         .when(F.col(\"call_hour\").between(0, 5), \"Early Morning\")\n",
    "         .otherwise(\"Night\")\n",
    "    ).groupBy(\"PRI_IDENTITY_HASH\", \"time_period\").agg(\n",
    "        F.count(\"*\").alias(\"calls_in_period\")\n",
    "    )\n",
    "    \n",
    "    # Pivot to get calls by time period for each user\n",
    "    user_time_pivot = user_time_patterns.groupBy(\"PRI_IDENTITY_HASH\").pivot(\"time_period\").sum(\"calls_in_period\").fillna(0)\n",
    "    \n",
    "    # Save the result\n",
    "    user_time_pivot.write.mode(\"overwrite\").saveAsTable(\"cdr_user_time_patterns\")\n",
    "    print(\"âœ… Created user time patterns table\")\n",
    "    \n",
    "    # Show sample\n",
    "    print(\"\\nSample User Time Patterns:\")\n",
    "    user_time_pivot.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Error creating complex user patterns: {str(e)}\")\n",
    "    print(\"Continuing with simplified analysis...\")\n",
    "\n",
    "# Unpersist to free memory\n",
    "df_temporal.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe149001-e494-4425-9070-6b6d52198d79",
   "metadata": {},
   "source": [
    "# 6. Create Views for Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b1f8bb47-48b7-406c-9eb5-70b11b1272c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ” CREATING ANALYTICAL VIEWS\n",
      "------------------------------------------------------------\n",
      "âœ… Created view: v_hourly_performance\n",
      "âœ… Created view: v_midnight_transition\n",
      "âœ… Created view: v_network_stress_hours\n",
      "âœ… Created view: v_hourly_service_mix\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\nðŸ” CREATING ANALYTICAL VIEWS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# View 1: Hourly Performance Dashboard\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_hourly_performance AS\n",
    "SELECT \n",
    "    hour_key,\n",
    "    CDR_DAY,\n",
    "    call_hour,\n",
    "    total_calls,\n",
    "    unique_users,\n",
    "    success_rate,\n",
    "    failure_rate,\n",
    "    ROUND(total_revenue, 2) as revenue,\n",
    "    ROUND(hourly_arpu, 2) as arpu,\n",
    "    is_spike_hour,\n",
    "    network_stress_level,\n",
    "    CASE \n",
    "        WHEN is_celebration_hour = 1 THEN 'Celebration Hour'\n",
    "        WHEN is_night_hour = 1 THEN 'Night Hour'\n",
    "        WHEN is_peak_hour = 1 THEN 'Peak Hour'\n",
    "        WHEN is_business_hour = 1 THEN 'Business Hour'\n",
    "        ELSE 'Off-Peak'\n",
    "    END as hour_category\n",
    "FROM {HOURLY_FEATURES}\n",
    "ORDER BY CDR_DAY, call_hour\n",
    "\"\"\")\n",
    "print(\"âœ… Created view: v_hourly_performance\")\n",
    "\n",
    "# View 2: Midnight Transition Analysis\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_midnight_transition AS\n",
    "SELECT \n",
    "    call_hour,                  \n",
    "    call_minute,                \n",
    "    timestamp,\n",
    "    calls_per_minute,\n",
    "    unique_callers,\n",
    "    successful_calls,\n",
    "    failed_calls,\n",
    "    ROUND(minute_revenue, 2) as revenue,\n",
    "    ROUND(failed_calls * 100.0 / calls_per_minute, 2) as failure_rate\n",
    "FROM {MINUTE_LEVEL}\n",
    "WHERE (call_hour = 23 AND call_minute >= 50) \n",
    "   OR (call_hour = 0 AND call_minute <= 10)\n",
    "ORDER BY timestamp\n",
    "\"\"\")\n",
    "print(\"âœ… Created view: v_midnight_transition\")\n",
    "\n",
    "# View 3: Network Stress Hours\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_network_stress_hours AS\n",
    "SELECT \n",
    "    hour_key,\n",
    "    CDR_DAY,\n",
    "    call_hour,\n",
    "    total_calls,\n",
    "    failure_rate,\n",
    "    network_stress_score,\n",
    "    network_stress_level,\n",
    "    unique_users,\n",
    "    active_cells\n",
    "FROM {HOURLY_FEATURES}\n",
    "WHERE network_stress_level IN ('High', 'Critical')\n",
    "ORDER BY network_stress_score DESC\n",
    "\"\"\")\n",
    "print(\"âœ… Created view: v_network_stress_hours\")\n",
    "\n",
    "# View 4: Service Pattern by Hour\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE VIEW v_hourly_service_mix AS\n",
    "SELECT \n",
    "    CDR_DAY,\n",
    "    call_hour,\n",
    "    voice_calls,\n",
    "    sms_count,\n",
    "    data_sessions,\n",
    "    voice_dominance,\n",
    "    sms_ratio,\n",
    "    total_calls\n",
    "FROM {HOURLY_FEATURES}\n",
    "ORDER BY CDR_DAY, call_hour\n",
    "\"\"\")\n",
    "print(\"âœ… Created view: v_hourly_service_mix\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0e3f359-e366-48df-9968-78b5cd42b73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š KEY INSIGHTS FROM HOURLY ANALYSIS\n",
      "============================================================\n",
      "\n",
      "1ï¸âƒ£ TOP 5 PEAK HOURS:\n",
      "+-------------+-----------+------------+-------------+-------------+\n",
      "|     hour_key|total_calls|unique_users|total_revenue|is_spike_hour|\n",
      "+-------------+-----------+------------+-------------+-------------+\n",
      "|2025-01-01_10|      18125|       11951|    9718766.0|  Minor Spike|\n",
      "|2025-01-01_09|      16393|       10888|    8206586.0|  Minor Spike|\n",
      "|2025-01-01_11|      14625|       10071|    6803847.0|  Minor Spike|\n",
      "|2024-12-31_23|       5271|        3116|    1589915.0|  Minor Spike|\n",
      "+-------------+-----------+------------+-------------+-------------+\n",
      "\n",
      "\n",
      "2ï¸âƒ£ MIDNIGHT SURGE PATTERN:\n",
      "+---------+-----------+--------------------+----------------+\n",
      "|call_hour|total_calls|max_calls_per_minute|avg_failure_rate|\n",
      "+---------+-----------+--------------------+----------------+\n",
      "|        0|        577|                  60|        0.178182|\n",
      "|       23|        785|                  93|        0.365000|\n",
      "+---------+-----------+--------------------+----------------+\n",
      "\n",
      "\n",
      "3ï¸âƒ£ NETWORK STRESS SUMMARY:\n",
      "+--------------+--------------------+\n",
      "|stressed_hours|network_stress_level|\n",
      "+--------------+--------------------+\n",
      "+--------------+--------------------+\n",
      "\n",
      "\n",
      "4ï¸âƒ£ SERVICE USAGE BY TIME PERIOD:\n",
      "+----------+---------+-----+---+-------------------+\n",
      "|   CDR_DAY|   period|voice|sms|avg_voice_dominance|\n",
      "+----------+---------+-----+---+-------------------+\n",
      "|2024-12-31|  Evening| 7181|  0|              100.0|\n",
      "|2025-01-01|Afternoon| 9731|  0|              100.0|\n",
      "|2025-01-01|  Morning|67200|  0|              100.0|\n",
      "|2025-01-01|    Night| 5799|  0|              100.0|\n",
      "+----------+---------+-----+---+-------------------+\n",
      "\n",
      "\n",
      "âœ… Hourly analysis complete!\n",
      "ðŸ“Š Tables created:\n",
      "   - cdr_hourly_aggregated: Main hourly aggregations\n",
      "   - cdr_hourly_features: Enriched hourly features\n",
      "   - cdr_minute_aggregated: Minute-level for midnight analysis\n",
      "   - cdr_hourly_user_behavior: User patterns by hour\n",
      "\n",
      "ðŸ” Views available for BI:\n",
      "   - v_hourly_performance\n",
      "   - v_midnight_transition\n",
      "   - v_network_stress_hours\n",
      "   - v_hourly_service_mix\n",
      "\n",
      "ðŸ“ˆ PREPARING DATA FOR TREND ANALYSIS\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 04:57:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 04:57:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 04:57:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 04:57:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 04:57:29 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Created trend analysis table: cdr_hourly_trends\n",
      "\n",
      "ðŸŽŠ CELEBRATION PHASE ANALYSIS:\n",
      "+-----------------+-----+-----------+-----------------+-------------+-------------------+\n",
      "|celebration_phase|hours|total_calls| avg_success_rate|total_revenue|   avg_stress_score|\n",
      "+-----------------+-----+-----------+-----------------+-------------+-------------------+\n",
      "|         Early NY|    3|       3556|99.89999999999999|    1943679.0|0.06999999999999972|\n",
      "|     New Year Day|    8|      76931|         99.80125|  3.9189624E7|          11.389125|\n",
      "| Post-Celebration|    3|       2243|            99.82|    1233960.0|0.12600000000000064|\n",
      "|  Pre-Celebration|    3|       7181|99.88333333333333|    3734442.0| 10.081666666666665|\n",
      "+-----------------+-----+-----------+-----------------+-------------+-------------------+\n",
      "\n",
      "\n",
      "âœ… Ready for advanced trend analysis!\n",
      "ðŸ’¡ Next: Run trend detection algorithms on hourly patterns\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 7: Key Insights from Hourly Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ“Š KEY INSIGHTS FROM HOURLY ANALYSIS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Identify peak hours\n",
    "peak_hours_df = spark.sql(f\"\"\"\n",
    "SELECT hour_key, total_calls, unique_users, total_revenue, is_spike_hour\n",
    "FROM {HOURLY_FEATURES}\n",
    "WHERE is_spike_hour IN ('Major Spike', 'Minor Spike')\n",
    "ORDER BY total_calls DESC\n",
    "LIMIT 5\n",
    "\"\"\")\n",
    "print(\"\\n1ï¸âƒ£ TOP 5 PEAK HOURS:\")\n",
    "peak_hours_df.show()\n",
    "\n",
    "# 2. Midnight surge analysis\n",
    "midnight_surge = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    call_hour,\n",
    "    SUM(calls_per_minute) as total_calls,\n",
    "    MAX(calls_per_minute) as max_calls_per_minute,\n",
    "    AVG(failure_rate) as avg_failure_rate\n",
    "FROM v_midnight_transition\n",
    "GROUP BY call_hour\n",
    "ORDER BY call_hour\n",
    "\"\"\")\n",
    "print(\"\\n2ï¸âƒ£ MIDNIGHT SURGE PATTERN:\")\n",
    "midnight_surge.show()\n",
    "\n",
    "# 3. Network stress periods\n",
    "stress_periods = spark.sql(f\"\"\"\n",
    "SELECT COUNT(*) as stressed_hours, network_stress_level\n",
    "FROM v_network_stress_hours\n",
    "GROUP BY network_stress_level\n",
    "\"\"\")\n",
    "print(\"\\n3ï¸âƒ£ NETWORK STRESS SUMMARY:\")\n",
    "stress_periods.show()\n",
    "\n",
    "# 4. Service evolution through the day\n",
    "service_evolution = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    CDR_DAY,\n",
    "    CASE \n",
    "        WHEN call_hour BETWEEN 0 AND 5 THEN 'Night'\n",
    "        WHEN call_hour BETWEEN 6 AND 11 THEN 'Morning'\n",
    "        WHEN call_hour BETWEEN 12 AND 17 THEN 'Afternoon'\n",
    "        ELSE 'Evening'\n",
    "    END as period,\n",
    "    SUM(voice_calls) as voice,\n",
    "    SUM(sms_count) as sms,\n",
    "    AVG(voice_dominance) as avg_voice_dominance\n",
    "FROM v_hourly_service_mix\n",
    "GROUP BY CDR_DAY, period\n",
    "ORDER BY CDR_DAY, period\n",
    "\"\"\")\n",
    "print(\"\\n4ï¸âƒ£ SERVICE USAGE BY TIME PERIOD:\")\n",
    "service_evolution.show()\n",
    "\n",
    "print(\"\\nâœ… Hourly analysis complete!\")\n",
    "print(\"ðŸ“Š Tables created:\")\n",
    "print(f\"   - {HOURLY_TABLE}: Main hourly aggregations\")\n",
    "print(f\"   - {HOURLY_FEATURES}: Enriched hourly features\")\n",
    "print(f\"   - {MINUTE_LEVEL}: Minute-level for midnight analysis\")\n",
    "print(f\"   - {HOURLY_USER_BEHAVIOR}: User patterns by hour\")\n",
    "print(\"\\nðŸ” Views available for BI:\")\n",
    "print(\"   - v_hourly_performance\")\n",
    "print(\"   - v_midnight_transition\")\n",
    "print(\"   - v_network_stress_hours\")\n",
    "print(\"   - v_hourly_service_mix\")\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 8: Export for Trend Analysis\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\nðŸ“ˆ PREPARING DATA FOR TREND ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Create a consolidated dataset for trend analysis\n",
    "trend_data = spark.sql(f\"\"\"\n",
    "SELECT \n",
    "    h.*,\n",
    "    CASE \n",
    "        WHEN h.CDR_DAY = '2024-12-31' AND h.call_hour >= 18 THEN 'Pre-Celebration'\n",
    "        WHEN h.CDR_DAY = '2024-12-31' AND h.call_hour >= 22 THEN 'Late NYE'\n",
    "        WHEN h.CDR_DAY = '2025-01-01' AND h.call_hour <= 2 THEN 'Early NY'\n",
    "        WHEN h.CDR_DAY = '2025-01-01' AND h.call_hour BETWEEN 3 AND 5 THEN 'Post-Celebration'\n",
    "        WHEN h.CDR_DAY = '2025-01-01' AND h.call_hour >= 6 THEN 'New Year Day'\n",
    "        ELSE 'Regular'\n",
    "    END as celebration_phase,\n",
    "    LAG(h.total_calls, 1) OVER (ORDER BY h.hour_of_week) as prev_hour_calls,\n",
    "    LAG(h.total_revenue, 1) OVER (ORDER BY h.hour_of_week) as prev_hour_revenue,\n",
    "    LEAD(h.total_calls, 1) OVER (ORDER BY h.hour_of_week) as next_hour_calls\n",
    "FROM {HOURLY_FEATURES} h\n",
    "\"\"\")\n",
    "\n",
    "# Calculate hour-over-hour growth\n",
    "trend_analysis = trend_data.withColumn(\n",
    "    \"hour_over_hour_growth\",\n",
    "    F.when(F.col(\"prev_hour_calls\") > 0,\n",
    "        F.round((F.col(\"total_calls\") - F.col(\"prev_hour_calls\")) / F.col(\"prev_hour_calls\") * 100, 2)\n",
    "    ).otherwise(None)\n",
    ").withColumn(\n",
    "    \"revenue_growth\",\n",
    "    F.when(F.col(\"prev_hour_revenue\") > 0,\n",
    "        F.round((F.col(\"total_revenue\") - F.col(\"prev_hour_revenue\")) / F.col(\"prev_hour_revenue\") * 100, 2)\n",
    "    ).otherwise(None)\n",
    ")\n",
    "\n",
    "trend_analysis.write.mode(\"overwrite\").saveAsTable(\"cdr_hourly_trends\")\n",
    "print(\"âœ… Created trend analysis table: cdr_hourly_trends\")\n",
    "\n",
    "# Show celebration phase summary\n",
    "print(\"\\nðŸŽŠ CELEBRATION PHASE ANALYSIS:\")\n",
    "celebration_summary = trend_analysis.groupBy(\"celebration_phase\").agg(\n",
    "    F.count(\"*\").alias(\"hours\"),\n",
    "    F.sum(\"total_calls\").alias(\"total_calls\"),\n",
    "    F.avg(\"success_rate\").alias(\"avg_success_rate\"),\n",
    "    F.sum(\"total_revenue\").alias(\"total_revenue\"),\n",
    "    F.avg(\"network_stress_score\").alias(\"avg_stress_score\")\n",
    ").orderBy(\"celebration_phase\")\n",
    "celebration_summary.show()\n",
    "\n",
    "print(\"\\nâœ… Ready for advanced trend analysis!\")\n",
    "print(\"ðŸ’¡ Next: Run trend detection algorithms on hourly patterns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa18431-54ef-48ca-9c57-cb7099e4dc0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Anonymization pipeline completed successfully!\n",
      "âœ… Spark session closed.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# 10. Cleanup\n",
    "# ----------------------------------------------------------------------------------\n",
    "spark.stop()\n",
    "print(\"\\nâœ… Anonymization pipeline completed successfully!\")\n",
    "print(\"âœ… Spark session closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa6edb2-b53b-4d27-8781-ef480ec0afbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
