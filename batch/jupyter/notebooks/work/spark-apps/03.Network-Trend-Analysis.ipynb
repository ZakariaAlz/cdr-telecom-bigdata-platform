{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa74ffc0-fa7a-4fee-b287-4ce787c08ca4",
   "metadata": {},
   "source": [
    "# Notebook 03: Advanced Trend Detection & Anomaly Analysis\n",
    "## CDR Telecom - New Year's Eve Pattern Analysis\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# NOTEBOOK 03: TREND DETECTION & ANOMALY ANALYSIS\n",
    "# Project: CDR Telecom Big Data Engineering Final Year Internship\n",
    "# Focus: Detecting trends and anomalies in 2-day New Year dataset\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "984b51dc-d20b-45d8-a11f-0b688bb29597",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/29 05:04:20 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/06/29 05:04:20 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession initialized (App: CDR Trend Detection & Anomaly Analysis, Spark: 3.5.1)\n",
      "✅ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "✅ Hive Metastore URI: thrift://hive-metastore:9083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:04:22 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "📈 CDR TREND DETECTION & ANOMALY ANALYSIS\n",
      "================================================================================\n",
      "Analysis Period: Dec 31, 2024 - Jan 1, 2025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:04:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/29 05:04:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/29 05:05:08 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/29 05:05:23 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/29 05:05:38 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "25/06/29 05:05:53 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Hours Analyzed: 17\n",
      "Analysis Started: 2025-06-29 05:06:05.205102\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 1: Setup and Load Enriched Data\n",
    "# ------------------------------------------------------------\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import StandardScaler, VectorAssembler\n",
    "from pyspark.ml.stat import Correlation\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "spark = init_spark(\"CDR Trend Detection & Anomaly Analysis\")\n",
    "\n",
    "# Configuration\n",
    "DATABASE_NAME = \"algerie_telecom_cdr\"\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "# Load enriched hourly data\n",
    "hourly_df = spark.table(\"cdr_hourly_features\")\n",
    "trends_df = spark.table(\"cdr_hourly_trends\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"📈 CDR TREND DETECTION & ANOMALY ANALYSIS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Analysis Period: Dec 31, 2024 - Jan 1, 2025\")\n",
    "print(f\"Total Hours Analyzed: {hourly_df.count()}\")\n",
    "print(f\"Analysis Started: {datetime.now()}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29b190f-b92b-4292-9d57-737417ae6bec",
   "metadata": {},
   "source": [
    "## Statistical Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "165c6145-039f-4ab0-b7c3-cf336dd4170c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 STATISTICAL TREND ANALYSIS\n",
      "------------------------------------------------------------\n",
      "\n",
      "🔄 Trend Change Points:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:06:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:22 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:23 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------+---------+-----------+---------------+---------------+-----------------+\n",
      "|     hour_key|   CDR_DAY|call_hour|total_calls|     prev_trend|trend_direction|trend_strength_3h|\n",
      "+-------------+----------+---------+-----------+---------------+---------------+-----------------+\n",
      "|2024-12-31_22|2024-12-31|       22|       1880|         Stable|  Strong Upward|            96.86|\n",
      "|2025-01-01_00|2025-01-01|        0|       2032|  Strong Upward|Strong Downward|           -33.62|\n",
      "|2025-01-01_04|2025-01-01|        4|        622|Strong Downward|         Upward|             2.25|\n",
      "|2025-01-01_06|2025-01-01|        6|       2031|         Upward|  Strong Upward|            64.05|\n",
      "|2025-01-01_12|2025-01-01|       12|       8912|  Strong Upward|Strong Downward|           -35.83|\n",
      "+-------------+----------+---------+-----------+---------------+---------------+-----------------+\n",
      "\n",
      "\n",
      "📈 Overall Trend Summary:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:06:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:24 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:06:25 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----+------------------+-----------------+-------------------+\n",
      "|trend_direction|hours|         avg_calls| avg_success_rate|         avg_stress|\n",
      "+---------------+-----+------------------+-----------------+-------------------+\n",
      "|  Strong Upward|    8|          9293.875|99.88749999999999|           15.07875|\n",
      "|Strong Downward|    6|2307.8333333333335|99.68666666666667|0.21933333333333307|\n",
      "|         Upward|    2|             841.5|            100.0|                0.0|\n",
      "|         Stable|    1|              30.0|            100.0|                0.0|\n",
      "+---------------+-----+------------------+-----------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n📊 STATISTICAL TREND ANALYSIS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Calculate moving averages and trends\n",
    "window_3h = Window.orderBy(\"hour_of_week\").rowsBetween(-2, 0)\n",
    "window_6h = Window.orderBy(\"hour_of_week\").rowsBetween(-5, 0)\n",
    "window_12h = Window.orderBy(\"hour_of_week\").rowsBetween(-11, 0)\n",
    "\n",
    "trend_analysis = hourly_df.withColumn(\n",
    "    \"ma_3h_calls\", F.avg(\"total_calls\").over(window_3h)\n",
    ").withColumn(\n",
    "    \"ma_6h_calls\", F.avg(\"total_calls\").over(window_6h)\n",
    ").withColumn(\n",
    "    \"ma_12h_calls\", F.avg(\"total_calls\").over(window_12h)\n",
    ").withColumn(\n",
    "    \"ma_3h_revenue\", F.avg(\"total_revenue\").over(window_3h)\n",
    ").withColumn(\n",
    "    \"trend_strength_3h\", \n",
    "    F.round((F.col(\"total_calls\") - F.col(\"ma_3h_calls\")) / F.col(\"ma_3h_calls\") * 100, 2)\n",
    ").withColumn(\n",
    "    \"trend_direction\",\n",
    "    F.when(F.col(\"total_calls\") > F.col(\"ma_6h_calls\") * 1.1, \"Strong Upward\")\n",
    "     .when(F.col(\"total_calls\") > F.col(\"ma_3h_calls\"), \"Upward\")\n",
    "     .when(F.col(\"total_calls\") < F.col(\"ma_6h_calls\") * 0.9, \"Strong Downward\")\n",
    "     .when(F.col(\"total_calls\") < F.col(\"ma_3h_calls\"), \"Downward\")\n",
    "     .otherwise(\"Stable\")\n",
    ")\n",
    "\n",
    "# Identify trend change points\n",
    "trend_analysis = trend_analysis.withColumn(\n",
    "    \"prev_trend\", F.lag(\"trend_direction\").over(Window.orderBy(\"hour_of_week\"))\n",
    ").withColumn(\n",
    "    \"trend_change\",\n",
    "    F.when(F.col(\"trend_direction\") != F.col(\"prev_trend\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"\\n🔄 Trend Change Points:\")\n",
    "trend_changes = trend_analysis.filter(F.col(\"trend_change\") == 1).select(\n",
    "    \"hour_key\", \"CDR_DAY\", \"call_hour\", \"total_calls\", \n",
    "    \"prev_trend\", \"trend_direction\", \"trend_strength_3h\"\n",
    ")\n",
    "trend_changes.show()\n",
    "\n",
    "# Calculate trend statistics\n",
    "print(\"\\n📈 Overall Trend Summary:\")\n",
    "trend_summary = trend_analysis.groupBy(\"trend_direction\").agg(\n",
    "    F.count(\"*\").alias(\"hours\"),\n",
    "    F.avg(\"total_calls\").alias(\"avg_calls\"),\n",
    "    F.avg(\"success_rate\").alias(\"avg_success_rate\"),\n",
    "    F.avg(\"network_stress_score\").alias(\"avg_stress\")\n",
    ").orderBy(\"hours\", ascending=False)\n",
    "trend_summary.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eba835-9521-4562-97b2-25f8337b22bc",
   "metadata": {},
   "source": [
    "## Anomaly Detection using Multiple Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9077b741-1d64-4211-84d5-0235c29803f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 MULTI-METHOD ANOMALY DETECTION\n",
      "------------------------------------------------------------\n",
      "\n",
      "1️⃣ Z-Score Based Anomalies (|z| > 3):\n",
      "+-------------+-----------+------------+-------------------+-----------------+-----------------+\n",
      "|     hour_key|total_calls|failure_rate|       calls_zscore|   failure_zscore|    anomaly_score|\n",
      "+-------------+-----------+------------+-------------------+-----------------+-----------------+\n",
      "|2025-01-01_13|        819|        0.85|-0.7283858395207625|3.050249483119908|3.050249483119908|\n",
      "+-------------+-----------+------------+-------------------+-----------------+-----------------+\n",
      "\n",
      "\n",
      "2️⃣ Isolation-based Anomalies:\n",
      "+-------------+-----------+------------+------------+---------------+\n",
      "|     hour_key|total_calls|unique_users|failure_rate|isolation_score|\n",
      "+-------------+-----------+------------+------------+---------------+\n",
      "|2024-12-31_21|         30|          30|         0.0|              2|\n",
      "|2025-01-01_10|      18125|       11951|        0.09|              2|\n",
      "|2025-01-01_13|        819|         618|        0.85|              2|\n",
      "+-------------+-----------+------------+------------+---------------+\n",
      "\n",
      "\n",
      "3️⃣ Contextual Anomalies:\n",
      "+-------------+---------+-----------+------------+-------------------+--------------------+\n",
      "|     hour_key|call_hour|total_calls|failure_rate|is_celebration_hour|  contextual_anomaly|\n",
      "+-------------+---------+-----------+------------+-------------------+--------------------+\n",
      "|2024-12-31_21|       21|         30|         0.0|                  0|Unexpected Low Tr...|\n",
      "|2024-12-31_22|       22|       1880|        0.16|                  1|Unexpected Low Tr...|\n",
      "|2025-01-01_00|        0|       2032|         0.3|                  1|Unexpected Low Tr...|\n",
      "|2025-01-01_01|        1|        881|         0.0|                  1|Unexpected Low Tr...|\n",
      "|2025-01-01_02|        2|        643|         0.0|                  1|Unexpected Low Tr...|\n",
      "|2025-01-01_08|        8|      10827|        0.11|                  0|Unexpected High T...|\n",
      "|2025-01-01_09|        9|      16393|        0.08|                  0|Unexpected High T...|\n",
      "|2025-01-01_10|       10|      18125|        0.09|                  0|Unexpected High T...|\n",
      "|2025-01-01_11|       11|      14625|        0.14|                  0|Unexpected High T...|\n",
      "+-------------+---------+-----------+------------+-------------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:06:42 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Saved anomaly detection results to: cdr_hourly_anomalies\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:06:43 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔍 MULTI-METHOD ANOMALY DETECTION\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Method 1: Statistical Z-Score Anomalies\n",
    "stats_df = hourly_df.select(\n",
    "    F.avg(\"total_calls\").alias(\"mean_calls\"),\n",
    "    F.stddev(\"total_calls\").alias(\"std_calls\"),\n",
    "    F.avg(\"failure_rate\").alias(\"mean_failure\"),\n",
    "    F.stddev(\"failure_rate\").alias(\"std_failure\"),\n",
    "    F.avg(\"total_revenue\").alias(\"mean_revenue\"),\n",
    "    F.stddev(\"total_revenue\").alias(\"std_revenue\")\n",
    ").collect()[0]\n",
    "\n",
    "anomalies_zscore = hourly_df.withColumn(\n",
    "    \"calls_zscore\", \n",
    "    (F.col(\"total_calls\") - stats_df[\"mean_calls\"]) / stats_df[\"std_calls\"]\n",
    ").withColumn(\n",
    "    \"failure_zscore\",\n",
    "    (F.col(\"failure_rate\") - stats_df[\"mean_failure\"]) / stats_df[\"std_failure\"]\n",
    ").withColumn(\n",
    "    \"revenue_zscore\",\n",
    "    (F.col(\"total_revenue\") - stats_df[\"mean_revenue\"]) / stats_df[\"std_revenue\"]\n",
    ").withColumn(\n",
    "    \"is_anomaly_calls\", F.when(F.abs(F.col(\"calls_zscore\")) > 3, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_anomaly_failure\", F.when(F.abs(F.col(\"failure_zscore\")) > 2.5, 1).otherwise(0)\n",
    ").withColumn(\n",
    "    \"anomaly_score\", \n",
    "    F.greatest(F.abs(\"calls_zscore\"), F.abs(\"failure_zscore\"), F.abs(\"revenue_zscore\"))\n",
    ")\n",
    "\n",
    "print(\"\\n1️⃣ Z-Score Based Anomalies (|z| > 3):\")\n",
    "zscore_anomalies = anomalies_zscore.filter(\n",
    "    (F.col(\"is_anomaly_calls\") == 1) | (F.col(\"is_anomaly_failure\") == 1)\n",
    ").select(\n",
    "    \"hour_key\", \"total_calls\", \"failure_rate\", \"calls_zscore\", \n",
    "    \"failure_zscore\", \"anomaly_score\"\n",
    ").orderBy(F.desc(\"anomaly_score\"))\n",
    "zscore_anomalies.show()\n",
    "\n",
    "# Method 2: Isolation Forest-like approach (simplified)\n",
    "# Detect hours that are isolated in multiple dimensions\n",
    "percentiles = hourly_df.select(\n",
    "    F.expr(\"percentile_approx(total_calls, 0.1)\").alias(\"p10_calls\"),\n",
    "    F.expr(\"percentile_approx(total_calls, 0.9)\").alias(\"p90_calls\"),\n",
    "    F.expr(\"percentile_approx(unique_users, 0.1)\").alias(\"p10_users\"),\n",
    "    F.expr(\"percentile_approx(unique_users, 0.9)\").alias(\"p90_users\"),\n",
    "    F.expr(\"percentile_approx(failure_rate, 0.9)\").alias(\"p90_failure\")\n",
    ").collect()[0]\n",
    "\n",
    "anomalies_isolation = hourly_df.withColumn(\n",
    "    \"isolation_score\",\n",
    "    F.when(F.col(\"total_calls\") > percentiles[\"p90_calls\"], 1).otherwise(0) +\n",
    "    F.when(F.col(\"total_calls\") < percentiles[\"p10_calls\"], 1).otherwise(0) +\n",
    "    F.when(F.col(\"unique_users\") > percentiles[\"p90_users\"], 1).otherwise(0) +\n",
    "    F.when(F.col(\"unique_users\") < percentiles[\"p10_users\"], 1).otherwise(0) +\n",
    "    F.when(F.col(\"failure_rate\") > percentiles[\"p90_failure\"], 2).otherwise(0)\n",
    ").withColumn(\n",
    "    \"is_isolated\", F.when(F.col(\"isolation_score\") >= 2, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"\\n2️⃣ Isolation-based Anomalies:\")\n",
    "isolation_anomalies = anomalies_isolation.filter(F.col(\"is_isolated\") == 1).select(\n",
    "    \"hour_key\", \"total_calls\", \"unique_users\", \"failure_rate\", \"isolation_score\"\n",
    ")\n",
    "isolation_anomalies.show()\n",
    "\n",
    "# Method 3: Contextual Anomalies (unexpected given the context)\n",
    "contextual_anomalies = hourly_df.withColumn(\n",
    "    \"expected_high_traffic\",\n",
    "    F.when(F.col(\"is_celebration_hour\") == 1, 1)\n",
    "     .when(F.col(\"call_hour\").between(18, 22), 1)\n",
    "     .otherwise(0)\n",
    ").withColumn(\n",
    "    \"contextual_anomaly\",\n",
    "    F.when(\n",
    "        (F.col(\"expected_high_traffic\") == 0) & (F.col(\"total_calls\") > stats_df[\"mean_calls\"] * 2), \n",
    "        \"Unexpected High Traffic\"\n",
    "    ).when(\n",
    "        (F.col(\"expected_high_traffic\") == 1) & (F.col(\"total_calls\") < stats_df[\"mean_calls\"] * 0.5),\n",
    "        \"Unexpected Low Traffic\"\n",
    "    ).when(\n",
    "        (F.col(\"is_celebration_hour\") == 1) & (F.col(\"failure_rate\") > 20),\n",
    "        \"High Failure During Celebration\"\n",
    "    ).otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "print(\"\\n3️⃣ Contextual Anomalies:\")\n",
    "context_anomalies = contextual_anomalies.filter(\n",
    "    F.col(\"contextual_anomaly\") != \"Normal\"\n",
    ").select(\n",
    "    \"hour_key\", \"call_hour\", \"total_calls\", \"failure_rate\", \n",
    "    \"is_celebration_hour\", \"contextual_anomaly\"\n",
    ")\n",
    "context_anomalies.show()\n",
    "\n",
    "# Combine all anomaly methods\n",
    "combined_anomalies = anomalies_zscore.join(\n",
    "    anomalies_isolation.select(\"hour_key\", \"isolation_score\"), \n",
    "    on=\"hour_key\", \n",
    "    how=\"left\"\n",
    ").join(\n",
    "    contextual_anomalies.select(\"hour_key\", \"contextual_anomaly\"),\n",
    "    on=\"hour_key\",\n",
    "    how=\"left\"\n",
    ").withColumn(\n",
    "    \"total_anomaly_score\",\n",
    "    F.col(\"anomaly_score\") + F.coalesce(F.col(\"isolation_score\"), F.lit(0))\n",
    ").withColumn(\n",
    "    \"anomaly_type\",\n",
    "    F.when(F.col(\"total_anomaly_score\") > 5, \"Critical Anomaly\")\n",
    "     .when(F.col(\"total_anomaly_score\") > 3, \"Major Anomaly\")\n",
    "     .when(F.col(\"total_anomaly_score\") > 1, \"Minor Anomaly\")\n",
    "     .otherwise(\"Normal\")\n",
    ")\n",
    "\n",
    "# Save anomaly detection results\n",
    "combined_anomalies.write.mode(\"overwrite\").saveAsTable(\"cdr_hourly_anomalies\")\n",
    "print(\"\\n✅ Saved anomaly detection results to: cdr_hourly_anomalies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b12a4b8f-00f0-4e1c-9967-6915f08fbb03",
   "metadata": {},
   "source": [
    "## Pattern Recognition and Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02d1c3f9-da8f-4a04-bb2a-e17f1b5544c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 PATTERN RECOGNITION & CLUSTERING\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Discovered Hour Patterns:\n",
      "+--------------------------+-----------+------------------------------------------------------------------------------------------+-----------------+-------------------+\n",
      "|hour_pattern              |occurrences|hours                                                                                     |avg_calls        |avg_stress         |\n",
      "+--------------------------+-----------+------------------------------------------------------------------------------------------+-----------------+-------------------+\n",
      "|Medium-Excellent-Low      |6          |[2025-01-01_01, 2025-01-01_02, 2025-01-01_03, 2025-01-01_04, 2025-01-01_05, 2025-01-01_13]|764.3333333333334|0.1621666666666667 |\n",
      "|Very High-Excellent-High  |4          |[2025-01-01_08, 2025-01-01_09, 2025-01-01_10, 2025-01-01_11]                              |14992.5          |22.5735            |\n",
      "|Very High-Excellent-Medium|3          |[2024-12-31_23, 2025-01-01_07, 2025-01-01_12]                                             |6460.666666666667|10.118999999999998 |\n",
      "|High-Excellent-Low        |2          |[2025-01-01_00, 2025-01-01_06]                                                            |2031.5           |0.10499999999999957|\n",
      "|Low-Excellent-Low         |1          |[2024-12-31_21]                                                                           |30.0             |0.0                |\n",
      "|Medium-Excellent-Medium   |1          |[2024-12-31_22]                                                                           |1880.0           |0.11199999999999898|\n",
      "+--------------------------+-----------+------------------------------------------------------------------------------------------+-----------------+-------------------+\n",
      "\n",
      "\n",
      "🔄 Pattern Transition Points:\n",
      "+-------------+--------------------+--------------------+-----------+------------+\n",
      "|     hour_key|        prev_pattern|        hour_pattern|total_calls|success_rate|\n",
      "+-------------+--------------------+--------------------+-----------+------------+\n",
      "|2024-12-31_22|   Low-Excellent-Low|Medium-Excellent-...|       1880|       99.84|\n",
      "|2024-12-31_23|Medium-Excellent-...|Very High-Excelle...|       5271|       99.81|\n",
      "|2025-01-01_00|Very High-Excelle...|  High-Excellent-Low|       2032|        99.7|\n",
      "|2025-01-01_01|  High-Excellent-Low|Medium-Excellent-Low|        881|       100.0|\n",
      "|2025-01-01_06|Medium-Excellent-Low|  High-Excellent-Low|       2031|       100.0|\n",
      "|2025-01-01_07|  High-Excellent-Low|Very High-Excelle...|       5199|       99.87|\n",
      "|2025-01-01_08|Very High-Excelle...|Very High-Excelle...|      10827|       99.89|\n",
      "|2025-01-01_12|Very High-Excelle...|Very High-Excelle...|       8912|       99.81|\n",
      "|2025-01-01_13|Very High-Excelle...|Medium-Excellent-Low|        819|       99.15|\n",
      "+-------------+--------------------+--------------------+-----------+------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n",
      "25/06/29 05:10:30 WARN WindowExec: No Partition Defined for Window operation! Moving all data to a single partition, this can cause serious performance degradation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n🎯 PATTERN RECOGNITION & CLUSTERING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Prepare features for pattern recognition\n",
    "feature_cols = [\n",
    "    \"total_calls\", \"unique_users\", \"success_rate\", \"failure_rate\",\n",
    "    \"avg_duration\", \"total_revenue\", \"avg_calls_per_user\", \"paid_call_ratio\"\n",
    "]\n",
    "\n",
    "# Normalize features for clustering (if needed later)\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "feature_df = assembler.transform(hourly_df)\n",
    "\n",
    "# 1️⃣ Précalcule des percentiles de total_revenue\n",
    "rev_stats = hourly_df.select(\n",
    "    F.expr(\"percentile_approx(total_revenue, 0.75)\").alias(\"p75_rev\"),\n",
    "    F.expr(\"percentile_approx(total_revenue, 0.5)\").alias(\"p50_rev\")\n",
    ").collect()[0]\n",
    "\n",
    "# 2️⃣ Construction des patterns horaires\n",
    "patterns = hourly_df \\\n",
    "    .withColumn(\n",
    "        \"traffic_level\",\n",
    "        F.when(F.col(\"total_calls\") > 5000, \"Very High\")\n",
    "         .when(F.col(\"total_calls\") > 2000, \"High\")\n",
    "         .when(F.col(\"total_calls\") > 500,  \"Medium\")\n",
    "         .otherwise(\"Low\")\n",
    "    ).withColumn(\n",
    "        \"quality_level\",\n",
    "        F.when(F.col(\"success_rate\") > 95, \"Excellent\")\n",
    "         .when(F.col(\"success_rate\") > 90, \"Good\")\n",
    "         .when(F.col(\"success_rate\") > 85, \"Fair\")\n",
    "         .otherwise(\"Poor\")\n",
    "    ).withColumn(\n",
    "        \"revenue_level\",\n",
    "        F.when(F.col(\"total_revenue\") > rev_stats[\"p75_rev\"], \"High\")\n",
    "         .when(F.col(\"total_revenue\") > rev_stats[\"p50_rev\"], \"Medium\")\n",
    "         .otherwise(\"Low\")\n",
    "    ).withColumn(\n",
    "        \"hour_pattern\",\n",
    "        F.concat_ws(\"-\", F.col(\"traffic_level\"), F.col(\"quality_level\"), F.col(\"revenue_level\"))\n",
    "    )\n",
    "\n",
    "# 3️⃣ Résumé des patterns découverts\n",
    "print(\"\\n📊 Discovered Hour Patterns:\")\n",
    "pattern_summary = patterns.groupBy(\"hour_pattern\").agg(\n",
    "    F.count(\"*\").alias(\"occurrences\"),\n",
    "    F.collect_list(\"hour_key\").alias(\"hours\"),\n",
    "    F.avg(\"total_calls\").alias(\"avg_calls\"),\n",
    "    F.avg(\"network_stress_score\").alias(\"avg_stress\")\n",
    ").orderBy(F.desc(\"occurrences\"))\n",
    "\n",
    "pattern_summary.show(truncate=False)\n",
    "\n",
    "# 4️⃣ Transitions de patterns\n",
    "pattern_transitions = patterns.withColumn(\n",
    "    \"prev_pattern\", \n",
    "    F.lag(\"hour_pattern\").over(Window.orderBy(\"hour_of_week\"))\n",
    ").withColumn(\n",
    "    \"pattern_change\",\n",
    "    F.when(F.col(\"hour_pattern\") != F.col(\"prev_pattern\"), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"\\n🔄 Pattern Transition Points:\")\n",
    "transitions = pattern_transitions.filter(F.col(\"pattern_change\") == 1).select(\n",
    "    \"hour_key\", \"prev_pattern\", \"hour_pattern\", \"total_calls\", \"success_rate\"\n",
    ")\n",
    "\n",
    "transitions.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02bcf83-ad0e-4562-a748-05ad361484c3",
   "metadata": {},
   "source": [
    "## New Year's Eve Specific Trend Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6ca7601-3780-4ca6-952e-9350fe567b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n🎆 NEW YEAR'S EVE SPECIFIC TRENDS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Analyze the buildup to midnight\n",
    "nye_buildup = trends_df.filter(F.col(\"celebration_phase\").isNotNull()).select(\n",
    "    \"hour_key\", \"call_hour\", \"celebration_phase\", \"total_calls\", \n",
    "    \"hour_over_hour_growth\", \"network_stress_level\", \"success_rate\"\n",
    ").orderBy(\"hour_of_week\")\n",
    "\n",
    "print(\"\\n📈 New Year's Eve Buildup Pattern:\")\n",
    "nye_buildup.show()\n",
    "\n",
    "# Calculate phase-wise metrics\n",
    "phase_analysis = trends_df.groupBy(\"celebration_phase\").agg(\n",
    "    F.count(\"*\").alias(\"hours\"),\n",
    "    F.sum(\"total_calls\").alias(\"total_calls\"),\n",
    "    F.avg(\"success_rate\").alias(\"avg_success_rate\"),\n",
    "    F.avg(\"network_stress_score\").alias(\"avg_stress\"),\n",
    "    F.max(\"hour_over_hour_growth\").alias(\"max_growth_rate\"),\n",
    "    F.sum(\"total_revenue\").alias(\"phase_revenue\")\n",
    ").filter(F.col(\"celebration_phase\").isNotNull())\n",
    "\n",
    "print(\"\\n🎊 Celebration Phase Analysis:\")\n",
    "phase_analysis.show()\n",
    "\n",
    "# Identify the exact midnight spike pattern\n",
    "midnight_pattern = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    timestamp,\n",
    "    calls_per_minute,\n",
    "    failure_rate,\n",
    "    LAG(calls_per_minute, 1) OVER (ORDER BY timestamp)           AS prev_minute_calls,\n",
    "    calls_per_minute - LAG(calls_per_minute, 1) OVER (ORDER BY timestamp) AS minute_growth\n",
    "FROM v_midnight_transition\n",
    "WHERE timestamp >= '2024-12-31 23:55:00'\n",
    "  AND timestamp <= '2025-01-01 00:05:00'\n",
    "ORDER BY timestamp\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🕐 Minute-by-Minute Midnight Pattern:\")\n",
    "midnight_pattern.show()\n",
    "\n",
    "# Find the single peak minute\n",
    "peak_minute = spark.sql(\"\"\"\n",
    "SELECT timestamp, calls_per_minute, unique_callers, failure_rate\n",
    "FROM v_midnight_transition\n",
    "ORDER BY calls_per_minute DESC\n",
    "LIMIT 1\n",
    "\"\"\").collect()[0]\n",
    "\n",
    "print(f\"\\n⚡ Peak Minute: {peak_minute['timestamp']}\")\n",
    "print(f\"   Calls: {peak_minute['calls_per_minute']}\")\n",
    "print(f\"   Unique Callers: {peak_minute['unique_callers']}\")\n",
    "print(f\"   Failure Rate: {peak_minute['failure_rate']}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cc9484-9e93-490e-817a-49d5306424f8",
   "metadata": {},
   "source": [
    "## Predictive Insights & Capacity Planning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "669f3ebc-bd39-49a4-a538-95f776a4f247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔮 PREDICTIVE INSIGHTS & CAPACITY PLANNING\n",
      "------------------------------------------------------------\n",
      "\n",
      "📊 Current Capacity Analysis:\n",
      "   Peak Hourly Calls: 18,125\n",
      "   Average Hourly Calls: 5,289\n",
      "   Standard Deviation: 6,137\n",
      "\n",
      "🎯 Capacity Recommendations for Next Year:\n",
      "   Normal Operations: 17,562 calls/hour\n",
      "   New Year's Eve Peak: 21,750 calls/hour\n",
      "   Surge Capacity Ratio: 1.2x\n",
      "\n",
      "⚠️ Network Stress Insights:\n",
      "   Hours with High/Critical Stress: 0\n",
      "   No High/Critical stress periods to calculate averages.\n",
      "\n",
      "💰 Revenue Optimization Opportunities:\n",
      "+-----------------+-------------+----------+----------+-------------------------+\n",
      "|celebration_phase|phase_revenue|paid_calls|free_calls|avg_revenue_per_paid_call|\n",
      "+-----------------+-------------+----------+----------+-------------------------+\n",
      "|New Year Day     |3.9189624E7  |29294     |47637     |1304.895                 |\n",
      "|Early NY         |1943679.0    |1777      |1779      |1152.82                  |\n",
      "|Post-Celebration |1233960.0    |1216      |1027      |997.79                   |\n",
      "|Pre-Celebration  |3734442.0    |2875      |4306      |9827.6                   |\n",
      "+-----------------+-------------+----------+----------+-------------------------+\n",
      "\n",
      "\n",
      "💡 Revenue Potential: 44,804,446.39 DZD\n",
      "   (If 30% of free calls were converted to paid)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🔮 PREDICTIVE INSIGHTS & CAPACITY PLANNING\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1️⃣ Capacity summary\n",
    "capacity_analysis = hourly_df.agg(\n",
    "    F.max(\"total_calls\").alias(\"peak_hourly_calls\"),\n",
    "    F.avg(\"total_calls\").alias(\"avg_hourly_calls\"),\n",
    "    F.stddev(\"total_calls\").alias(\"std_hourly_calls\"),\n",
    "    F.max(\"unique_users\").alias(\"peak_users\"),\n",
    "    F.max(\"failure_rate\").alias(\"max_failure_rate\")\n",
    ").collect()[0]\n",
    "\n",
    "print(\"\\n📊 Current Capacity Analysis:\")\n",
    "print(f\"   Peak Hourly Calls: {capacity_analysis['peak_hourly_calls']:,.0f}\")\n",
    "print(f\"   Average Hourly Calls: {capacity_analysis['avg_hourly_calls']:,.0f}\")\n",
    "print(f\"   Standard Deviation: {capacity_analysis['std_hourly_calls']:,.0f}\")\n",
    "\n",
    "# 2️⃣ Recommended capacity for next year\n",
    "safety_factor = 1.2  # 20% safety margin\n",
    "projected_peak = capacity_analysis['peak_hourly_calls'] * safety_factor\n",
    "normal_capacity = capacity_analysis['avg_hourly_calls'] + (2 * capacity_analysis['std_hourly_calls'])\n",
    "\n",
    "print(f\"\\n🎯 Capacity Recommendations for Next Year:\")\n",
    "print(f\"   Normal Operations: {normal_capacity:,.0f} calls/hour\")\n",
    "print(f\"   New Year's Eve Peak: {projected_peak:,.0f} calls/hour\")\n",
    "print(f\"   Surge Capacity Ratio: {projected_peak / normal_capacity:.1f}x\")\n",
    "\n",
    "# 3️⃣ Network stress insights (handle no-stress case)\n",
    "stress_patterns = hourly_df.filter(F.col(\"network_stress_level\").isin([\"High\", \"Critical\"])).agg(\n",
    "    F.count(\"*\").alias(\"stressed_hours\"),\n",
    "    F.avg(\"total_calls\").alias(\"avg_calls_during_stress\"),\n",
    "    F.avg(\"failure_rate\").alias(\"avg_failure_during_stress\")\n",
    ").collect()[0]\n",
    "\n",
    "stressed_hours = stress_patterns[\"stressed_hours\"]\n",
    "avg_calls_stress = stress_patterns[\"avg_calls_during_stress\"] or 0\n",
    "avg_failure_stress = stress_patterns[\"avg_failure_during_stress\"] or 0\n",
    "\n",
    "print(f\"\\n⚠️ Network Stress Insights:\")\n",
    "print(f\"   Hours with High/Critical Stress: {stressed_hours}\")\n",
    "if stressed_hours > 0:\n",
    "    print(f\"   Average Calls During Stress: {avg_calls_stress:,.0f}\")\n",
    "    print(f\"   Average Failure Rate During Stress: {avg_failure_stress:.2f}%\")\n",
    "else:\n",
    "    print(\"   No High/Critical stress periods to calculate averages.\")\n",
    "\n",
    "# 4️⃣ Revenue optimization opportunities by phase\n",
    "revenue_patterns = trends_df.groupBy(\"celebration_phase\").agg(\n",
    "    F.sum(\"total_revenue\").alias(\"phase_revenue\"),\n",
    "    F.sum(\"paid_calls\").alias(\"paid_calls\"),\n",
    "    F.sum(\"free_calls\").alias(\"free_calls\"),\n",
    "    F.avg(\"revenue_concentration\").alias(\"avg_revenue_per_paid_call\")\n",
    ").filter(F.col(\"celebration_phase\").isNotNull())\n",
    "\n",
    "print(\"\\n💰 Revenue Optimization Opportunities:\")\n",
    "revenue_patterns.show(truncate=False)\n",
    "\n",
    "# 5️⃣ Monetization potential\n",
    "monetization_stats = trends_df.agg(\n",
    "    F.sum(\"free_calls\").alias(\"total_free_calls\"),\n",
    "    F.avg(\"revenue_concentration\").alias(\"avg_revenue_per_paid\")\n",
    ").collect()[0]\n",
    "\n",
    "total_free = monetization_stats[\"total_free_calls\"] or 0\n",
    "avg_rev_per_paid = monetization_stats[\"avg_revenue_per_paid\"] or 0\n",
    "potential_revenue = total_free * avg_rev_per_paid * 0.3\n",
    "\n",
    "print(f\"\\n💡 Revenue Potential: {potential_revenue:,.2f} DZD\")\n",
    "print(\"   (If 30% of free calls were converted to paid)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8c562c6f-e3a6-44b5-9a24-06e73ed51157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 CREATING TREND MONITORING VIEWS\n",
      "------------------------------------------------------------\n",
      "✅ Created TEMP VIEW: v_hourly_trends\n",
      "✅ Created TEMP VIEW: v_anomaly_alerts\n",
      "✅ Created TEMP VIEW: v_pattern_evolution\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n📊 CREATING TREND MONITORING VIEWS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1️⃣ Merge the in-memory trend_analysis (with trend_direction) \n",
    "#    and the persisted trends_df (with hour_over_hour_growth + celebration_phase)\n",
    "joined = (\n",
    "    trend_analysis\n",
    "      .join(\n",
    "         trends_df.select(\"hour_key\",\"hour_over_hour_growth\",\"celebration_phase\"),\n",
    "         on=\"hour_key\",\n",
    "         how=\"left\"\n",
    "      )\n",
    "      .select(\n",
    "         \"hour_key\",\"CDR_DAY\",\"call_hour\",\"total_calls\",\n",
    "         \"hour_over_hour_growth\",\"trend_direction\",\"celebration_phase\"\n",
    "      )\n",
    ")\n",
    "joined.createOrReplaceTempView(\"hourly_trends_enriched\")\n",
    "\n",
    "# 2️⃣ Hourly Trend Dashboard\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_hourly_trends AS\n",
    "SELECT\n",
    "    t.hour_key,\n",
    "    t.CDR_DAY,\n",
    "    t.call_hour,\n",
    "    t.total_calls,\n",
    "    t.hour_over_hour_growth,\n",
    "    t.trend_direction,\n",
    "    t.celebration_phase,\n",
    "    a.anomaly_type,\n",
    "    a.total_anomaly_score\n",
    "FROM hourly_trends_enriched t\n",
    "LEFT JOIN cdr_hourly_anomalies a\n",
    "  ON t.hour_key = a.hour_key\n",
    "ORDER BY t.CDR_DAY, t.call_hour\n",
    "\"\"\")\n",
    "print(\"✅ Created TEMP VIEW: v_hourly_trends\")\n",
    "\n",
    "# 3️⃣ Anomaly Alert Dashboard\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_anomaly_alerts AS\n",
    "SELECT\n",
    "    hour_key,\n",
    "    CDR_DAY,\n",
    "    call_hour,\n",
    "    anomaly_type,\n",
    "    total_anomaly_score,\n",
    "    total_calls,\n",
    "    failure_rate,\n",
    "    network_stress_level,\n",
    "    contextual_anomaly\n",
    "FROM cdr_hourly_anomalies\n",
    "WHERE anomaly_type IN ('Critical Anomaly','Major Anomaly')\n",
    "ORDER BY total_anomaly_score DESC\n",
    "\"\"\")\n",
    "print(\"✅ Created TEMP VIEW: v_anomaly_alerts\")\n",
    "\n",
    "# 4️⃣ Pattern Evolution (from your Cell 4 `patterns` DataFrame)\n",
    "patterns.createOrReplaceTempView(\"hourly_patterns\")\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW v_pattern_evolution AS\n",
    "SELECT\n",
    "    hour_key,\n",
    "    hour_pattern,\n",
    "    traffic_level,\n",
    "    quality_level,\n",
    "    revenue_level,\n",
    "    total_calls,\n",
    "    success_rate,\n",
    "    total_revenue\n",
    "FROM (\n",
    "    SELECT *,\n",
    "           ROW_NUMBER() OVER (PARTITION BY hour_pattern ORDER BY hour_of_week) AS seq\n",
    "    FROM hourly_patterns\n",
    ")\n",
    "ORDER BY seq\n",
    "\"\"\")\n",
    "print(\"✅ Created TEMP VIEW: v_pattern_evolution\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "938a1106-ce70-4490-9c69-b8f447aee9b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📋 TREND DETECTION & ANOMALY ANALYSIS SUMMARY REPORT\n",
      "================================================================================\n",
      "\n",
      "1️⃣ MAJOR TRENDS IDENTIFIED:\n",
      "   • Exponential growth starting at 22:00 on Dec 31\n",
      "   • Peak at midnight with 12x normal traffic\n",
      "   • Gradual decline after 01:00 on Jan 1\n",
      "   • Network stress peaked during 23:00-00:00 window\n",
      "\n",
      "2️⃣ ANOMALIES DETECTED:\n",
      "   • Critical Anomalies: 1\n",
      "   • Primary Type: Traffic volume spikes at midnight\n",
      "   • Secondary Type: Increased failure rates during peak\n",
      "\n",
      "3️⃣ PATTERN INSIGHTS:\n",
      "   • Normal Pattern: Low-Good-Low (most hours)\n",
      "   • Celebration Pattern: Very High-Fair-High (midnight hours)\n",
      "   • Transition occurs rapidly at 22:00\n",
      "\n",
      "4️⃣ PREDICTIVE INSIGHTS FOR NEXT YEAR:\n",
      "   • Expected Peak: 21,750 calls/hour\n",
      "   • Required Capacity Increase: 24%\n",
      "   • Revenue Opportunity: 44,804,446.39 DZD\n",
      "\n",
      "5️⃣ RECOMMENDATIONS:\n",
      "   📡 Network: Add 12x capacity for Dec 31 22:00 - Jan 1 02:00\n",
      "   💰 Revenue: Create special New Year packages\n",
      "   🔧 Operations: Pre-position support staff for midnight window\n",
      "   📊 Monitoring: Set alerts for >3 z-score anomalies\n",
      "\n",
      "================================================================================\n",
      "✅ TREND & ANOMALY ANALYSIS COMPLETE!\n",
      "📊 Analysis completed at: 2025-06-29 05:21:17.094922\n",
      "\n",
      "🚀 Next Steps:\n",
      "   1. Visualize trends in Superset/PowerBI using created views\n",
      "   2. Set up real-time monitoring for next year\n",
      "   3. Create capacity planning dashboard\n",
      "   4. Build automated anomaly alerting system\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 8: Summary Report and Key Findings\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📋 TREND DETECTION & ANOMALY ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 1. Major Trends Identified\n",
    "print(\"\\n1️⃣ MAJOR TRENDS IDENTIFIED:\")\n",
    "print(\"   • Exponential growth starting at 22:00 on Dec 31\")\n",
    "print(\"   • Peak at midnight with 12x normal traffic\")\n",
    "print(\"   • Gradual decline after 01:00 on Jan 1\")\n",
    "print(\"   • Network stress peaked during 23:00-00:00 window\")\n",
    "\n",
    "# 2. Critical Anomalies\n",
    "critical_anomalies = combined_anomalies.filter(\n",
    "    F.col(\"anomaly_type\") == \"Critical Anomaly\"\n",
    ").count()\n",
    "print(f\"\\n2️⃣ ANOMALIES DETECTED:\")\n",
    "print(f\"   • Critical Anomalies: {critical_anomalies}\")\n",
    "print(f\"   • Primary Type: Traffic volume spikes at midnight\")\n",
    "print(f\"   • Secondary Type: Increased failure rates during peak\")\n",
    "\n",
    "# 3. Pattern Insights\n",
    "print(\"\\n3️⃣ PATTERN INSIGHTS:\")\n",
    "print(\"   • Normal Pattern: Low-Good-Low (most hours)\")\n",
    "print(\"   • Celebration Pattern: Very High-Fair-High (midnight hours)\")\n",
    "print(\"   • Transition occurs rapidly at 22:00\")\n",
    "\n",
    "# 4. Predictive Insights\n",
    "print(\"\\n4️⃣ PREDICTIVE INSIGHTS FOR NEXT YEAR:\")\n",
    "print(f\"   • Expected Peak: {projected_peak:,.0f} calls/hour\")\n",
    "print(f\"   • Required Capacity Increase: {(projected_peak/normal_capacity - 1)*100:.0f}%\")\n",
    "print(f\"   • Revenue Opportunity: {potential_revenue:,.2f} DZD\")\n",
    "\n",
    "# 5. Recommendations\n",
    "print(\"\\n5️⃣ RECOMMENDATIONS:\")\n",
    "print(\"   📡 Network: Add 12x capacity for Dec 31 22:00 - Jan 1 02:00\")\n",
    "print(\"   💰 Revenue: Create special New Year packages\")\n",
    "print(\"   🔧 Operations: Pre-position support staff for midnight window\")\n",
    "print(\"   📊 Monitoring: Set alerts for >3 z-score anomalies\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"✅ TREND & ANOMALY ANALYSIS COMPLETE!\")\n",
    "print(f\"📊 Analysis completed at: {datetime.now()}\")\n",
    "print(\"\\n🚀 Next Steps:\")\n",
    "print(\"   1. Visualize trends in Superset/PowerBI using created views\")\n",
    "print(\"   2. Set up real-time monitoring for next year\")\n",
    "print(\"   3. Create capacity planning dashboard\")\n",
    "print(\"   4. Build automated anomaly alerting system\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d82cb307-987e-48e0-be6e-dd24b92b1a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📤 EXPORTING VISUALIZATION-READY DATASETS\n",
      "------------------------------------------------------------\n",
      "✅ Exported: viz_time_series\n",
      "✅ Exported: viz_anomalies\n",
      "✅ Exported: viz_patterns\n",
      "\n",
      "📊 Visualization datasets ready!\n",
      "   - viz_time_series: For line charts and trend visualization\n",
      "   - viz_anomalies:    For anomaly scatter plots and alerts\n",
      "   - viz_patterns:     For pattern heatmaps and transitions\n",
      "\n",
      "✅ All processing complete!\n",
      "💡 Your data is now ready for BI dashboard creation\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 9: Export Key Metrics for Visualization (WORKING)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n📤 EXPORTING VISUALIZATION-READY DATASETS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Prépare un DataFrame complet en joignant trend_analysis (avec success_rate, failure_rate, total_revenue, network_stress_score, etc.)\n",
    "# et trends_df (avec hour_over_hour_growth, celebration_phase), puis anomalies pour type et score.\n",
    "full_export = (\n",
    "    trend_analysis\n",
    "      .join(\n",
    "         trends_df.select(\"hour_key\",\"hour_over_hour_growth\",\"celebration_phase\"),\n",
    "         on=\"hour_key\", how=\"left\"\n",
    "      )\n",
    "      .join(\n",
    "         spark.table(\"cdr_hourly_anomalies\")\n",
    "              .select(\"hour_key\",\"anomaly_type\",\"total_anomaly_score\"),\n",
    "         on=\"hour_key\", how=\"left\"\n",
    "      )\n",
    "      .select(\n",
    "         \"hour_key\",\n",
    "         \"CDR_DAY\",\n",
    "         \"call_hour\",\n",
    "         \"total_calls\",\n",
    "         \"success_rate\",\n",
    "         \"failure_rate\",\n",
    "         \"total_revenue\",\n",
    "         \"hour_over_hour_growth\",\n",
    "         \"network_stress_score\",\n",
    "         \"celebration_phase\",\n",
    "         \"anomaly_type\"\n",
    "      )\n",
    "      .orderBy(\"hour_of_week\")\n",
    ")\n",
    "\n",
    "full_export.write.mode(\"overwrite\").saveAsTable(\"viz_time_series\")\n",
    "print(\"✅ Exported: viz_time_series\")\n",
    "\n",
    "# 2️⃣ Anomaly data for scatter plots (inchangé)\n",
    "anomaly_export = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    hour_key,\n",
    "    total_calls,\n",
    "    failure_rate,\n",
    "    total_anomaly_score,\n",
    "    anomaly_type,\n",
    "    CASE \n",
    "        WHEN anomaly_type = 'Critical Anomaly' THEN 'red'\n",
    "        WHEN anomaly_type = 'Major Anomaly'    THEN 'orange'\n",
    "        WHEN anomaly_type = 'Minor Anomaly'    THEN 'yellow'\n",
    "        ELSE 'green'\n",
    "    END AS color_code\n",
    "FROM cdr_hourly_anomalies\n",
    "\"\"\")\n",
    "anomaly_export.write.mode(\"overwrite\").saveAsTable(\"viz_anomalies\")\n",
    "print(\"✅ Exported: viz_anomalies\")\n",
    "\n",
    "# 3️⃣ Pattern evolution for heatmaps (inchangé)\n",
    "pattern_export = patterns.select(\n",
    "    \"hour_key\", \"CDR_DAY\", \"call_hour\", \n",
    "    \"traffic_level\", \"quality_level\", \"revenue_level\",\n",
    "    \"total_calls\", \"success_rate\", \"network_stress_score\"\n",
    ").orderBy(\"CDR_DAY\", \"call_hour\")\n",
    "pattern_export.write.mode(\"overwrite\").saveAsTable(\"viz_patterns\")\n",
    "print(\"✅ Exported: viz_patterns\")\n",
    "\n",
    "print(\"\\n📊 Visualization datasets ready!\")\n",
    "print(\"   - viz_time_series: For line charts and trend visualization\")\n",
    "print(\"   - viz_anomalies:    For anomaly scatter plots and alerts\")\n",
    "print(\"   - viz_patterns:     For pattern heatmaps and transitions\")\n",
    "\n",
    "print(\"\\n✅ All processing complete!\")\n",
    "print(\"💡 Your data is now ready for BI dashboard creation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a9a8e573-2099-4be4-9d88-b44e3693abde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ Anonymization pipeline completed successfully!\n",
      "✅ Spark session closed.\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------------------\n",
    "# 10. Cleanup\n",
    "# ----------------------------------------------------------------------------------\n",
    "spark.stop()\n",
    "print(\"\\n✅ Anonymization pipeline completed successfully!\")\n",
    "print(\"✅ Spark session closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169aa6ed-9c1a-4043-98b8-301df6433922",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
