{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43a6f671-2e35-4deb-b475-9f34700f7e55",
   "metadata": {},
   "source": [
    "# CDR Telecom - Creating Compelling New Year's Eve Dashboards"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "761143c3-8232-4fa8-9ca6-4cf51422773e",
   "metadata": {},
   "source": [
    "# ============================================================\n",
    "# NOTEBOOK 04: VISUALIZATION & DASHBOARD SETUP\n",
    "# Project: CDR Telecom Big Data Engineering Final Year Internship\n",
    "# Focus: Creating visualizations for Superset & PowerBI\n",
    "# ============================================================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "385766b9-e3e3-4572-b8c3-90b2f5a60fd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/29 05:27:33 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession initialized (App: CDR Visualization Setup, Spark: 3.5.1)\n",
      "✅ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "✅ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "================================================================================\n",
      "📊 CDR VISUALIZATION & DASHBOARD SETUP\n",
      "================================================================================\n",
      "Creating optimized datasets for Superset and PowerBI\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:27:36 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Cell 1: Setup and Prepare Visualization Datasets\n",
    "# ------------------------------------------------------------\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "spark = init_spark(\"CDR Visualization Setup\")\n",
    "\n",
    "DATABASE_NAME = \"algerie_telecom_cdr\"\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"📊 CDR VISUALIZATION & DASHBOARD SETUP\")\n",
    "print(\"=\" * 80)\n",
    "print(\"Creating optimized datasets for Superset and PowerBI\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29e4fafe-8184-4dc0-9711-f65a307b0060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 CREATING MASTER TIME SERIES DATASET\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/29 05:32:09 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created master time series dataset: viz_master_timeseries\n",
      "\n",
      "📊 Dataset Summary:\n",
      "Total Hours: 17\n",
      "Columns: 31\n",
      "+-------------+----------+---------+-------------------+------------+-----------------+-----------+------------+----------------+------------+------------+------------+------------+-----------------+---------------+-------------+-----------+----------+----------+---------------+-----------+---------+-------------+---------------------+------------------+-------------+--------------------+--------------------+-------------+-------------------+----------------+\n",
      "|hour_key     |CDR_DAY   |call_hour|timestamp          |hour_of_week|celebration_phase|total_calls|unique_users|successful_calls|failed_calls|active_cells|success_rate|failure_rate|avg_duration     |median_duration|total_revenue|hourly_arpu|paid_calls|free_calls|paid_call_ratio|voice_calls|sms_count|data_sessions|hour_over_hour_growth|calls_vs_daily_avg|is_spike_hour|network_stress_score|network_stress_level|anomaly_type |total_anomaly_score|anomaly_severity|\n",
      "+-------------+----------+---------+-------------------+------------+-----------------+-----------+------------+----------------+------------+------------+------------+------------+-----------------+---------------+-------------+-----------+----------+----------+---------------+-----------+---------+-------------+---------------------+------------------+-------------+--------------------+--------------------+-------------+-------------------+----------------+\n",
      "|2024-12-31_21|2024-12-31|21       |2024-12-31 21:00:00|21          |Normal           |30         |30          |30              |0           |0           |100.0       |0.0         |2907.0           |3443.0         |216000.0     |7200.0     |8         |22        |26.67          |30         |0        |0            |NULL                 |-98.75            |Normal       |0.0                 |Low                 |Minor Anomaly|2.8569566568273372 |1               |\n",
      "|2024-12-31_22|2024-12-31|22       |2024-12-31 22:00:00|22          |Celebration      |1880       |1076        |1877            |3           |1           |99.84       |0.16        |146.9563829787234|48.0           |1928527.0    |1792.31    |1334      |546       |70.96          |1880       |0        |0            |6166.67              |-21.46            |Normal       |0.11199999999999898 |Low                 |Normal       |0.5554914958423397 |0               |\n",
      "|2024-12-31_23|2024-12-31|23       |2024-12-31 23:00:00|23          |Celebration      |5271       |3116        |5261            |10          |1           |99.81       |0.19        |74.09618668184405|53.0           |1589915.0    |510.24     |1533      |3738      |29.08          |5271       |0        |0            |180.37               |120.21            |Minor Spike  |30.133              |Medium              |Normal       |0.36112433565735486|0               |\n",
      "|2025-01-01_00|2025-01-01|0        |2025-01-01 00:00:00|24          |Celebration      |2032       |982         |2026            |6           |1           |99.7        |0.3         |64.12795275590551|43.0           |810174.0     |825.02     |881       |1151      |43.36          |2032       |0        |0            |-61.45               |-65.61            |Normal       |0.20999999999999913 |Low                 |Normal       |0.6121011622626417 |0               |\n",
      "|2025-01-01_01|2025-01-01|1        |2025-01-01 01:00:00|25          |Celebration      |881        |440         |881             |0           |1           |100.0       |0.0         |68.51645856980704|29.0           |612753.0     |1392.62    |498       |383       |56.53          |881        |0        |0            |-56.64               |-85.09            |Normal       |0.0                 |Low                 |Normal       |0.7266232701862336 |0               |\n",
      "+-------------+----------+---------+-------------------+------------+-----------------+-----------+------------+----------------+------------+------------+------------+------------+-----------------+---------------+-------------+-----------+----------+----------+---------------+-----------+---------+-------------+---------------------+------------------+-------------+--------------------+--------------------+-------------+-------------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 2: Create Master Time Series Dataset (FIXED)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n📈 CREATING MASTER TIME SERIES DATASET\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "master_timeseries = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    -- Time dimensions\n",
    "    h.hour_key,\n",
    "    h.CDR_DAY,\n",
    "    h.call_hour,\n",
    "    CONCAT(h.CDR_DAY, ' ', LPAD(h.call_hour, 2, '0'), ':00:00') AS timestamp,\n",
    "    h.hour_of_week,\n",
    "\n",
    "    -- On compare is_celebration_hour à 1\n",
    "    CASE WHEN h.is_celebration_hour = 1 THEN 'Celebration' ELSE 'Normal' END AS celebration_phase,\n",
    "\n",
    "    -- Volume metrics\n",
    "    h.total_calls,\n",
    "    h.unique_users,\n",
    "    h.successful_calls,\n",
    "    h.failed_calls,\n",
    "    h.active_cells,\n",
    "\n",
    "    -- Performance metrics\n",
    "    h.success_rate,\n",
    "    h.failure_rate,\n",
    "    h.avg_duration,\n",
    "    h.median_duration,\n",
    "\n",
    "    -- Revenue metrics\n",
    "    h.total_revenue,\n",
    "    h.hourly_arpu,\n",
    "    h.paid_calls,\n",
    "    h.free_calls,\n",
    "    h.paid_call_ratio,\n",
    "\n",
    "    -- Service breakdown\n",
    "    h.voice_calls,\n",
    "    h.sms_count,\n",
    "    h.data_sessions,\n",
    "\n",
    "    -- Trend indicator\n",
    "    t.hour_over_hour_growth,\n",
    "\n",
    "    -- Daily-normal comparisons\n",
    "    h.calls_vs_daily_avg,\n",
    "    h.is_spike_hour,\n",
    "\n",
    "    -- Network health\n",
    "    h.network_stress_score,\n",
    "    h.network_stress_level,\n",
    "\n",
    "    -- Anomaly indicators\n",
    "    a.anomaly_type,\n",
    "    a.total_anomaly_score,\n",
    "    CASE \n",
    "        WHEN a.anomaly_type = 'Critical Anomaly' THEN 3\n",
    "        WHEN a.anomaly_type = 'Major Anomaly'    THEN 2\n",
    "        WHEN a.anomaly_type = 'Minor Anomaly'    THEN 1\n",
    "        ELSE 0\n",
    "    END AS anomaly_severity\n",
    "\n",
    "FROM cdr_hourly_features h\n",
    "LEFT JOIN cdr_hourly_trends   t ON h.hour_key = t.hour_key\n",
    "LEFT JOIN cdr_hourly_anomalies a ON h.hour_key = a.hour_key\n",
    "\n",
    "ORDER BY h.hour_of_week\n",
    "\"\"\")\n",
    "\n",
    "master_timeseries.write.mode(\"overwrite\").saveAsTable(\"viz_master_timeseries\")\n",
    "print(\"✅ Created master time series dataset: viz_master_timeseries\")\n",
    "\n",
    "# Show summary\n",
    "print(\"\\n📊 Dataset Summary:\")\n",
    "print(f\"Total Hours: {master_timeseries.count()}\")\n",
    "print(f\"Columns: {len(master_timeseries.columns)}\")\n",
    "master_timeseries.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4729e677-59b5-4f5a-bfad-f1eafb0c69f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 CREATING KPI SUMMARY TABLES\n",
      "------------------------------------------------------------\n",
      "✅ Created overall KPIs table: viz_overall_kpis\n",
      "✅ Created daily KPIs table: viz_daily_kpis\n",
      "✅ Created celebration phase KPIs table: viz_phase_kpis\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 3: Create KPI Summary Tables (UPDATED)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n📊 CREATING KPI SUMMARY TABLES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Overall KPIs\n",
    "overall_kpis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    COUNT(DISTINCT CDR_DAY)                AS days_analyzed,\n",
    "    SUM(total_calls)                       AS total_calls,\n",
    "    COUNT(DISTINCT unique_users)           AS total_unique_users,\n",
    "    ROUND(AVG(success_rate), 2)            AS avg_success_rate,\n",
    "    ROUND(SUM(total_revenue), 2)           AS total_revenue,\n",
    "    MAX(total_calls)                       AS peak_hourly_calls,\n",
    "    ROUND(AVG(network_stress_score), 2)    AS avg_network_stress\n",
    "FROM viz_master_timeseries\n",
    "\"\"\")\n",
    "overall_kpis.write.mode(\"overwrite\").saveAsTable(\"viz_overall_kpis\")\n",
    "print(\"✅ Created overall KPIs table: viz_overall_kpis\")\n",
    "\n",
    "# Daily comparison KPIs\n",
    "daily_kpis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    CDR_DAY,\n",
    "    SUM(total_calls)                       AS daily_calls,\n",
    "    MAX(unique_users)                      AS daily_unique_users,\n",
    "    ROUND(AVG(success_rate), 2)            AS daily_success_rate,\n",
    "    ROUND(SUM(total_revenue), 2)           AS daily_revenue,\n",
    "    MAX(total_calls)                       AS peak_hour_calls,\n",
    "    SUM(CASE WHEN network_stress_level IN ('High','Critical') THEN 1 ELSE 0 END) AS stressed_hours\n",
    "FROM viz_master_timeseries\n",
    "GROUP BY CDR_DAY\n",
    "ORDER BY CDR_DAY\n",
    "\"\"\")\n",
    "daily_kpis.write.mode(\"overwrite\").saveAsTable(\"viz_daily_kpis\")\n",
    "print(\"✅ Created daily KPIs table: viz_daily_kpis\")\n",
    "\n",
    "# Celebration phase KPIs\n",
    "phase_kpis = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    celebration_phase,\n",
    "    COUNT(*)                              AS phase_hours,\n",
    "    SUM(total_calls)                      AS phase_calls,\n",
    "    ROUND(AVG(success_rate), 2)           AS phase_success_rate,\n",
    "    ROUND(SUM(total_revenue), 2)          AS phase_revenue,\n",
    "    MAX(hour_over_hour_growth)            AS max_growth_rate,\n",
    "    ROUND(AVG(network_stress_score), 2)   AS avg_stress\n",
    "FROM viz_master_timeseries\n",
    "GROUP BY celebration_phase\n",
    "ORDER BY\n",
    "    CASE celebration_phase\n",
    "      WHEN 'Celebration' THEN 1\n",
    "      ELSE 2\n",
    "    END\n",
    "\"\"\")\n",
    "phase_kpis.write.mode(\"overwrite\").saveAsTable(\"viz_phase_kpis\")\n",
    "print(\"✅ Created celebration phase KPIs table: viz_phase_kpis\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8b2f8b1-adce-462c-b20b-2f4e70d750ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+---------+-------+\n",
      "|col_name              |data_type|comment|\n",
      "+----------------------+---------+-------+\n",
      "|CDR_DAY               |date     |NULL   |\n",
      "|call_hour             |int      |NULL   |\n",
      "|hour_key              |string   |NULL   |\n",
      "|total_calls           |bigint   |NULL   |\n",
      "|unique_users          |bigint   |NULL   |\n",
      "|active_cells          |bigint   |NULL   |\n",
      "|unique_sessions       |bigint   |NULL   |\n",
      "|successful_calls      |bigint   |NULL   |\n",
      "|failed_calls          |bigint   |NULL   |\n",
      "|total_duration_seconds|double   |NULL   |\n",
      "|avg_duration          |double   |NULL   |\n",
      "|stddev_duration       |double   |NULL   |\n",
      "|min_duration          |double   |NULL   |\n",
      "|max_duration          |double   |NULL   |\n",
      "|median_duration       |double   |NULL   |\n",
      "|p95_duration          |double   |NULL   |\n",
      "|short_calls_30s       |bigint   |NULL   |\n",
      "|medium_calls_2min     |bigint   |NULL   |\n",
      "|normal_calls_5min     |bigint   |NULL   |\n",
      "|long_calls_over5min   |bigint   |NULL   |\n",
      "|total_revenue         |double   |NULL   |\n",
      "|avg_revenue_per_call  |double   |NULL   |\n",
      "|paid_calls            |bigint   |NULL   |\n",
      "|free_calls            |bigint   |NULL   |\n",
      "|max_single_charge     |double   |NULL   |\n",
      "|voice_calls           |bigint   |NULL   |\n",
      "|sms_count             |bigint   |NULL   |\n",
      "|data_sessions         |bigint   |NULL   |\n",
      "|local_calls           |bigint   |NULL   |\n",
      "|national_calls        |bigint   |NULL   |\n",
      "|international_calls   |bigint   |NULL   |\n",
      "|roaming_calls         |bigint   |NULL   |\n",
      "|forwarded_calls       |bigint   |NULL   |\n",
      "|first_call_time       |timestamp|NULL   |\n",
      "|last_call_time        |timestamp|NULL   |\n",
      "|success_rate          |double   |NULL   |\n",
      "|failure_rate          |double   |NULL   |\n",
      "|avg_calls_per_user    |double   |NULL   |\n",
      "|hourly_arpu           |double   |NULL   |\n",
      "|paid_call_ratio       |double   |NULL   |\n",
      "+----------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DESCRIBE cdr_hourly_aggregated\").show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "898fcb04-c6d7-49be-be62-6973df44340d",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `cdr_raw` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [cdr_raw], [], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1) on agrège d’abord par cellule + heure\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cell_hourly \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m----> 3\u001b[0m   \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcdr_raw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;241m.\u001b[39mgroupBy(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCallingCellID\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhour_key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;241m.\u001b[39magg(\n\u001b[1;32m      6\u001b[0m       F\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_calls\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      7\u001b[0m       F\u001b[38;5;241m.\u001b[39mcountDistinct(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUserID\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124munique_users\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      8\u001b[0m       F\u001b[38;5;241m.\u001b[39mavg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailure_flag\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfailure_rate\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m      9\u001b[0m       F\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevenue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39malias(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtotal_revenue\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m     )\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     12\u001b[0m cell_hourly\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcdr_hourly_by_cell\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# 2) ensuite on rejoint votre master_timeseries pour récupérer celebration_phase, etc.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1667\u001b[0m, in \u001b[0;36mSparkSession.table\u001b[0;34m(self, tableName)\u001b[0m\n\u001b[1;32m   1636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtable\u001b[39m(\u001b[38;5;28mself\u001b[39m, tableName: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   1637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the specified table as a :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1638\u001b[0m \n\u001b[1;32m   1639\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 2.0.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1665\u001b[0m \u001b[38;5;124;03m    +---+\u001b[39;00m\n\u001b[1;32m   1666\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1667\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtableName\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `cdr_raw` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS.;\n'UnresolvedRelation [cdr_raw], [], false\n"
     ]
    }
   ],
   "source": [
    "# 1) on agrège d’abord par cellule + heure\n",
    "cell_hourly = (\n",
    "  spark.table(\"cdr_raw\")\n",
    "    .groupBy(\"CallingCellID\",\"hour_key\")\n",
    "    .agg(\n",
    "      F.count(\"*\").alias(\"total_calls\"),\n",
    "      F.countDistinct(\"UserID\").alias(\"unique_users\"),\n",
    "      F.avg(\"failure_flag\").alias(\"failure_rate\"),\n",
    "      F.sum(\"revenue\").alias(\"total_revenue\")\n",
    "    )\n",
    ")\n",
    "cell_hourly.createOrReplaceTempView(\"cdr_hourly_by_cell\")\n",
    "\n",
    "# 2) ensuite on rejoint votre master_timeseries pour récupérer celebration_phase, etc.\n",
    "cell_performance = spark.sql(\"\"\"\n",
    "SELECT\n",
    "  c.CallingCellID,\n",
    "  COUNT(*) AS total_hours,\n",
    "  SUM(c.total_calls) AS cell_total_calls,\n",
    "  ROUND(AVG(c.unique_users),2) AS avg_users_per_hour,\n",
    "  ROUND(AVG(c.failure_rate),2) AS avg_failure_rate,\n",
    "  ROUND(SUM(c.total_revenue),2) AS cell_revenue,\n",
    "  MAX(c.total_calls) AS peak_hour_calls,\n",
    "  SUM(CASE WHEN v.celebration_phase='Celebration' THEN c.total_calls ELSE 0 END) AS nye_calls,\n",
    "  MAX(CASE WHEN v.celebration_phase='Celebration' THEN c.total_calls ELSE 0 END) AS nye_peak\n",
    "FROM cdr_hourly_by_cell c\n",
    "JOIN viz_master_timeseries v\n",
    "  ON c.hour_key = v.hour_key\n",
    "GROUP BY c.CallingCellID\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b82b7752-9de2-4aaf-9b14-47fba7997505",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+-----------+\n",
      "|namespace          |tableName             |isTemporary|\n",
      "+-------------------+----------------------+-----------+\n",
      "|algerie_telecom_cdr|cdr_anonymized        |false      |\n",
      "|algerie_telecom_cdr|cdr_daily_summary     |false      |\n",
      "|algerie_telecom_cdr|cdr_network_metrics   |false      |\n",
      "|algerie_telecom_cdr|v_daily_trends        |false      |\n",
      "|algerie_telecom_cdr|v_network_performance |false      |\n",
      "|algerie_telecom_cdr|data_quality_checks   |false      |\n",
      "|algerie_telecom_cdr|cdr_hourly_aggregated |false      |\n",
      "|algerie_telecom_cdr|cdr_minute_aggregated |false      |\n",
      "|algerie_telecom_cdr|cdr_hourly_features   |false      |\n",
      "|algerie_telecom_cdr|cdr_user_time_patterns|false      |\n",
      "|algerie_telecom_cdr|v_hourly_performance  |false      |\n",
      "|algerie_telecom_cdr|v_midnight_transition |false      |\n",
      "|algerie_telecom_cdr|v_network_stress_hours|false      |\n",
      "|algerie_telecom_cdr|v_hourly_service_mix  |false      |\n",
      "|algerie_telecom_cdr|cdr_hourly_trends     |false      |\n",
      "|algerie_telecom_cdr|cdr_hourly_anomalies  |false      |\n",
      "|algerie_telecom_cdr|viz_time_series       |false      |\n",
      "|algerie_telecom_cdr|viz_anomalies         |false      |\n",
      "|algerie_telecom_cdr|viz_patterns          |false      |\n",
      "|algerie_telecom_cdr|viz_master_timeseries |false      |\n",
      "|algerie_telecom_cdr|viz_overall_kpis      |false      |\n",
      "|algerie_telecom_cdr|viz_daily_kpis        |false      |\n",
      "|algerie_telecom_cdr|viz_phase_kpis        |false      |\n",
      "+-------------------+----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES\").show(50, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5fba7ba-ac33-4cbf-8ed8-2c468bb896d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------------------+-----------+\n",
      "|namespace          |tableName             |isTemporary|\n",
      "+-------------------+----------------------+-----------+\n",
      "|algerie_telecom_cdr|cdr_anonymized        |false      |\n",
      "|algerie_telecom_cdr|cdr_daily_summary     |false      |\n",
      "|algerie_telecom_cdr|cdr_network_metrics   |false      |\n",
      "|algerie_telecom_cdr|v_daily_trends        |false      |\n",
      "|algerie_telecom_cdr|v_network_performance |false      |\n",
      "|algerie_telecom_cdr|data_quality_checks   |false      |\n",
      "|algerie_telecom_cdr|cdr_hourly_aggregated |false      |\n",
      "|algerie_telecom_cdr|cdr_minute_aggregated |false      |\n",
      "|algerie_telecom_cdr|cdr_hourly_features   |false      |\n",
      "|algerie_telecom_cdr|cdr_user_time_patterns|false      |\n",
      "|algerie_telecom_cdr|v_hourly_performance  |false      |\n",
      "|algerie_telecom_cdr|v_midnight_transition |false      |\n",
      "|algerie_telecom_cdr|v_network_stress_hours|false      |\n",
      "|algerie_telecom_cdr|v_hourly_service_mix  |false      |\n",
      "|algerie_telecom_cdr|cdr_hourly_trends     |false      |\n",
      "|algerie_telecom_cdr|cdr_hourly_anomalies  |false      |\n",
      "|algerie_telecom_cdr|viz_time_series       |false      |\n",
      "|algerie_telecom_cdr|viz_anomalies         |false      |\n",
      "|algerie_telecom_cdr|viz_patterns          |false      |\n",
      "|algerie_telecom_cdr|viz_master_timeseries |false      |\n",
      "|algerie_telecom_cdr|viz_overall_kpis      |false      |\n",
      "|algerie_telecom_cdr|viz_daily_kpis        |false      |\n",
      "|algerie_telecom_cdr|viz_phase_kpis        |false      |\n",
      "+-------------------+----------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SHOW TABLES IN algerie_telecom_cdr\").show(40, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "363fef60-b4c7-4e68-aaf3-112bdc9a6bfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺️ CREATING GEOGRAPHIC CELL PERFORMANCE DATA\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`CallingCellID` cannot be resolved. Did you mean one of the following? [`c`.`active_cells`, `v`.`active_cells`, `c`.`failed_calls`, `c`.`paid_calls`, `c`.`roaming_calls`].; line 15 pos 6;\n'Aggregate ['c.CallingCellID], ['c.CallingCellID, count(1) AS total_hours#1777L, 'SUM('c.total_calls) AS cell_total_calls#1778, 'ROUND('AVG('c.unique_users), 2) AS avg_users_per_hour#1779, 'ROUND('AVG('c.failure_rate), 2) AS avg_failure_rate#1780, 'ROUND('SUM('c.total_revenue), 2) AS cell_revenue#1781, 'MAX('c.total_calls) AS peak_hour_calls#1782, 'SUM(CASE WHEN ('v.celebration_phase = Celebration) THEN 'c.total_calls ELSE 0 END) AS nye_calls#1783, 'MAX(CASE WHEN ('v.celebration_phase = Celebration) THEN 'c.total_calls ELSE 0 END) AS nye_peak#1784]\n+- 'Filter isnotnull('c.CallingCellID)\n   +- Join Inner, (hour_key#760 = hour_key#439)\n      :- SubqueryAlias c\n      :  +- SubqueryAlias spark_catalog.algerie_telecom_cdr.cdr_hourly_aggregated\n      :     +- Relation spark_catalog.algerie_telecom_cdr.cdr_hourly_aggregated[CDR_DAY#758,call_hour#759,hour_key#760,total_calls#761L,unique_users#762L,active_cells#763L,unique_sessions#764L,successful_calls#765L,failed_calls#766L,total_duration_seconds#767,avg_duration#768,stddev_duration#769,min_duration#770,max_duration#771,median_duration#772,p95_duration#773,short_calls_30s#774L,medium_calls_2min#775L,normal_calls_5min#776L,long_calls_over5min#777L,total_revenue#778,avg_revenue_per_call#779,paid_calls#780L,free_calls#781L,... 16 more fields] parquet\n      +- SubqueryAlias v\n         +- SubqueryAlias spark_catalog.algerie_telecom_cdr.viz_master_timeseries\n            +- Relation spark_catalog.algerie_telecom_cdr.viz_master_timeseries[hour_key#439,CDR_DAY#440,call_hour#441,timestamp#442,hour_of_week#443,celebration_phase#444,total_calls#445L,unique_users#446L,successful_calls#447L,failed_calls#448L,active_cells#449L,success_rate#450,failure_rate#451,avg_duration#452,median_duration#453,total_revenue#454,hourly_arpu#455,paid_calls#456L,free_calls#457L,paid_call_ratio#458,voice_calls#459L,sms_count#460L,data_sessions#461L,hour_over_hour_growth#462,... 7 more fields] parquet\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# 1️⃣ Cell performance metrics\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m cell_performance \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124;43mSELECT \u001b[39;49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;124;43m    c.CallingCellID,\u001b[39;49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;124;43m    COUNT(*)                                        AS total_hours,\u001b[39;49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;43m    SUM(c.total_calls)                              AS cell_total_calls,\u001b[39;49m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;43m    ROUND(AVG(c.unique_users), 2)                   AS avg_users_per_hour,\u001b[39;49m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;43m    ROUND(AVG(c.failure_rate), 2)                   AS avg_failure_rate,\u001b[39;49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;43m    ROUND(SUM(c.total_revenue), 2)                  AS cell_revenue,\u001b[39;49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;43m    MAX(c.total_calls)                              AS peak_hour_calls,\u001b[39;49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;43m    -- New Year\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43ms specific metrics via viz_master_timeseries\u001b[39;49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;43m    SUM(CASE WHEN v.celebration_phase = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCelebration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m THEN c.total_calls ELSE 0 END) AS nye_calls,\u001b[39;49m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;43m    MAX(CASE WHEN v.celebration_phase = \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mCelebration\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m THEN c.total_calls ELSE 0 END) AS nye_peak\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;43mFROM cdr_hourly_aggregated c\u001b[39;49m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;124;43mJOIN viz_master_timeseries          v ON c.hour_key = v.hour_key\u001b[39;49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;43mWHERE c.CallingCellID IS NOT NULL\u001b[39;49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;43mGROUP BY c.CallingCellID\u001b[39;49m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m cell_performance\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mmode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39msaveAsTable(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviz_cell_performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Created cell performance table: viz_cell_performance\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `c`.`CallingCellID` cannot be resolved. Did you mean one of the following? [`c`.`active_cells`, `v`.`active_cells`, `c`.`failed_calls`, `c`.`paid_calls`, `c`.`roaming_calls`].; line 15 pos 6;\n'Aggregate ['c.CallingCellID], ['c.CallingCellID, count(1) AS total_hours#1777L, 'SUM('c.total_calls) AS cell_total_calls#1778, 'ROUND('AVG('c.unique_users), 2) AS avg_users_per_hour#1779, 'ROUND('AVG('c.failure_rate), 2) AS avg_failure_rate#1780, 'ROUND('SUM('c.total_revenue), 2) AS cell_revenue#1781, 'MAX('c.total_calls) AS peak_hour_calls#1782, 'SUM(CASE WHEN ('v.celebration_phase = Celebration) THEN 'c.total_calls ELSE 0 END) AS nye_calls#1783, 'MAX(CASE WHEN ('v.celebration_phase = Celebration) THEN 'c.total_calls ELSE 0 END) AS nye_peak#1784]\n+- 'Filter isnotnull('c.CallingCellID)\n   +- Join Inner, (hour_key#760 = hour_key#439)\n      :- SubqueryAlias c\n      :  +- SubqueryAlias spark_catalog.algerie_telecom_cdr.cdr_hourly_aggregated\n      :     +- Relation spark_catalog.algerie_telecom_cdr.cdr_hourly_aggregated[CDR_DAY#758,call_hour#759,hour_key#760,total_calls#761L,unique_users#762L,active_cells#763L,unique_sessions#764L,successful_calls#765L,failed_calls#766L,total_duration_seconds#767,avg_duration#768,stddev_duration#769,min_duration#770,max_duration#771,median_duration#772,p95_duration#773,short_calls_30s#774L,medium_calls_2min#775L,normal_calls_5min#776L,long_calls_over5min#777L,total_revenue#778,avg_revenue_per_call#779,paid_calls#780L,free_calls#781L,... 16 more fields] parquet\n      +- SubqueryAlias v\n         +- SubqueryAlias spark_catalog.algerie_telecom_cdr.viz_master_timeseries\n            +- Relation spark_catalog.algerie_telecom_cdr.viz_master_timeseries[hour_key#439,CDR_DAY#440,call_hour#441,timestamp#442,hour_of_week#443,celebration_phase#444,total_calls#445L,unique_users#446L,successful_calls#447L,failed_calls#448L,active_cells#449L,success_rate#450,failure_rate#451,avg_duration#452,median_duration#453,total_revenue#454,hourly_arpu#455,paid_calls#456L,free_calls#457L,paid_call_ratio#458,voice_calls#459L,sms_count#460L,data_sessions#461L,hour_over_hour_growth#462,... 7 more fields] parquet\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 4: Create Cell Performance Geographic Data (UPDATED)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n🗺️ CREATING GEOGRAPHIC CELL PERFORMANCE DATA\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1️⃣ Cell performance metrics\n",
    "cell_performance = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.CallingCellID,\n",
    "    COUNT(*)                                        AS total_hours,\n",
    "    SUM(c.total_calls)                              AS cell_total_calls,\n",
    "    ROUND(AVG(c.unique_users), 2)                   AS avg_users_per_hour,\n",
    "    ROUND(AVG(c.failure_rate), 2)                   AS avg_failure_rate,\n",
    "    ROUND(SUM(c.total_revenue), 2)                  AS cell_revenue,\n",
    "    MAX(c.total_calls)                              AS peak_hour_calls,\n",
    "    -- New Year's specific metrics via viz_master_timeseries\n",
    "    SUM(CASE WHEN v.celebration_phase = 'Celebration' THEN c.total_calls ELSE 0 END) AS nye_calls,\n",
    "    MAX(CASE WHEN v.celebration_phase = 'Celebration' THEN c.total_calls ELSE 0 END) AS nye_peak\n",
    "FROM cdr_hourly_aggregated c\n",
    "JOIN viz_master_timeseries          v ON c.hour_key = v.hour_key\n",
    "WHERE c.CallingCellID IS NOT NULL\n",
    "GROUP BY c.CallingCellID\n",
    "\"\"\")\n",
    "cell_performance.write.mode(\"overwrite\").saveAsTable(\"viz_cell_performance\")\n",
    "print(\"✅ Created cell performance table: viz_cell_performance\")\n",
    "\n",
    "# 2️⃣ Cell stress analysis\n",
    "cell_stress = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.CallingCellID,\n",
    "    c.call_hour,\n",
    "    c.total_calls,\n",
    "    c.failure_rate,\n",
    "    CASE \n",
    "        WHEN c.failure_rate > 30 THEN 'Critical'\n",
    "        WHEN c.failure_rate > 20 THEN 'High'\n",
    "        WHEN c.failure_rate > 10 THEN 'Medium'\n",
    "        ELSE 'Low'\n",
    "    END                                             AS stress_level,\n",
    "    v.celebration_phase\n",
    "FROM cdr_hourly_aggregated c\n",
    "JOIN viz_master_timeseries          v ON c.hour_key = v.hour_key\n",
    "WHERE c.CallingCellID IS NOT NULL\n",
    "  AND v.celebration_phase = 'Celebration'\n",
    "ORDER BY c.failure_rate DESC\n",
    "\"\"\")\n",
    "cell_stress.write.mode(\"overwrite\").saveAsTable(\"viz_cell_stress\")\n",
    "print(\"✅ Created cell stress analysis table: viz_cell_stress\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6e950ace-155e-483c-bdd7-436dad57ddc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created temp view: cdr_hourly_by_cell\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 3.5: Build hourly-by-cell view from raw CDR data\n",
    "# ------------------------------------------------------------\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "raw = spark.table(\"cdr_anonymized\")\n",
    "\n",
    "cell_hourly = (\n",
    "    raw\n",
    "    # 1️⃣ Extraire un timestamp et en dériver jour/heure\n",
    "    .withColumn(\"ts\", F.to_timestamp(\"CUST_LOCAL_START_DATE\"))\n",
    "    .withColumn(\"CDR_DAY\", F.to_date(\"ts\"))\n",
    "    .withColumn(\"call_hour\", F.hour(\"ts\"))\n",
    "    # 2️⃣ Construire hour_key comme dans ton master_timeseries\n",
    "    .withColumn(\n",
    "        \"hour_key\",\n",
    "        F.concat_ws(\n",
    "            \"_\",\n",
    "            F.date_format(\"ts\", \"yyyy-MM-dd\"),\n",
    "            F.lpad(F.col(\"call_hour\").cast(\"string\"), 2, \"0\")\n",
    "        )\n",
    "    )\n",
    "    # 3️⃣ Agréger par cellule & heure\n",
    "    #    → remplacer OBJ_ID par la colonne qui contient ton CellID\n",
    "    .groupBy(\"OBJ_ID\", \"hour_key\", \"CDR_DAY\", \"call_hour\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"total_calls\"),\n",
    "        F.countDistinct(\"SESSION_ID\").alias(\"unique_users\"),      # ex. SESSION_ID comme proxy user\n",
    "        (F.sum(F.expr(\"CASE WHEN CallForwardIndicator>0 THEN 1 ELSE 0 END\")) \n",
    "             / F.count(\"*\")\n",
    "        ).alias(\"failure_rate\"),                                  # derive failure_flag\n",
    "        F.sum(\"DEBIT_AMOUNT\").alias(\"total_revenue\")               # DEBIT_AMOUNT pour revenue\n",
    "    )\n",
    "    .withColumnRenamed(\"OBJ_ID\", \"CallingCellID\")\n",
    ")\n",
    "\n",
    "cell_hourly.createOrReplaceTempView(\"cdr_hourly_by_cell\")\n",
    "print(\"✅ Created temp view: cdr_hourly_by_cell\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "25761fec-3e28-4ee2-b9c1-be3811ab55d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🗺️ CREATING GEOGRAPHIC CELL PERFORMANCE DATA\n",
      "------------------------------------------------------------\n",
      "✅ Created cell performance table: viz_cell_performance\n",
      "✅ Created cell stress analysis table: viz_cell_stress\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n🗺️ CREATING GEOGRAPHIC CELL PERFORMANCE DATA\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1️⃣ Cell performance metrics\n",
    "cell_performance = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.CallingCellID,\n",
    "    COUNT(*)                            AS total_hours,\n",
    "    SUM(c.total_calls)                  AS cell_total_calls,\n",
    "    ROUND(AVG(c.unique_users), 2)       AS avg_users_per_hour,\n",
    "    ROUND(AVG(c.failure_rate), 2)       AS avg_failure_rate,\n",
    "    ROUND(SUM(c.total_revenue), 2)      AS cell_revenue,\n",
    "    MAX(c.total_calls)                  AS peak_hour_calls,\n",
    "    SUM(\n",
    "      CASE WHEN v.celebration_phase = 'Celebration' \n",
    "           THEN c.total_calls \n",
    "           ELSE 0 \n",
    "      END\n",
    "    ) AS nye_calls,\n",
    "    MAX(\n",
    "      CASE WHEN v.celebration_phase = 'Celebration' \n",
    "           THEN c.total_calls \n",
    "           ELSE 0 \n",
    "      END\n",
    "    ) AS nye_peak\n",
    "FROM cdr_hourly_by_cell c\n",
    "JOIN viz_master_timeseries     v \n",
    "  ON c.hour_key = v.hour_key\n",
    "GROUP BY c.CallingCellID\n",
    "\"\"\")\n",
    "cell_performance.write.mode(\"overwrite\").saveAsTable(\"viz_cell_performance\")\n",
    "print(\"✅ Created cell performance table: viz_cell_performance\")\n",
    "\n",
    "# 2️⃣ Cell stress analysis\n",
    "cell_stress = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    c.CallingCellID,\n",
    "    c.call_hour,\n",
    "    c.total_calls,\n",
    "    c.failure_rate,\n",
    "    CASE \n",
    "      WHEN c.failure_rate > 30 THEN 'Critical'\n",
    "      WHEN c.failure_rate > 20 THEN 'High'\n",
    "      WHEN c.failure_rate > 10 THEN 'Medium'\n",
    "      ELSE 'Low'\n",
    "    END                           AS stress_level,\n",
    "    v.celebration_phase\n",
    "FROM cdr_hourly_by_cell c\n",
    "JOIN viz_master_timeseries     v \n",
    "  ON c.hour_key = v.hour_key\n",
    "WHERE v.celebration_phase = 'Celebration'\n",
    "ORDER BY c.failure_rate DESC\n",
    "\"\"\")\n",
    "cell_stress.write.mode(\"overwrite\").saveAsTable(\"viz_cell_stress\")\n",
    "print(\"✅ Created cell stress analysis table: viz_cell_stress\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73539ae2-acf9-43dd-a5af-853deafb52f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "👥 CREATING USER BEHAVIOR VISUALIZATION DATA\n",
      "------------------------------------------------------------\n",
      "✅ Created user segments table: viz_user_segments\n",
      "✅ Created user activity patterns table: viz_user_activity\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 5: Create User Behavior Visualization Data (FIXED)\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n👥 CREATING USER BEHAVIOR VISUALIZATION DATA\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1️⃣ Part de cdr_user_time_patterns pour calculer la répartition des appels par créneau\n",
    "user_segments = spark.sql(\"\"\"\n",
    "SELECT usage_segment, segment_calls\n",
    "FROM (\n",
    "  SELECT 'Early Morning'  AS usage_segment, SUM(`Early Morning`)  AS segment_calls FROM cdr_user_time_patterns\n",
    "  UNION ALL\n",
    "  SELECT 'Morning'        AS usage_segment, SUM(Morning)        AS segment_calls FROM cdr_user_time_patterns\n",
    "  UNION ALL\n",
    "  SELECT 'Afternoon'      AS usage_segment, SUM(Afternoon)      AS segment_calls FROM cdr_user_time_patterns\n",
    "  UNION ALL\n",
    "  SELECT 'Evening'        AS usage_segment, SUM(Evening)        AS segment_calls FROM cdr_user_time_patterns\n",
    "  UNION ALL\n",
    "  SELECT 'Late Night'     AS usage_segment, SUM(`Late Night`)   AS segment_calls FROM cdr_user_time_patterns\n",
    ") t\n",
    "\"\"\")\n",
    "user_segments.write.mode(\"overwrite\").saveAsTable(\"viz_user_segments\")\n",
    "print(\"✅ Created user segments table: viz_user_segments\")\n",
    "\n",
    "# 2️⃣ Hourly user activity patterns (depuis viz_master_timeseries)\n",
    "user_activity_patterns = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    call_hour,\n",
    "    SUM(unique_users)                                AS active_users,\n",
    "    SUM(total_calls)                                 AS total_calls,\n",
    "    ROUND(SUM(total_calls) / NULLIF(SUM(unique_users),0), 2) AS calls_per_user,\n",
    "    SUM(CASE WHEN total_calls <= 30 THEN 1 ELSE 0 END)       AS short_call_hours,\n",
    "    SUM(CASE WHEN total_calls > 300 THEN 1 ELSE 0 END)       AS long_call_hours\n",
    "FROM viz_master_timeseries\n",
    "GROUP BY call_hour\n",
    "ORDER BY call_hour\n",
    "\"\"\")\n",
    "user_activity_patterns.write.mode(\"overwrite\").saveAsTable(\"viz_user_activity\")\n",
    "print(\"✅ Created user activity patterns table: viz_user_activity\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cb8cd927-81be-47b8-b764-2b8df1cbde03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎨 CREATING SUPERSET DASHBOARD CONFIGURATIONS\n",
      "------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Created dashboard configurations\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 6: Create Superset Dashboard Configurations\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n🎨 CREATING SUPERSET DASHBOARD CONFIGURATIONS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Dashboard 1: Executive Overview\n",
    "executive_dashboard = {\n",
    "    \"title\": \"CDR New Year's Eve Executive Dashboard\",\n",
    "    \"charts\": [\n",
    "        {\n",
    "            \"type\": \"big_number\",\n",
    "            \"title\": \"Total Calls\",\n",
    "            \"query\": \"SELECT SUM(total_calls) FROM viz_overall_kpis\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"big_number_with_trendline\",\n",
    "            \"title\": \"Peak Hour Calls\",\n",
    "            \"query\": \"SELECT MAX(total_calls) as value, hour_key FROM viz_master_timeseries GROUP BY hour_key ORDER BY value DESC LIMIT 1\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"line_chart\",\n",
    "            \"title\": \"Hourly Call Volume\",\n",
    "            \"query\": \"SELECT timestamp, total_calls, celebration_phase FROM viz_master_timeseries ORDER BY timestamp\",\n",
    "            \"x_axis\": \"timestamp\",\n",
    "            \"y_axis\": \"total_calls\",\n",
    "            \"color_by\": \"celebration_phase\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"bar_chart\",\n",
    "            \"title\": \"Revenue by Celebration Phase\",\n",
    "            \"query\": \"SELECT celebration_phase, phase_revenue FROM viz_phase_kpis\",\n",
    "            \"x_axis\": \"celebration_phase\",\n",
    "            \"y_axis\": \"phase_revenue\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Dashboard 2: Network Operations\n",
    "network_dashboard = {\n",
    "    \"title\": \"Network Operations Dashboard\",\n",
    "    \"charts\": [\n",
    "        {\n",
    "            \"type\": \"heatmap\",\n",
    "            \"title\": \"Network Stress Heatmap\",\n",
    "            \"query\": \"SELECT call_hour, CDR_DAY, network_stress_score FROM viz_master_timeseries\",\n",
    "            \"x_axis\": \"call_hour\",\n",
    "            \"y_axis\": \"CDR_DAY\",\n",
    "            \"metric\": \"network_stress_score\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"scatter_plot\",\n",
    "            \"title\": \"Calls vs Failure Rate\",\n",
    "            \"query\": \"SELECT total_calls, failure_rate, anomaly_type, hour_key, unique_users FROM viz_master_timeseries\",\n",
    "            \"x_axis\": \"total_calls\",\n",
    "            \"y_axis\": \"failure_rate\",\n",
    "            \"color_by\": \"anomaly_type\",\n",
    "            \"bubble_size\": \"unique_users\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"gauge_chart\",\n",
    "            \"title\": \"Average Network Stress\",\n",
    "            \"query\": \"SELECT AVG(network_stress_score) as stress FROM viz_master_timeseries\",\n",
    "            \"max_value\": 100,\n",
    "            \"thresholds\": [40, 60, 80]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Dashboard 3: User Analytics\n",
    "user_dashboard = {\n",
    "    \"title\": \"User Behavior Analytics\",\n",
    "    \"charts\": [\n",
    "        {\n",
    "            \"type\": \"pie_chart\",\n",
    "            \"title\": \"User Segments\",\n",
    "            \"query\": \"SELECT usage_segment, segment_calls AS user_count FROM viz_user_segments\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"area_chart\",\n",
    "            \"title\": \"User Activity by Hour\",\n",
    "            \"query\": \"SELECT call_hour, active_users, calls_per_user FROM viz_user_activity ORDER BY call_hour\",\n",
    "            \"x_axis\": \"call_hour\",\n",
    "            \"metrics\": [\"active_users\", \"calls_per_user\"]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Save configurations as JSON\n",
    "configs = {\n",
    "    \"executive\": executive_dashboard,\n",
    "    \"network\": network_dashboard,\n",
    "    \"user\": user_dashboard\n",
    "}\n",
    "\n",
    "# Create a configurations table\n",
    "config_df = spark.createDataFrame([\n",
    "    (\"executive_dashboard\", json.dumps(executive_dashboard)),\n",
    "    (\"network_dashboard\", json.dumps(network_dashboard)),\n",
    "    (\"user_dashboard\", json.dumps(user_dashboard))\n",
    "], [\"dashboard_name\", \"config_json\"])\n",
    "\n",
    "config_df.write.mode(\"overwrite\").saveAsTable(\"viz_dashboard_configs\")\n",
    "print(\"✅ Created dashboard configurations\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0b05d729-6c3c-4dc0-9c8d-2dae3a1b2db7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📊 CREATING POWERBI-OPTIMIZED TABLES\n",
      "------------------------------------------------------------\n",
      "✅ Created PowerBI time series table\n",
      "✅ Created PowerBI fact table\n",
      "✅ Created PowerBI time dimension\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 7: Create PowerBI-Optimized Tables\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n📊 CREATING POWERBI-OPTIMIZED TABLES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# 1️⃣ Flatten the master dataset for PowerBI\n",
    "powerbi_flat = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    timestamp,\n",
    "    CDR_DAY         AS Date,\n",
    "    call_hour       AS Hour,\n",
    "    CASE \n",
    "      WHEN call_hour BETWEEN 0 AND 5  THEN '00-05 Night'\n",
    "      WHEN call_hour BETWEEN 6 AND 11 THEN '06-11 Morning'\n",
    "      WHEN call_hour BETWEEN 12 AND 17 THEN '12-17 Afternoon'\n",
    "      ELSE '18-23 Evening'\n",
    "    END AS TimePeriod,\n",
    "    celebration_phase     AS CelebrationPhase,\n",
    "    total_calls           AS TotalCalls,\n",
    "    successful_calls      AS SuccessfulCalls,\n",
    "    failed_calls          AS FailedCalls,\n",
    "    unique_users          AS UniqueUsers,\n",
    "    total_revenue         AS Revenue,\n",
    "    success_rate          AS SuccessRate,\n",
    "    failure_rate          AS FailureRate,\n",
    "    hourly_arpu           AS ARPU,\n",
    "    hour_over_hour_growth AS GrowthRate,\n",
    "    network_stress_level  AS NetworkStress,\n",
    "    anomaly_type          AS AnomalyType\n",
    "    -- on SUPPRIME trend_direction car absent de viz_master_timeseries\n",
    "FROM viz_master_timeseries\n",
    "\"\"\")\n",
    "powerbi_flat.write.mode(\"overwrite\").saveAsTable(\"powerbi_timeseries\")\n",
    "print(\"✅ Created PowerBI time series table\")\n",
    "\n",
    "# 2️⃣ Fact table\n",
    "fact_calls = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    hour_key,\n",
    "    total_calls,\n",
    "    successful_calls,\n",
    "    failed_calls,\n",
    "    unique_users,\n",
    "    total_revenue,\n",
    "    voice_calls,\n",
    "    sms_count,\n",
    "    data_sessions\n",
    "FROM cdr_hourly_aggregated\n",
    "\"\"\")\n",
    "fact_calls.write.mode(\"overwrite\").saveAsTable(\"powerbi_fact_calls\")\n",
    "print(\"✅ Created PowerBI fact table\")\n",
    "\n",
    "# 3️⃣ Time dimension\n",
    "powerbi_dim_time = spark.sql(\"\"\"\n",
    "SELECT DISTINCT\n",
    "    h.hour_key,\n",
    "    h.CDR_DAY,\n",
    "    h.call_hour,\n",
    "    CASE \n",
    "      WHEN h.call_hour BETWEEN 0 AND 5  THEN 'Night'\n",
    "      WHEN h.call_hour BETWEEN 6 AND 11 THEN 'Morning'\n",
    "      WHEN h.call_hour BETWEEN 12 AND 17 THEN 'Afternoon'\n",
    "      ELSE 'Evening'\n",
    "    END AS period_of_day,\n",
    "    CASE WHEN h.is_celebration_hour = 1 THEN 'Celebration' ELSE 'Normal' END AS CelebrationPhase\n",
    "FROM cdr_hourly_features h\n",
    "\"\"\")\n",
    "powerbi_dim_time.write.mode(\"overwrite\").saveAsTable(\"powerbi_dim_time\")\n",
    "print(\"✅ Created PowerBI time dimension\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e0b9003f-2e1c-4b36-ba61-acb0246d3291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📝 VISUALIZATION SQL QUERIES\n",
      "------------------------------------------------------------\n",
      "✅ Created visualization query library\n",
      "\n",
      "--- MIDNIGHT_SPIKE ---\n",
      "\n",
      "-- Minute-by-minute analysis around midnight\n",
      "SELECT \n",
      "    timestamp,\n",
      "    calls_per_minute,\n",
      "    unique_callers,\n",
      "    failure_rate\n",
      "FROM cdr_minute_aggregated\n",
      "WHERE timestamp BETWEEN '2024-12-31 23:00:00' AND '2025-01-01 01:00:00'\n",
      "ORDER BY timestamp\n",
      "\n",
      "\n",
      "--- HOURLY_COMPARISON ---\n",
      "\n",
      "-- Compare same hours across both days\n",
      "SELECT \n",
      "    call_hour,\n",
      "    SUM(CASE WHEN CDR_DAY = '2024-12-31' THEN total_calls ELSE 0 END) as dec31_calls,\n",
      "    SUM(CASE WHEN CDR_DAY = '2025-01-01' THEN total_calls ELSE 0 END) as jan01_calls,\n",
      "    ROUND(\n",
      "        (SUM(CASE WHEN CDR_DAY = '2025-01-01' THEN total_calls ELSE 0 END) - \n",
      "         SUM(CASE WHEN CDR_DAY = '2024-12-31' THEN total_calls ELSE 0 END)) * 100.0 / \n",
      "         NULLIF(SUM(CASE WHEN CDR_DAY = '2024-12-31' THEN total_calls ELSE 0 END), 0), 2\n",
      "    ) as growth_percentage\n",
      "FROM viz_master_timeseries\n",
      "GROUP BY call_hour\n",
      "ORDER BY call_hour\n",
      "\n",
      "\n",
      "--- ANOMALY_TIMELINE ---\n",
      "\n",
      "-- Anomalies with context\n",
      "SELECT \n",
      "    timestamp,\n",
      "    total_calls,\n",
      "    anomaly_type,\n",
      "    anomaly_severity,\n",
      "    celebration_phase,\n",
      "    network_stress_level\n",
      "FROM viz_master_timeseries\n",
      "WHERE anomaly_severity > 0\n",
      "ORDER BY timestamp\n",
      "\n",
      "\n",
      "--- REVENUE_WATERFALL ---\n",
      "\n",
      "-- Revenue breakdown by phase\n",
      "WITH phase_revenue AS (\n",
      "    SELECT \n",
      "        celebration_phase,\n",
      "        SUM(total_revenue) as revenue,\n",
      "        ROW_NUMBER() OVER (ORDER BY \n",
      "            CASE celebration_phase\n",
      "                WHEN 'Pre-Celebration' THEN 1\n",
      "                WHEN 'Late NYE' THEN 2\n",
      "                WHEN 'Early NY' THEN 3\n",
      "                WHEN 'Post-Celebration' THEN 4\n",
      "                WHEN 'New Year Day' THEN 5\n",
      "            END\n",
      "        ) as phase_order\n",
      "    FROM viz_master_timeseries\n",
      "    WHERE celebration_phase IS NOT NULL\n",
      "    GROUP BY celebration_phase\n",
      ")\n",
      "SELECT \n",
      "    celebration_phase,\n",
      "    revenue,\n",
      "    SUM(revenue) OVER (ORDER BY phase_order) as cumulative_revenue\n",
      "FROM phase_revenue\n",
      "ORDER BY phase_order\n",
      "\n",
      "\n",
      "--- CELL_PERFORMANCE_MAP ---\n",
      "\n",
      "-- For geographic visualization\n",
      "SELECT \n",
      "    CallingCellID,\n",
      "    cell_total_calls,\n",
      "    avg_failure_rate,\n",
      "    cell_revenue,\n",
      "    nye_peak,\n",
      "    CASE \n",
      "        WHEN avg_failure_rate > 20 THEN 'Poor'\n",
      "        WHEN avg_failure_rate > 10 THEN 'Fair'\n",
      "        ELSE 'Good'\n",
      "    END as performance_category\n",
      "FROM viz_cell_performance\n",
      "WHERE CallingCellID IS NOT NULL\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 8: Create Visualization SQL Queries\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n📝 VISUALIZATION SQL QUERIES\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Store useful queries for dashboard creation\n",
    "visualization_queries = {\n",
    "    \"midnight_spike\": \"\"\"\n",
    "-- Minute-by-minute analysis around midnight\n",
    "SELECT \n",
    "    timestamp,\n",
    "    calls_per_minute,\n",
    "    unique_callers,\n",
    "    failure_rate\n",
    "FROM cdr_minute_aggregated\n",
    "WHERE timestamp BETWEEN '2024-12-31 23:00:00' AND '2025-01-01 01:00:00'\n",
    "ORDER BY timestamp\n",
    "\"\"\",\n",
    "    \n",
    "    \"hourly_comparison\": \"\"\"\n",
    "-- Compare same hours across both days\n",
    "SELECT \n",
    "    call_hour,\n",
    "    SUM(CASE WHEN CDR_DAY = '2024-12-31' THEN total_calls ELSE 0 END) as dec31_calls,\n",
    "    SUM(CASE WHEN CDR_DAY = '2025-01-01' THEN total_calls ELSE 0 END) as jan01_calls,\n",
    "    ROUND(\n",
    "        (SUM(CASE WHEN CDR_DAY = '2025-01-01' THEN total_calls ELSE 0 END) - \n",
    "         SUM(CASE WHEN CDR_DAY = '2024-12-31' THEN total_calls ELSE 0 END)) * 100.0 / \n",
    "         NULLIF(SUM(CASE WHEN CDR_DAY = '2024-12-31' THEN total_calls ELSE 0 END), 0), 2\n",
    "    ) as growth_percentage\n",
    "FROM viz_master_timeseries\n",
    "GROUP BY call_hour\n",
    "ORDER BY call_hour\n",
    "\"\"\",\n",
    "    \n",
    "    \"anomaly_timeline\": \"\"\"\n",
    "-- Anomalies with context\n",
    "SELECT \n",
    "    timestamp,\n",
    "    total_calls,\n",
    "    anomaly_type,\n",
    "    anomaly_severity,\n",
    "    celebration_phase,\n",
    "    network_stress_level\n",
    "FROM viz_master_timeseries\n",
    "WHERE anomaly_severity > 0\n",
    "ORDER BY timestamp\n",
    "\"\"\",\n",
    "    \n",
    "    \"revenue_waterfall\": \"\"\"\n",
    "-- Revenue breakdown by phase\n",
    "WITH phase_revenue AS (\n",
    "    SELECT \n",
    "        celebration_phase,\n",
    "        SUM(total_revenue) as revenue,\n",
    "        ROW_NUMBER() OVER (ORDER BY \n",
    "            CASE celebration_phase\n",
    "                WHEN 'Pre-Celebration' THEN 1\n",
    "                WHEN 'Late NYE' THEN 2\n",
    "                WHEN 'Early NY' THEN 3\n",
    "                WHEN 'Post-Celebration' THEN 4\n",
    "                WHEN 'New Year Day' THEN 5\n",
    "            END\n",
    "        ) as phase_order\n",
    "    FROM viz_master_timeseries\n",
    "    WHERE celebration_phase IS NOT NULL\n",
    "    GROUP BY celebration_phase\n",
    ")\n",
    "SELECT \n",
    "    celebration_phase,\n",
    "    revenue,\n",
    "    SUM(revenue) OVER (ORDER BY phase_order) as cumulative_revenue\n",
    "FROM phase_revenue\n",
    "ORDER BY phase_order\n",
    "\"\"\",\n",
    "    \n",
    "    \"cell_performance_map\": \"\"\"\n",
    "-- For geographic visualization\n",
    "SELECT \n",
    "    CallingCellID,\n",
    "    cell_total_calls,\n",
    "    avg_failure_rate,\n",
    "    cell_revenue,\n",
    "    nye_peak,\n",
    "    CASE \n",
    "        WHEN avg_failure_rate > 20 THEN 'Poor'\n",
    "        WHEN avg_failure_rate > 10 THEN 'Fair'\n",
    "        ELSE 'Good'\n",
    "    END as performance_category\n",
    "FROM viz_cell_performance\n",
    "WHERE CallingCellID IS NOT NULL\n",
    "\"\"\"\n",
    "}\n",
    "\n",
    "# Save queries for reference\n",
    "queries_df = spark.createDataFrame(\n",
    "    [(k, v) for k, v in visualization_queries.items()],\n",
    "    [\"query_name\", \"query_sql\"]\n",
    ")\n",
    "queries_df.write.mode(\"overwrite\").saveAsTable(\"viz_query_library\")\n",
    "print(\"✅ Created visualization query library\")\n",
    "\n",
    "# Print queries for easy copying\n",
    "for name, query in visualization_queries.items():\n",
    "    print(f\"\\n--- {name.upper()} ---\")\n",
    "    print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "94d2b910-ac04-4e65-9b12-d13b0a04df8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🚨 CREATING ALERT CONFIGURATIONS\n",
      "------------------------------------------------------------\n",
      "✅ Created alert configurations\n",
      "\n",
      "🚨 Active Alerts Summary:\n",
      "+--------------------+-----+\n",
      "|     alert_triggered|count|\n",
      "+--------------------+-----+\n",
      "|    High Call Volume|    7|\n",
      "|Major Anomaly Det...|    1|\n",
      "+--------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 9: Create Alert Configurations\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n🚨 CREATING ALERT CONFIGURATIONS\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "# Define alert thresholds based on observed patterns\n",
    "alert_configs = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    'High Call Volume' as alert_name,\n",
    "    'total_calls > 5000' as condition,\n",
    "    'warning' as severity,\n",
    "    'Hourly calls exceed 5000' as description\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Critical Failure Rate' as alert_name,\n",
    "    'failure_rate > 25' as condition,\n",
    "    'critical' as severity,\n",
    "    'Failure rate exceeds 25%' as description\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Network Stress Critical' as alert_name,\n",
    "    'network_stress_score > 70' as condition,\n",
    "    'critical' as severity,\n",
    "    'Network stress score above 70' as description\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Major Anomaly Detected' as alert_name,\n",
    "    'anomaly_severity >= 2' as condition,\n",
    "    'warning' as severity,\n",
    "    'Major or critical anomaly detected' as description\n",
    "UNION ALL\n",
    "SELECT \n",
    "    'Revenue Spike' as alert_name,\n",
    "    'hour_over_hour_growth > 200' as condition,\n",
    "    'info' as severity,\n",
    "    'Hour-over-hour growth exceeds 200%' as description\n",
    "\"\"\")\n",
    "\n",
    "alert_configs.write.mode(\"overwrite\").saveAsTable(\"viz_alert_configs\")\n",
    "print(\"✅ Created alert configurations\")\n",
    "\n",
    "# Current alerts based on data\n",
    "current_alerts = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    hour_key,\n",
    "    timestamp,\n",
    "    CASE \n",
    "        WHEN total_calls > 5000 THEN 'High Call Volume'\n",
    "        WHEN failure_rate > 25 THEN 'Critical Failure Rate'\n",
    "        WHEN network_stress_score > 70 THEN 'Network Stress Critical'\n",
    "        WHEN anomaly_severity >= 2 THEN 'Major Anomaly Detected'\n",
    "        ELSE NULL\n",
    "    END as alert_triggered,\n",
    "    total_calls,\n",
    "    failure_rate,\n",
    "    network_stress_score\n",
    "FROM viz_master_timeseries\n",
    "WHERE total_calls > 5000 \n",
    "   OR failure_rate > 25 \n",
    "   OR network_stress_score > 70 \n",
    "   OR anomaly_severity >= 2\n",
    "ORDER BY timestamp\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\n🚨 Active Alerts Summary:\")\n",
    "current_alerts.groupBy(\"alert_triggered\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b486695d-57cd-46b6-8747-070f81e7a92a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📊 VISUALIZATION SETUP COMPLETE\n",
      "================================================================================\n",
      "\n",
      "✅ TABLES CREATED FOR VISUALIZATION:\n",
      "   1. viz_master_timeseries - Complete time series data\n",
      "   2. viz_overall_kpis - High-level KPIs\n",
      "   3. viz_daily_kpis - Daily comparison metrics\n",
      "   4. viz_phase_kpis - Celebration phase analysis\n",
      "   5. viz_cell_performance - Geographic cell data\n",
      "   6. viz_user_segments - User segmentation\n",
      "   7. powerbi_* tables - PowerBI optimized datasets\n",
      "\n",
      "🎨 RECOMMENDED DASHBOARDS:\n",
      "\n",
      "1. EXECUTIVE DASHBOARD:\n",
      "   - KPI cards: Total calls, Peak hour, Success rate, Revenue\n",
      "   - Line chart: Hourly call volume with celebration phases\n",
      "   - Bar chart: Revenue by phase\n",
      "   - Gauge: Network stress level\n",
      "\n",
      "2. OPERATIONS DASHBOARD:\n",
      "   - Heatmap: Network stress by hour\n",
      "   - Scatter plot: Calls vs failure rate (colored by anomaly)\n",
      "   - Time series: Minute-by-minute midnight analysis\n",
      "   - Table: Active alerts\n",
      "\n",
      "3. USER ANALYTICS DASHBOARD:\n",
      "   - Pie chart: User segments\n",
      "   - Area chart: Active users by hour\n",
      "   - Bar chart: Call duration distribution\n",
      "   - Line chart: ARPU trends\n",
      "\n",
      "📝 SUPERSET SETUP INSTRUCTIONS:\n",
      "1. Import the datasets from Hive\n",
      "2. Create calculated fields for growth rates\n",
      "3. Set up color schemes (green→yellow→red for stress)\n",
      "4. Configure auto-refresh for real-time monitoring\n",
      "5. Add filters for date and celebration phase\n",
      "\n",
      "📝 POWERBI SETUP INSTRUCTIONS:\n",
      "1. Connect to Hive using ODBC connector\n",
      "2. Import powerbi_* tables\n",
      "3. Create relationships: fact_calls → dim_time\n",
      "4. Add DAX measures for YoY comparisons\n",
      "5. Use conditional formatting for anomalies\n",
      "\n",
      "🎯 KEY INSIGHTS TO HIGHLIGHT:\n",
      "   • 11.5x traffic surge on New Year's Day\n",
      "   • Midnight spike pattern (exact timing)\n",
      "   • Network handled load with <15% failure rate\n",
      "   • Revenue opportunity in free calls\n",
      "   • Cell-specific stress patterns\n",
      "\n",
      "✅ Visualization setup completed at: 2025-06-29 05:53:39.790781\n",
      "🚀 Ready to create stunning dashboards!\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 10: Final Dashboard Summary and Instructions\n",
    "# ------------------------------------------------------------\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"📊 VISUALIZATION SETUP COMPLETE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n✅ TABLES CREATED FOR VISUALIZATION:\")\n",
    "print(\"   1. viz_master_timeseries - Complete time series data\")\n",
    "print(\"   2. viz_overall_kpis - High-level KPIs\")\n",
    "print(\"   3. viz_daily_kpis - Daily comparison metrics\")\n",
    "print(\"   4. viz_phase_kpis - Celebration phase analysis\")\n",
    "print(\"   5. viz_cell_performance - Geographic cell data\")\n",
    "print(\"   6. viz_user_segments - User segmentation\")\n",
    "print(\"   7. powerbi_* tables - PowerBI optimized datasets\")\n",
    "\n",
    "print(\"\\n🎨 RECOMMENDED DASHBOARDS:\")\n",
    "print(\"\\n1. EXECUTIVE DASHBOARD:\")\n",
    "print(\"   - KPI cards: Total calls, Peak hour, Success rate, Revenue\")\n",
    "print(\"   - Line chart: Hourly call volume with celebration phases\")\n",
    "print(\"   - Bar chart: Revenue by phase\")\n",
    "print(\"   - Gauge: Network stress level\")\n",
    "\n",
    "print(\"\\n2. OPERATIONS DASHBOARD:\")\n",
    "print(\"   - Heatmap: Network stress by hour\")\n",
    "print(\"   - Scatter plot: Calls vs failure rate (colored by anomaly)\")\n",
    "print(\"   - Time series: Minute-by-minute midnight analysis\")\n",
    "print(\"   - Table: Active alerts\")\n",
    "\n",
    "print(\"\\n3. USER ANALYTICS DASHBOARD:\")\n",
    "print(\"   - Pie chart: User segments\")\n",
    "print(\"   - Area chart: Active users by hour\")\n",
    "print(\"   - Bar chart: Call duration distribution\")\n",
    "print(\"   - Line chart: ARPU trends\")\n",
    "\n",
    "print(\"\\n📝 SUPERSET SETUP INSTRUCTIONS:\")\n",
    "print(\"1. Import the datasets from Hive\")\n",
    "print(\"2. Create calculated fields for growth rates\")\n",
    "print(\"3. Set up color schemes (green→yellow→red for stress)\")\n",
    "print(\"4. Configure auto-refresh for real-time monitoring\")\n",
    "print(\"5. Add filters for date and celebration phase\")\n",
    "\n",
    "print(\"\\n📝 POWERBI SETUP INSTRUCTIONS:\")\n",
    "print(\"1. Connect to Hive using ODBC connector\")\n",
    "print(\"2. Import powerbi_* tables\")\n",
    "print(\"3. Create relationships: fact_calls → dim_time\")\n",
    "print(\"4. Add DAX measures for YoY comparisons\")\n",
    "print(\"5. Use conditional formatting for anomalies\")\n",
    "\n",
    "print(\"\\n🎯 KEY INSIGHTS TO HIGHLIGHT:\")\n",
    "print(\"   • 11.5x traffic surge on New Year's Day\")\n",
    "print(\"   • Midnight spike pattern (exact timing)\")\n",
    "print(\"   • Network handled load with <15% failure rate\")\n",
    "print(\"   • Revenue opportunity in free calls\")\n",
    "print(\"   • Cell-specific stress patterns\")\n",
    "\n",
    "print(f\"\\n✅ Visualization setup completed at: {datetime.now()}\")\n",
    "print(\"🚀 Ready to create stunning dashboards!\")\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8130e25-b287-4bf2-bab4-79301cb618b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession initialized (App: CDR Viz Exports to BI, Spark: 3.5.1)\n",
      "✅ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "✅ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "▶ Export de viz_master_timeseries → /home/jovyan/work/dashboards/exports/viz_master_timeseries\n",
      "▶ Export de viz_overall_kpis → /home/jovyan/work/dashboards/exports/viz_overall_kpis\n",
      "▶ Export de viz_daily_kpis → /home/jovyan/work/dashboards/exports/viz_daily_kpis\n",
      "▶ Export de viz_phase_kpis → /home/jovyan/work/dashboards/exports/viz_phase_kpis\n",
      "▶ Export de viz_cell_performance → /home/jovyan/work/dashboards/exports/viz_cell_performance\n",
      "▶ Export de viz_user_segments → /home/jovyan/work/dashboards/exports/viz_user_segments\n",
      "▶ Export de viz_user_activity → /home/jovyan/work/dashboards/exports/viz_user_activity\n",
      "▶ Export de powerbi_timeseries → /home/jovyan/work/dashboards/exports/powerbi_timeseries\n",
      "▶ Export de powerbi_fact_calls → /home/jovyan/work/dashboards/exports/powerbi_fact_calls\n",
      "▶ Export de powerbi_dim_time → /home/jovyan/work/dashboards/exports/powerbi_dim_time\n",
      "▶ Export de viz_dashboard_configs → /home/jovyan/work/dashboards/exports/viz_dashboard_configs\n",
      "▶ Export de viz_query_library → /home/jovyan/work/dashboards/exports/viz_query_library\n",
      "▶ Export de viz_alert_configs → /home/jovyan/work/dashboards/exports/viz_alert_configs\n",
      "✅ Export terminé. Maintenant fais un Refresh dans le file-browser sur ~/work/dashboards/exports\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = init_spark(\"CDR Viz Exports to BI\")\n",
    "spark.sql(\"USE algerie_telecom_cdr\")\n",
    "\n",
    "tables_to_export = [\n",
    "    \"viz_master_timeseries\",\n",
    "    \"viz_overall_kpis\",\n",
    "    \"viz_daily_kpis\",\n",
    "    \"viz_phase_kpis\",\n",
    "    \"viz_cell_performance\",\n",
    "    \"viz_user_segments\",\n",
    "    \"viz_user_activity\",\n",
    "    \"powerbi_timeseries\",\n",
    "    \"powerbi_fact_calls\",\n",
    "    \"powerbi_dim_time\",\n",
    "    \"viz_dashboard_configs\",\n",
    "    \"viz_query_library\",\n",
    "    \"viz_alert_configs\",\n",
    "]\n",
    "\n",
    "# on construit le chemin dans ton home\n",
    "home = os.environ[\"HOME\"]  # /home/jovyan\n",
    "export_root = os.path.join(home, \"work\", \"dashboards\", \"exports\")\n",
    "os.makedirs(export_root, exist_ok=True)\n",
    "\n",
    "for tbl in tables_to_export:\n",
    "    out_dir = os.path.join(export_root, tbl)\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    print(f\"▶ Export de {tbl} → {out_dir}\")\n",
    "    spark.table(tbl) \\\n",
    "         .coalesce(1) \\\n",
    "         .write \\\n",
    "         .mode(\"overwrite\") \\\n",
    "         .option(\"header\", \"true\") \\\n",
    "         .csv(out_dir)\n",
    "\n",
    "print(\"✅ Export terminé. Maintenant fais un Refresh dans le file-browser sur ~/work/dashboards/exports\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7c4e79f0-d9c6-4010-bcd0-95a578b7b524",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ exports.zip créé sous ~/work/dashboards/exports.zip\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "shutil.make_archive(\n",
    "    os.path.join(home, \"work\", \"dashboards\", \"exports\"), \n",
    "    \"zip\", \n",
    "    os.path.join(home, \"work\", \"dashboards\", \"exports\")\n",
    ")\n",
    "print(\"✅ exports.zip créé sous ~/work/dashboards/exports.zip\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1f8bb30-5fda-44c8-b8c1-a1d206529187",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cwd: /home/jovyan/work/work/spark-apps\n",
      "listing root: ['dev', 'etc', 'libx32', 'tmp', 'lib', 'boot', 'media', 'srv', 'sys', 'bin', 'lib32', 'opt', 'home', 'proc', 'root', 'sbin', 'var', 'lib64', 'mnt', 'run', 'usr']\n",
      "listing cwd: ['.ipynb_checkpoints', '01.Data_quality.ipynb', '02.Hive_Feature_Engineering_Real_CDR.ipynb', '03.Network-Trend-Analysis.ipynb', '04.Data_Viz.ipynb', 'anonymize_cdr.py', 'Anon_CDR_EDA_Quality.ipynb', 'CDR_Analysis.ipynb', 'CDR_Schema_Load', 'Delta_Lake_Tables.ipynb', 'derby.log', 'EDA_generate_CDR_AT.ipynb', 'exports.zip', 'Feature_Engineering.ipynb', 'Hive_Tables.ipynb', 'tmp']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"cwd:\", os.getcwd())\n",
    "print(\"listing root:\", os.listdir(\"/\"))\n",
    "print(\"listing cwd:\", os.listdir(os.getcwd()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3069566-a311-41b4-9ac4-46355464c04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan/work/work/spark-apps\n"
     ]
    }
   ],
   "source": [
    "!pwd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7504ab88-d2d6-4c4c-b4b9-5574e8dd8d2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 4048\n",
      "-rwxrwxrwx 1 root root  560136 Jul  2 00:59 01.Data_quality.ipynb\n",
      "-rwxrwxrwx 1 root root   94651 Jul  2 01:07 02.Hive_Feature_Engineering_Real_CDR.ipynb\n",
      "-rwxrwxrwx 1 root root   60766 Jun 29 05:26 03.Network-Trend-Analysis.ipynb\n",
      "-rwxrwxrwx 1 root root   85788 Jul  2 04:30 04.Data_Viz.ipynb\n",
      "-rwxrwxrwx 1 root root   80672 Jun 29 04:14 Anon_CDR_EDA_Quality.ipynb\n",
      "-rwxrwxrwx 1 root root    1621 Jun 16 21:05 anonymize_cdr.py\n",
      "-rwxrwxrwx 1 root root  131489 Jun 16 21:39 CDR_Analysis.ipynb\n",
      "drwxrwxrwx 1 root root    4096 Jun 16 12:10 CDR_Schema_Load\n",
      "-rwxrwxrwx 1 root root   31103 Jun  2 12:49 Delta_Lake_Tables.ipynb\n",
      "-rwxrwxrwx 1 root root 2613096 May  7 18:54 derby.log\n",
      "-rwxrwxrwx 1 root root   11454 Jun 25 00:50 EDA_generate_CDR_AT.ipynb\n",
      "-rwxrwxrwx 1 root root      22 Jul  2 01:49 exports.zip\n",
      "-rwxrwxrwx 1 root root  102069 Jun 22 15:21 Feature_Engineering.ipynb\n",
      "-rwxrwxrwx 1 root root  356166 Jun 25 20:55 Hive_Tables.ipynb\n",
      "drwxrwxrwx 1 root root    4096 Jul  2 01:30 tmp\n"
     ]
    }
   ],
   "source": [
    "!ls -l\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "91495a28-1663-4421-8c4f-34db7a76b2e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 0\n",
      "drwxrwxrwx 1 root root 4096 Jul  2 01:30 exports\n",
      "total 0\n"
     ]
    }
   ],
   "source": [
    "!ls -l tmp\n",
    "!ls -l tmp/exports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e834ad4-5ba2-467a-9380-8b79f9d42541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Je regarde : /home/jovyan/work/work/spark-apps/tmp/exports\n",
      "/home/jovyan/work/work/spark-apps/tmp/exports → 0 sous-dossiers ; 0 fichiers\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "base = os.getcwd()\n",
    "export_root = os.path.join(base, \"tmp\", \"exports\")\n",
    "print(\"Je regarde :\", export_root)\n",
    "for root, dirs, files in os.walk(export_root):\n",
    "    print(root, \"→\", len(dirs), \"sous-dossiers ;\", len(files), \"fichiers\")\n",
    "    for d in dirs:\n",
    "        print(\"  └─\", d)\n",
    "    for f in files:\n",
    "        print(\"     •\", f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a781a53d-5c03-4f57-9f0d-d73c70d3829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: hdfs: command not found\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /tmp/exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ab311e5-3adf-4ec9-9768-a1c55bf57d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "bash: line 5: hdfs: command not found\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'# cr\\xc3\\xa9e un dossier local\\nmkdir -p exports_hdfs\\n\\n# copie tout le r\\xc3\\xa9pertoire HDFS exports en local\\nhdfs dfs -copyToLocal /home/jovyan/work/dashboards/exports exports_hdfs\\n'' returned non-zero exit status 127.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m# crée un dossier local\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmkdir -p exports_hdfs\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m# copie tout le répertoire HDFS exports en local\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mhdfs dfs -copyToLocal /home/jovyan/work/dashboards/exports exports_hdfs\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/interactiveshell.py:2493\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2491\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2492\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2493\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2495\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2496\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2497\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'# cr\\xc3\\xa9e un dossier local\\nmkdir -p exports_hdfs\\n\\n# copie tout le r\\xc3\\xa9pertoire HDFS exports en local\\nhdfs dfs -copyToLocal /home/jovyan/work/dashboards/exports exports_hdfs\\n'' returned non-zero exit status 127."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# crée un dossier local\n",
    "mkdir -p exports_hdfs\n",
    "\n",
    "# copie tout le répertoire HDFS exports en local\n",
    "hdfs dfs -copyToLocal /home/jovyan/work/dashboards/exports exports_hdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d51bb0b5-f45e-4c89-891c-b323a15124d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 1: hdfs: command not found\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p exports_hdfs\n",
    "!hdfs dfs -copyToLocal /home/jovyan/work/dashboards/exports exports_hdfs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "9f5c1841-ca19-48d6-96b9-7e1403eaab13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/jovyan/local_exports': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -l ~/local_exports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "574b68d9-eb33-434b-bdfc-8ca0fd1a01b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ls: cannot access '/home/jovyan/local_exports': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!ls -R ~/local_exports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "12734663-5b65-4fc8-b8d4-37ed08feae93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://namenode:9000\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    spark.sparkContext._jsc\n",
    "                 .hadoopConfiguration()\n",
    "                 .get(\"fs.defaultFS\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "59f495f0-6e53-4370-8071-5653ae96c866",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hdfs://namenode:9000\n"
     ]
    }
   ],
   "source": [
    "fs_default = spark.sparkContext._jsc \\\n",
    "                .hadoopConfiguration() \\\n",
    "                .get(\"fs.defaultFS\")\n",
    "print(fs_default)   # e.g. hdfs://namenode:9000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604728a-2ba3-4e70-87c2-f4a77393d1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
