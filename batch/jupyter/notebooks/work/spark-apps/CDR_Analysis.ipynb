{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73073044-48b8-44cc-a1c7-a749217a6458",
   "metadata": {},
   "source": [
    "# CDR Analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "265605b9-40fd-450d-9d36-7c274bbe63f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"HADOOP_HOME\"] = \"/opt/hadoop\"\n",
    "os.environ[\"PATH\"] = os.environ[\"PATH\"] + \":/opt/hadoop/bin\"\n",
    "os.environ[\"HADOOP_CONF_DIR\"] = \"/hadoop/etc/hadoop\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c484127-a27c-4fae-86a8-54a9c09e911e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/05/07 19:04:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/05/07 19:04:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "25/05/07 19:04:07 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "\n",
      "Spark configuration:\n",
      "  spark.executor.memory: 2g\n",
      "  spark.driver.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "  spark.hadoop.fs.defaultFS: hdfs://namenode:9000\n",
      "  spark.executor.id: driver\n",
      "  spark.app.name: CDR-Analysis\n",
      "  spark.driver.host: 33292a9c2018\n",
      "  spark.driver.port: 45393\n",
      "  spark.rdd.compress: True\n",
      "  spark.executor.extraJavaOptions: -Djava.net.preferIPv6Addresses=false -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED -Djdk.reflect.useDirectMethodHandle=false\n",
      "  spark.master: spark://spark-master:7077\n",
      "  spark.driver.memory: 2g\n",
      "  spark.app.id: app-20250507190407-0001\n",
      "  spark.serializer.objectStreamReset: 100\n",
      "  spark.submit.pyFiles: \n",
      "  spark.submit.deployMode: client\n",
      "  spark.app.submitTime: 1746644646668\n",
      "  spark.ui.showConsoleProgress: true\n",
      "  spark.app.startTime: 1746644646733\n",
      "\n",
      "HDFS root directory contents:\n",
      "  tmp\n",
      "  user\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with HDFS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDR-Analysis\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Print Spark configuration for troubleshooting\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"\\nSpark configuration:\")\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"  {item[0]}: {item[1]}\")\n",
    "\n",
    "# Check if we can access HDFS through Spark\n",
    "try:\n",
    "    # List files in HDFS\n",
    "    files = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark.sparkContext._jsc.hadoopConfiguration()).listStatus(\n",
    "        spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\"/\"))\n",
    "    \n",
    "    print(\"\\nHDFS root directory contents:\")\n",
    "    for file in files:\n",
    "        print(f\"  {file.getPath().getName()}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError accessing HDFS through Spark: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3e155af-7226-460e-9d85-651653296207",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.5\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "48370ad1-ebd6-4dbc-91e7-125c66c9b11e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: findspark in /opt/conda/lib/python3.10/site-packages (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e11a5ffa-c6be-4ed8-b51f-aad6841ef091",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark session with HDFS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDR-Analysis\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8097d44f-8762-45e1-9ced-f1b24f733721",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in HDFS root directory:\n",
      "  hdfs://namenode:9000/tmp\n",
      "  hdfs://namenode:9000/user\n",
      "\n",
      "The /data directory doesn't exist in HDFS.\n"
     ]
    }
   ],
   "source": [
    "# List files in HDFS root directory using Spark\n",
    "try:\n",
    "    # Get the Hadoop FileSystem\n",
    "    fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark.sparkContext._jsc.hadoopConfiguration())\n",
    "    \n",
    "    # List files in the root directory\n",
    "    root_status = fs.listStatus(spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\"/\"))\n",
    "    \n",
    "    print(\"Files in HDFS root directory:\")\n",
    "    for status in root_status:\n",
    "        path = status.getPath().toString()\n",
    "        print(f\"  {path}\")\n",
    "        \n",
    "    # Now try to list your data directory (where your CDR data should be)\n",
    "    data_path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\"/data\")\n",
    "    if fs.exists(data_path):\n",
    "        data_status = fs.listStatus(data_path)\n",
    "        print(\"\\nFiles in /data directory:\")\n",
    "        for status in data_status:\n",
    "            path = status.getPath().toString()\n",
    "            print(f\"  {path}\")\n",
    "    else:\n",
    "        print(\"\\nThe /data directory doesn't exist in HDFS.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error accessing HDFS: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5fa8362-353a-4037-9c5e-01a414a7c004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error exploring HDFS directories: An error occurred while calling o110.listStatus.\n",
      ": java.io.FileNotFoundException: File /cdr does not exist.\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatusInternal(DistributedFileSystem.java:1104)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.access$600(DistributedFileSystem.java:147)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1175)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem$24.doCall(DistributedFileSystem.java:1172)\n",
      "\tat org.apache.hadoop.fs.FileSystemLinkResolver.resolve(FileSystemLinkResolver.java:81)\n",
      "\tat org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:1182)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explore the /cdr directory\n",
    "try:\n",
    "    cdr_path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\"/cdr\")\n",
    "    fs = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark.sparkContext._jsc.hadoopConfiguration())\n",
    "    \n",
    "    cdr_status = fs.listStatus(cdr_path)\n",
    "    print(\"Files in /cdr directory:\")\n",
    "    for status in cdr_status:\n",
    "        path = status.getPath().toString()\n",
    "        print(f\"  {path}\")\n",
    "        \n",
    "    # Check if there's a subdirectory like /cdr/data\n",
    "    cdr_data_path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\"/cdr/data\")\n",
    "    if fs.exists(cdr_data_path):\n",
    "        cdr_data_status = fs.listStatus(cdr_data_path)\n",
    "        print(\"\\nFiles in /cdr/data directory:\")\n",
    "        for status in cdr_data_status:\n",
    "            path = status.getPath().toString()\n",
    "            print(f\"  {path}\")\n",
    "    \n",
    "    # Explore the /data/converted directory\n",
    "    converted_path = spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\"/data/converted\")\n",
    "    converted_status = fs.listStatus(converted_path)\n",
    "    print(\"\\nFiles in /data/converted directory:\")\n",
    "    for status in converted_status:\n",
    "        path = status.getPath().toString()\n",
    "        print(f\"  {path}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Error exploring HDFS directories: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61da1325-0391-4354-8c67-ff2c51ed1a00",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/cdr/data/*.csv.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Read all CDR data files at once\u001b[39;00m\n\u001b[1;32m      2\u001b[0m cdr_df \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mheader\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minferSchema\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrue\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m\\\u001b[49m\n\u001b[0;32m----> 5\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhdfs://namenode:9000/cdr/data/*.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Check the schema\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCDR data schema:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/readwriter.py:740\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[1;32m    739\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spark\u001b[38;5;241m.\u001b[39m_sc\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 740\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    741\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    743\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/cdr/data/*.csv."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/05/07 19:09:38 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: Master removed our application: KILLED\n",
      "25/05/07 19:09:38 ERROR Inbox: Ignoring error\n",
      "org.apache.spark.SparkException: Exiting due to error from cluster scheduler: Master removed our application: KILLED\n",
      "\tat org.apache.spark.errors.SparkCoreErrors$.clusterSchedulerError(SparkCoreErrors.scala:291)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.error(TaskSchedulerImpl.scala:981)\n",
      "\tat org.apache.spark.scheduler.cluster.StandaloneSchedulerBackend.dead(StandaloneSchedulerBackend.scala:165)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint.markDead(StandaloneAppClient.scala:263)\n",
      "\tat org.apache.spark.deploy.client.StandaloneAppClient$ClientEndpoint$$anonfun$receive$1.applyOrElse(StandaloneAppClient.scala:170)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.$anonfun$process$1(Inbox.scala:115)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.safelyCall(Inbox.scala:213)\n",
      "\tat org.apache.spark.rpc.netty.Inbox.process(Inbox.scala:100)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop.org$apache$spark$rpc$netty$MessageLoop$$receiveLoop(MessageLoop.scala:75)\n",
      "\tat org.apache.spark.rpc.netty.MessageLoop$$anon$1.run(MessageLoop.scala:41)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1128)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:628)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "# Read all CDR data files at once\n",
    "cdr_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://namenode:9000/cdr/data/*.csv\")\n",
    "\n",
    "# Check the schema\n",
    "print(\"CDR data schema:\")\n",
    "cdr_df.printSchema()\n",
    "\n",
    "# Check the count of records\n",
    "print(f\"\\nTotal number of records: {cdr_df.count()}\")\n",
    "\n",
    "# Display a sample of the data\n",
    "print(\"\\nSample data:\")\n",
    "cdr_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98d8fa7a-5459-410b-904d-80ef482866bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CDR data schema:\n",
      "root\n",
      " |-- CDR_ID: long (nullable = true)\n",
      " |-- CDR_SUB_ID: integer (nullable = true)\n",
      " |-- CDR_TYPE: string (nullable = true)\n",
      " |-- SPLIT_CDR_REASON: string (nullable = true)\n",
      " |-- CDR_BATCH_ID: long (nullable = true)\n",
      " |-- SRC_REC_LINE_NO: integer (nullable = true)\n",
      " |-- SRC_CDR_ID: integer (nullable = true)\n",
      " |-- SRC_CDR_NO: long (nullable = true)\n",
      " |-- STATUS: string (nullable = true)\n",
      " |-- RE_RATING_TIMES: integer (nullable = true)\n",
      " |-- CREATE_DATE: long (nullable = true)\n",
      " |-- START_DATE: long (nullable = true)\n",
      " |-- END_DATE: long (nullable = true)\n",
      " |-- CUST_LOCAL_START_DATE: long (nullable = true)\n",
      " |-- CUST_LOCAL_END_DATE: long (nullable = true)\n",
      " |-- STD_EVT_TYPE_ID: integer (nullable = true)\n",
      " |-- EVT_SOURCE_CATEGORY: string (nullable = true)\n",
      " |-- OBJ_TYPE: string (nullable = true)\n",
      " |-- OBJ_ID: long (nullable = true)\n",
      " |-- OWNER_CUST_ID: long (nullable = true)\n",
      " |-- DEFAULT_ACCT_ID: long (nullable = true)\n",
      " |-- PRI_IDENTITY: integer (nullable = true)\n",
      " |-- BILL_CYCLE_ID: integer (nullable = true)\n",
      " |-- SERVICE_CATEGORY: integer (nullable = true)\n",
      " |-- USAGE_SERVICE_TYPE: integer (nullable = true)\n",
      " |-- SESSION_ID: string (nullable = true)\n",
      " |-- RESULT_CODE: integer (nullable = true)\n",
      " |-- RESULT_REASON: integer (nullable = true)\n",
      " |-- BE_ID: integer (nullable = true)\n",
      " |-- HOT_SEQ: integer (nullable = true)\n",
      " |-- CP_ID: integer (nullable = true)\n",
      " |-- RECIPIENT_NUMBER: long (nullable = true)\n",
      " |-- USAGE_MEASURE_ID: integer (nullable = true)\n",
      " |-- ACTUAL_USAGE: integer (nullable = true)\n",
      " |-- RATE_USAGE: integer (nullable = true)\n",
      " |-- SERVICE_UNIT_TYPE: integer (nullable = true)\n",
      " |-- USAGE_MEASURE_ID2: integer (nullable = true)\n",
      " |-- ACTUAL_USAGE2: integer (nullable = true)\n",
      " |-- RATE_USAGE2: integer (nullable = true)\n",
      " |-- SERVICE_UNIT_TYPE2: integer (nullable = true)\n",
      " |-- DEBIT_AMOUNT: integer (nullable = true)\n",
      " |-- UN_DEBIT_AMOUNT: integer (nullable = true)\n",
      " |-- DEBIT_FROM_PREPAID: integer (nullable = true)\n",
      " |-- DEBIT_FROM_ADVANCE_PREPAID: integer (nullable = true)\n",
      " |-- DEBIT_FROM_POSTPAID: integer (nullable = true)\n",
      " |-- DEBIT_FROM_ADVANCE_POSTPAID: integer (nullable = true)\n",
      " |-- DEBIT_FROM_CREDIT_POSTPAID: integer (nullable = true)\n",
      " |-- TOTAL_TAX: integer (nullable = true)\n",
      " |-- FREE_UNIT_AMOUNT_OF_TIMES: integer (nullable = true)\n",
      " |-- FREE_UNIT_AMOUNT_OF_DURATION: integer (nullable = true)\n",
      " |-- FREE_UNIT_AMOUNT_OF_FLUX: integer (nullable = true)\n",
      " |-- ACCT_ID: double (nullable = true)\n",
      " |-- ACCT_BALANCE_ID: double (nullable = true)\n",
      " |-- BALANCE_TYPE: double (nullable = true)\n",
      " |-- CUR_BALANCE: double (nullable = true)\n",
      " |-- CHG_BALANCE: double (nullable = true)\n",
      " |-- CURRENCY_ID: double (nullable = true)\n",
      " |-- OPER_TYPE: double (nullable = true)\n",
      " |-- ACCT_ID.1: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.1: string (nullable = true)\n",
      " |-- BALANCE_TYPE.1: string (nullable = true)\n",
      " |-- CUR_BALANCE.1: string (nullable = true)\n",
      " |-- CHG_BALANCE.1: string (nullable = true)\n",
      " |-- CURRENCY_ID.1: string (nullable = true)\n",
      " |-- OPER_TYPE.1: string (nullable = true)\n",
      " |-- ACCT_ID.2: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.2: string (nullable = true)\n",
      " |-- BALANCE_TYPE.2: string (nullable = true)\n",
      " |-- CUR_BALANCE.2: string (nullable = true)\n",
      " |-- CHG_BALANCE.2: string (nullable = true)\n",
      " |-- CURRENCY_ID.2: string (nullable = true)\n",
      " |-- OPER_TYPE.2: string (nullable = true)\n",
      " |-- ACCT_ID.3: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.3: string (nullable = true)\n",
      " |-- BALANCE_TYPE.3: string (nullable = true)\n",
      " |-- CUR_BALANCE.3: string (nullable = true)\n",
      " |-- CHG_BALANCE.3: string (nullable = true)\n",
      " |-- CURRENCY_ID.3: string (nullable = true)\n",
      " |-- OPER_TYPE.3: string (nullable = true)\n",
      " |-- ACCT_ID.4: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.4: string (nullable = true)\n",
      " |-- BALANCE_TYPE.4: string (nullable = true)\n",
      " |-- CUR_BALANCE.4: string (nullable = true)\n",
      " |-- CHG_BALANCE.4: string (nullable = true)\n",
      " |-- CURRENCY_ID.4: string (nullable = true)\n",
      " |-- OPER_TYPE.4: string (nullable = true)\n",
      " |-- ACCT_ID.5: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.5: string (nullable = true)\n",
      " |-- BALANCE_TYPE.5: string (nullable = true)\n",
      " |-- CUR_BALANCE.5: string (nullable = true)\n",
      " |-- CHG_BALANCE.5: string (nullable = true)\n",
      " |-- CURRENCY_ID.5: string (nullable = true)\n",
      " |-- OPER_TYPE.5: string (nullable = true)\n",
      " |-- ACCT_ID.6: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.6: string (nullable = true)\n",
      " |-- BALANCE_TYPE.6: string (nullable = true)\n",
      " |-- CUR_BALANCE.6: string (nullable = true)\n",
      " |-- CHG_BALANCE.6: string (nullable = true)\n",
      " |-- CURRENCY_ID.6: string (nullable = true)\n",
      " |-- OPER_TYPE.6: string (nullable = true)\n",
      " |-- ACCT_ID.7: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.7: string (nullable = true)\n",
      " |-- BALANCE_TYPE.7: string (nullable = true)\n",
      " |-- CUR_BALANCE.7: string (nullable = true)\n",
      " |-- CHG_BALANCE.7: string (nullable = true)\n",
      " |-- CURRENCY_ID.7: string (nullable = true)\n",
      " |-- OPER_TYPE.7: string (nullable = true)\n",
      " |-- ACCT_ID.8: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.8: string (nullable = true)\n",
      " |-- BALANCE_TYPE.8: string (nullable = true)\n",
      " |-- CUR_BALANCE.8: string (nullable = true)\n",
      " |-- CHG_BALANCE.8: string (nullable = true)\n",
      " |-- CURRENCY_ID.8: string (nullable = true)\n",
      " |-- OPER_TYPE.8: string (nullable = true)\n",
      " |-- ACCT_ID.9: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.9: string (nullable = true)\n",
      " |-- BALANCE_TYPE.9: string (nullable = true)\n",
      " |-- CUR_BALANCE.9: string (nullable = true)\n",
      " |-- CHG_BALANCE.9: string (nullable = true)\n",
      " |-- CURRENCY_ID.9: string (nullable = true)\n",
      " |-- OPER_TYPE.9: string (nullable = true)\n",
      " |-- FU_OWN_TYPE: string (nullable = true)\n",
      " |-- FU_OWN_ID: double (nullable = true)\n",
      " |-- FREE_UNIT_ID: double (nullable = true)\n",
      " |-- FREE_UNIT_TYPE: double (nullable = true)\n",
      " |-- CUR_AMOUNT: double (nullable = true)\n",
      " |-- CHG_AMOUNT: double (nullable = true)\n",
      " |-- FU_MEASURE_ID: double (nullable = true)\n",
      " |-- OPER_TYPE.10: double (nullable = true)\n",
      " |-- FU_OWN_TYPE.1: string (nullable = true)\n",
      " |-- FU_OWN_ID.1: double (nullable = true)\n",
      " |-- FREE_UNIT_ID.1: double (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.1: double (nullable = true)\n",
      " |-- CUR_AMOUNT.1: double (nullable = true)\n",
      " |-- CHG_AMOUNT.1: double (nullable = true)\n",
      " |-- FU_MEASURE_ID.1: double (nullable = true)\n",
      " |-- OPER_TYPE.11: double (nullable = true)\n",
      " |-- FU_OWN_TYPE.2: string (nullable = true)\n",
      " |-- FU_OWN_ID.2: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.2: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.2: string (nullable = true)\n",
      " |-- CUR_AMOUNT.2: string (nullable = true)\n",
      " |-- CHG_AMOUNT.2: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.2: string (nullable = true)\n",
      " |-- OPER_TYPE.12: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.3: string (nullable = true)\n",
      " |-- FU_OWN_ID.3: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.3: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.3: string (nullable = true)\n",
      " |-- CUR_AMOUNT.3: string (nullable = true)\n",
      " |-- CHG_AMOUNT.3: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.3: string (nullable = true)\n",
      " |-- OPER_TYPE.13: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.4: string (nullable = true)\n",
      " |-- FU_OWN_ID.4: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.4: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.4: string (nullable = true)\n",
      " |-- CUR_AMOUNT.4: string (nullable = true)\n",
      " |-- CHG_AMOUNT.4: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.4: string (nullable = true)\n",
      " |-- OPER_TYPE.14: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.5: string (nullable = true)\n",
      " |-- FU_OWN_ID.5: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.5: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.5: string (nullable = true)\n",
      " |-- CUR_AMOUNT.5: string (nullable = true)\n",
      " |-- CHG_AMOUNT.5: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.5: string (nullable = true)\n",
      " |-- OPER_TYPE.15: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.6: string (nullable = true)\n",
      " |-- FU_OWN_ID.6: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.6: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.6: string (nullable = true)\n",
      " |-- CUR_AMOUNT.6: string (nullable = true)\n",
      " |-- CHG_AMOUNT.6: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.6: string (nullable = true)\n",
      " |-- OPER_TYPE.16: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.7: string (nullable = true)\n",
      " |-- FU_OWN_ID.7: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.7: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.7: string (nullable = true)\n",
      " |-- CUR_AMOUNT.7: string (nullable = true)\n",
      " |-- CHG_AMOUNT.7: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.7: string (nullable = true)\n",
      " |-- OPER_TYPE.17: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.8: string (nullable = true)\n",
      " |-- FU_OWN_ID.8: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.8: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.8: string (nullable = true)\n",
      " |-- CUR_AMOUNT.8: string (nullable = true)\n",
      " |-- CHG_AMOUNT.8: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.8: string (nullable = true)\n",
      " |-- OPER_TYPE.18: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.9: string (nullable = true)\n",
      " |-- FU_OWN_ID.9: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.9: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.9: string (nullable = true)\n",
      " |-- CUR_AMOUNT.9: string (nullable = true)\n",
      " |-- CHG_AMOUNT.9: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.9: string (nullable = true)\n",
      " |-- OPER_TYPE.19: string (nullable = true)\n",
      " |-- ACCT_ID.10: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.10: string (nullable = true)\n",
      " |-- BALANCE_TYPE.10: string (nullable = true)\n",
      " |-- BONUS_AMOUNT: string (nullable = true)\n",
      " |-- CURRENT_BALANCE: string (nullable = true)\n",
      " |-- CURRENCY_ID.10: string (nullable = true)\n",
      " |-- OPER_TYPE.20: string (nullable = true)\n",
      " |-- ACCT_ID.11: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.11: string (nullable = true)\n",
      " |-- BALANCE_TYPE.11: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.1: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.1: string (nullable = true)\n",
      " |-- CURRENCY_ID.11: string (nullable = true)\n",
      " |-- OPER_TYPE.21: string (nullable = true)\n",
      " |-- ACCT_ID.12: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.12: string (nullable = true)\n",
      " |-- BALANCE_TYPE.12: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.2: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.2: string (nullable = true)\n",
      " |-- CURRENCY_ID.12: string (nullable = true)\n",
      " |-- OPER_TYPE.22: string (nullable = true)\n",
      " |-- ACCT_ID.13: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.13: string (nullable = true)\n",
      " |-- BALANCE_TYPE.13: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.3: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.3: string (nullable = true)\n",
      " |-- CURRENCY_ID.13: string (nullable = true)\n",
      " |-- OPER_TYPE.23: string (nullable = true)\n",
      " |-- ACCT_ID.14: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.14: string (nullable = true)\n",
      " |-- BALANCE_TYPE.14: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.4: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.4: string (nullable = true)\n",
      " |-- CURRENCY_ID.14: string (nullable = true)\n",
      " |-- OPER_TYPE.24: string (nullable = true)\n",
      " |-- ACCT_ID.15: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.15: string (nullable = true)\n",
      " |-- BALANCE_TYPE.15: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.5: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.5: string (nullable = true)\n",
      " |-- CURRENCY_ID.15: string (nullable = true)\n",
      " |-- OPER_TYPE.25: string (nullable = true)\n",
      " |-- ACCT_ID.16: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.16: string (nullable = true)\n",
      " |-- BALANCE_TYPE.16: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.6: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.6: string (nullable = true)\n",
      " |-- CURRENCY_ID.16: string (nullable = true)\n",
      " |-- OPER_TYPE.26: string (nullable = true)\n",
      " |-- ACCT_ID.17: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.17: string (nullable = true)\n",
      " |-- BALANCE_TYPE.17: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.7: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.7: string (nullable = true)\n",
      " |-- CURRENCY_ID.17: string (nullable = true)\n",
      " |-- OPER_TYPE.27: string (nullable = true)\n",
      " |-- ACCT_ID.18: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.18: string (nullable = true)\n",
      " |-- BALANCE_TYPE.18: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.8: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.8: string (nullable = true)\n",
      " |-- CURRENCY_ID.18: string (nullable = true)\n",
      " |-- OPER_TYPE.28: string (nullable = true)\n",
      " |-- ACCT_ID.19: string (nullable = true)\n",
      " |-- ACCT_BALANCE_ID.19: string (nullable = true)\n",
      " |-- BALANCE_TYPE.19: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.9: string (nullable = true)\n",
      " |-- CURRENT_BALANCE.9: string (nullable = true)\n",
      " |-- CURRENCY_ID.19: string (nullable = true)\n",
      " |-- OPER_TYPE.29: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.10: string (nullable = true)\n",
      " |-- FU_OWN_ID.10: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.10: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.10: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.10: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.10: string (nullable = true)\n",
      " |-- OPER_TYPE.30: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.11: string (nullable = true)\n",
      " |-- FU_OWN_ID.11: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.11: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.11: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.11: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.1: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.11: string (nullable = true)\n",
      " |-- OPER_TYPE.31: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.12: string (nullable = true)\n",
      " |-- FU_OWN_ID.12: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.12: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.12: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.12: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.2: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.12: string (nullable = true)\n",
      " |-- OPER_TYPE.32: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.13: string (nullable = true)\n",
      " |-- FU_OWN_ID.13: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.13: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.13: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.13: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.3: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.13: string (nullable = true)\n",
      " |-- OPER_TYPE.33: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.14: string (nullable = true)\n",
      " |-- FU_OWN_ID.14: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.14: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.14: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.14: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.4: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.14: string (nullable = true)\n",
      " |-- OPER_TYPE.34: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.15: string (nullable = true)\n",
      " |-- FU_OWN_ID.15: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.15: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.15: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.15: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.5: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.15: string (nullable = true)\n",
      " |-- OPER_TYPE.35: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.16: string (nullable = true)\n",
      " |-- FU_OWN_ID.16: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.16: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.16: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.16: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.6: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.16: string (nullable = true)\n",
      " |-- OPER_TYPE.36: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.17: string (nullable = true)\n",
      " |-- FU_OWN_ID.17: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.17: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.17: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.17: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.7: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.17: string (nullable = true)\n",
      " |-- OPER_TYPE.37: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.18: string (nullable = true)\n",
      " |-- FU_OWN_ID.18: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.18: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.18: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.18: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.8: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.18: string (nullable = true)\n",
      " |-- OPER_TYPE.38: string (nullable = true)\n",
      " |-- FU_OWN_TYPE.19: string (nullable = true)\n",
      " |-- FU_OWN_ID.19: string (nullable = true)\n",
      " |-- FREE_UNIT_TYPE.19: string (nullable = true)\n",
      " |-- FREE_UNIT_ID.19: string (nullable = true)\n",
      " |-- BONUS_AMOUNT.19: string (nullable = true)\n",
      " |-- CURRENT_AMOUNT.9: string (nullable = true)\n",
      " |-- FU_MEASURE_ID.19: string (nullable = true)\n",
      " |-- OPER_TYPE.39: string (nullable = true)\n",
      " |-- CallingPartyNumber: long (nullable = true)\n",
      " |-- CalledPartyNumber: long (nullable = true)\n",
      " |-- CallingPartyIMSI: double (nullable = true)\n",
      " |-- CalledPartyIMSI: double (nullable = true)\n",
      " |-- DialedNumber: double (nullable = true)\n",
      " |-- OriginalCalledParty: double (nullable = true)\n",
      " |-- ServiceFlow: integer (nullable = true)\n",
      " |-- CallForwardIndicator: integer (nullable = true)\n",
      " |-- CallingRoamInfo: string (nullable = true)\n",
      " |-- CallingCellID: double (nullable = true)\n",
      " |-- CalledRoamInfo: string (nullable = true)\n",
      " |-- CalledCellID: string (nullable = true)\n",
      " |-- TimeStampOfSSP: long (nullable = true)\n",
      " |-- TimeZoneOfSSP: integer (nullable = true)\n",
      " |-- BearerCapability: integer (nullable = true)\n",
      " |-- ChargingTime: long (nullable = true)\n",
      " |-- WaitDuration: integer (nullable = true)\n",
      " |-- TerminationReason: integer (nullable = true)\n",
      " |-- CallReferenceNumber: string (nullable = true)\n",
      " |-- IMEI: string (nullable = true)\n",
      " |-- AccessPrefix: string (nullable = true)\n",
      " |-- RoutingPrefix: string (nullable = true)\n",
      " |-- RedirectingPartyID: double (nullable = true)\n",
      " |-- MSCAddress: string (nullable = true)\n",
      " |-- BrandID: integer (nullable = true)\n",
      " |-- MainOfferingID: integer (nullable = true)\n",
      " |-- ChargingPartyNumber: integer (nullable = true)\n",
      " |-- ChargePartyIndicator: string (nullable = true)\n",
      " |-- PayType: integer (nullable = true)\n",
      " |-- ChargingType: integer (nullable = true)\n",
      " |-- CallType: integer (nullable = true)\n",
      " |-- RoamState: integer (nullable = true)\n",
      " |-- CallingHomeCountryCode: integer (nullable = true)\n",
      " |-- CallingHomeAreaNumber: integer (nullable = true)\n",
      " |-- CallingHomeNetworkCode: integer (nullable = true)\n",
      " |-- CallingRoamCountryCode: integer (nullable = true)\n",
      " |-- CallingRoamAreaNumber: integer (nullable = true)\n",
      " |-- CallingRoamNetworkCode: integer (nullable = true)\n",
      " |-- CalledHomeCountryCode: integer (nullable = true)\n",
      " |-- CalledHomeAreaNumber: integer (nullable = true)\n",
      " |-- CalledHomeNetworkCode: integer (nullable = true)\n",
      " |-- CalledRoamCountryCode: integer (nullable = true)\n",
      " |-- CalledRoamAreaNumber: integer (nullable = true)\n",
      " |-- CalledRoamNetworkCode: integer (nullable = true)\n",
      " |-- ServiceType: string (nullable = true)\n",
      " |-- HotLineIndicator: integer (nullable = true)\n",
      " |-- HomeZoneID: integer (nullable = true)\n",
      " |-- SpecialZoneID: integer (nullable = true)\n",
      " |-- NPFlag: integer (nullable = true)\n",
      " |-- NPPrefix: string (nullable = true)\n",
      " |-- CallingCUGNo: integer (nullable = true)\n",
      " |-- CalledCUGNo: integer (nullable = true)\n",
      " |-- OpposeNumberType: integer (nullable = true)\n",
      " |-- CallingNetworkType: integer (nullable = true)\n",
      " |-- CalledNetworkType: integer (nullable = true)\n",
      " |-- CallingVPNTopGroupNumber: double (nullable = true)\n",
      " |-- CallingVPNGroupNumber: double (nullable = true)\n",
      " |-- CallingVPNShortNumber: double (nullable = true)\n",
      " |-- CalledVPNTopGroupNumber: double (nullable = true)\n",
      " |-- CalledVPNGroupNumber: double (nullable = true)\n",
      " |-- CalledVPNShortNumber: double (nullable = true)\n",
      " |-- GroupCallType: integer (nullable = true)\n",
      " |-- OnlineChargingFlag: integer (nullable = true)\n",
      " |-- StartTimeOfBillCycle: long (nullable = true)\n",
      " |-- LastEffectOffering: integer (nullable = true)\n",
      " |-- DTDiscount: integer (nullable = true)\n",
      " |-- OpposeMainOfferingID: integer (nullable = true)\n",
      " |-- MainBalanceInfo: string (nullable = true)\n",
      " |-- ChgBalanceInfo: string (nullable = true)\n",
      " |-- ChgFreeUnitInfo: string (nullable = true)\n",
      " |-- UserState: integer (nullable = true)\n",
      " |-- GroupPayFlag: integer (nullable = true)\n",
      " |-- RoamingZoneID: string (nullable = true)\n",
      " |-- PrimaryOfferChgAmt: integer (nullable = true)\n",
      " |-- OriginatingIOI: string (nullable = true)\n",
      " |-- TerminatingIOI: string (nullable = true)\n",
      " |-- IMSChargingIdentifier: string (nullable = true)\n",
      " |-- TFTFlag: integer (nullable = true)\n",
      "\n",
      "\n",
      "Total number of records: 89911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Read all CDR data files at once \n",
    "cdr_df = spark.read \\\n",
    ".option(\"header\", \"true\") \\\n",
    ".option(\"inferSchema\" , \"true\")\\\n",
    ".csv(\"hdfs://namenode:9000/cdr/data/*csv\")\n",
    "\n",
    "# Check the schema \n",
    "print(\"CDR data schema:\")\n",
    "cdr_df.printSchema()\n",
    "\n",
    "# Check the count of records\n",
    "print(f\"\\nTotal number of records: {cdr_df.count()}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04204e2a-e659-42af-9dab-e9c4bb19484a",
   "metadata": {},
   "source": [
    "### Cleaning CDR Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef49a999-abd2-42c6-903f-9e12be76b1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if we can access HDFS through Spark\n",
    "try:\n",
    "    # List files in HDFS\n",
    "    files = spark.sparkContext._jvm.org.apache.hadoop.fs.FileSystem.get(\n",
    "        spark.sparkContext._jsc.hadoopConfiguration()).listStatus(\n",
    "        spark.sparkContext._jvm.org.apache.hadoop.fs.Path(\"/\"))\n",
    "    \n",
    "    print(\"\\nHDFS root directory contents:\")\n",
    "    for file in files:\n",
    "        print(f\"  {file.getPath().getName()}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"\\nError accessing HDFS through Spark: {e}\")\n",
    "\n",
    "# Add project root to Python path for imports\n",
    "current_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(current_dir, \"../..\"))\n",
    "sys.path.append(project_root)\n",
    "\n",
    "# Import backup utilities\n",
    "try:\n",
    "    from src.utils.backup_utils import backup_hdfs_to_local, backup_hdfs_to_hdfs\n",
    "except ImportError:\n",
    "    print(\"Warning: Couldn't import backup utilities. Proceeding without backup.\")\n",
    "    \n",
    "    def backup_hdfs_to_local(src, dst):\n",
    "        print(f\"Skipping local backup from {src} to {dst}\")\n",
    "        return None\n",
    "        \n",
    "    def backup_hdfs_to_hdfs(src, dst):\n",
    "        print(f\"Skipping HDFS backup from {src} to {dst}\")\n",
    "        return None\n",
    "\n",
    "# Source and backup paths\n",
    "hdfs_source_path = \"/cdr/data\"\n",
    "local_backup_dir = \"D:/network_trend_analysis/backups/cdr_data\"\n",
    "hdfs_backup_prefix = \"/user/backup/cdr_data/backup\"\n",
    "\n",
    "# Perform backups before processing\n",
    "print(\"Creating backups before processing data...\")\n",
    "hdfs_backup_path = backup_hdfs_to_hdfs(hdfs_source_path, hdfs_backup_prefix)\n",
    "local_backup_path = backup_hdfs_to_local(hdfs_source_path, local_backup_dir)\n",
    "\n",
    "if not (hdfs_backup_path or local_backup_path):\n",
    "    print(\"WARNING: All backups failed. Do you want to continue anyway? (y/n)\")\n",
    "    response = input().strip().lower()\n",
    "    if response != 'y':\n",
    "        print(\"Aborting data processing.\")\n",
    "        sys.exit(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "754af045-960c-4a93-a69f-ea739e02e59e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/19 01:46:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "Spark session initialized.\n",
      "Spark version: 3.3.4\n",
      "\n",
      "Spark configuration:\n",
      "  spark.executor.memory: 2g\n",
      "  spark.app.submitTime: 1745023947046\n",
      "  spark.driver.host: 9b90dc4a8722\n",
      "  spark.hadoop.fs.defaultFS: hdfs://namenode:9000\n",
      "  spark.executor.id: driver\n",
      "  spark.app.name: CDR-Analysis\n",
      "  spark.app.id: app-20250419005228-0000\n",
      "  spark.app.startTime: 1745023947158\n",
      "  spark.driver.extraJavaOptions: -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "  spark.rdd.compress: True\n",
      "  spark.driver.port: 44123\n",
      "  spark.master: spark://spark-master:7077\n",
      "  spark.driver.memory: 2g\n",
      "  spark.serializer.objectStreamReset: 100\n",
      "  spark.submit.pyFiles: \n",
      "  spark.submit.deployMode: client\n",
      "  spark.executor.extraJavaOptions: -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "  spark.sql.warehouse.dir: file:/home/jovyan/work/spark-warehouse\n",
      "  spark.ui.showConsoleProgress: true\n",
      "Loading CDR data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 89911 CDR records.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1330.get.\n: java.util.NoSuchElementException: spark.sql.execution.pythonUDF.arrow.enabled\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:1678)\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:4587)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:4587)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:72)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Register UDF\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m convert_epoch_udf \u001b[38;5;241m=\u001b[39m \u001b[43mudf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_epoch_to_datetime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Determine the line type based on Algerian phone number format\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_line_type\u001b[39m(phone_number):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/functions.py:15767\u001b[0m, in \u001b[0;36mudf\u001b[0;34m(f, returnType, useArrow)\u001b[0m\n\u001b[1;32m  15761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m  15762\u001b[0m         _create_py_udf,\n\u001b[1;32m  15763\u001b[0m         returnType\u001b[38;5;241m=\u001b[39mreturn_type,\n\u001b[1;32m  15764\u001b[0m         useArrow\u001b[38;5;241m=\u001b[39museArrow,\n\u001b[1;32m  15765\u001b[0m     )\n\u001b[1;32m  15766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 15767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_py_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museArrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43museArrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/udf.py:127\u001b[0m, in \u001b[0;36m_create_py_udf\u001b[0;34m(f, returnType, useArrow)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m    123\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession\n\u001b[1;32m    124\u001b[0m     is_arrow_enabled \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.execution.pythonUDF.arrow.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     is_arrow_enabled \u001b[38;5;241m=\u001b[39m useArrow\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/conf.py:54\u001b[0m, in \u001b[0;36mRuntimeConfig.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkType(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m _NoValue:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1330.get.\n: java.util.NoSuchElementException: spark.sql.execution.pythonUDF.arrow.enabled\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:1678)\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:4587)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:4587)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:72)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create Spark session with HDFS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDR-Analysis\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized.\")\n",
    "\n",
    "# Print Spark configuration for troubleshooting\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"\\nSpark configuration:\")\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"  {item[0]}: {item[1]}\")\n",
    "\n",
    "\n",
    "# Define constants for Algerian telecom data\n",
    "# Wilaya prefix mapping with format validation\n",
    "wilaya_prefix_map = {\n",
    "    'Algiers': ['021', '023', '044'],  # First prefix is for ADSL, second is for FTTH\n",
    "    'Oran': ['041', '042'],\n",
    "    'Constantine': ['031', '043'],\n",
    "    'Annaba': ['038', '045'],\n",
    "    'Bejaia': ['034', '046'],\n",
    "    'Tizi Ouzou': ['026', '047'],\n",
    "    'Blida': ['025', '048'],\n",
    "    'Setif': ['036', '049'],\n",
    "    'Batna': ['033', '039'],\n",
    "    'Tlemcen': ['043', '032'],\n",
    "    'Mostaganem': ['045', '037'],\n",
    "    'Boumerdas': ['024', '050'],\n",
    "}\n",
    "\n",
    "# Mobile operator prefixes - ensuring format 0yxx\n",
    "mobile_operator_prefixes = {\n",
    "    'Ooredoo': ['055', '056'],  # 0550, 0560 format\n",
    "    'Mobilis': ['066', '067'],  # 0660, 0661 format\n",
    "    'Djezzy': ['077', '078'],   # 0770, 0771, 0772 format\n",
    "}\n",
    "\n",
    "# ADSL and FTTH prefixes - ensuring landline format 0yy\n",
    "adsl_prefixes = [\"021\", \"023\", \"041\", \"031\", \"038\", \"034\", \"026\", \"025\", \"036\", \"033\", \"043\", \"045\"]\n",
    "ftth_prefixes = [\"044\", \"042\", \"043\", \"045\", \"046\", \"047\", \"048\", \"049\", \"032\", \"037\", \"039\", \"050\"]\n",
    "\n",
    "# Call types\n",
    "call_types = {\n",
    "    1: \"VOICE\",\n",
    "    2: \"SMS\",\n",
    "    3: \"DATA\",\n",
    "    4: \"MMS\",\n",
    "    5: \"VOICE_ROAMING\",\n",
    "    6: \"SMS_ROAMING\",\n",
    "    7: \"DATA_ROAMING\",\n",
    "    8: \"MMS_ROAMING\"\n",
    "}\n",
    "print(\"Loading CDR data...\")\n",
    "\n",
    "# Load the CDR data\n",
    "cdr_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"hdfs://namenode:9000{hdfs_source_path}/*csv\")\n",
    "\n",
    "print(f\"Loaded {cdr_df.count()} CDR records.\")\n",
    "\n",
    "# Function to convert epoch milliseconds to datetime with proper format\n",
    "def convert_epoch_to_datetime(epoch_time):\n",
    "    try:\n",
    "        if epoch_time is not None and epoch_time > 0:\n",
    "            dt = datetime.datetime.fromtimestamp(epoch_time/1000)\n",
    "            # Format as DD/MM/YY HH:MM:SS\n",
    "            return dt.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting epoch time: {e}\")\n",
    "        return None\n",
    "\n",
    "# Register UDF\n",
    "convert_epoch_udf = udf(convert_epoch_to_datetime, StringType())\n",
    "\n",
    "# Determine the line type based on Algerian phone number format\n",
    "def get_line_type(phone_number):\n",
    "    if phone_number is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Convert to string and ensure it's properly formatted\n",
    "    try:\n",
    "        num_str = str(int(phone_number))\n",
    "    except:\n",
    "        return \"Invalid\"\n",
    "    \n",
    "    # Check for emergency numbers\n",
    "    for service, number in emergency_numbers.items():\n",
    "        if num_str.endswith(number):\n",
    "            return f\"Emergency-{service}\"\n",
    "    \n",
    "    # Standardize format for analysis\n",
    "    if num_str.startswith('00'):\n",
    "        # International format with 00 prefix\n",
    "        return \"International\"\n",
    "    elif num_str.startswith('213'):\n",
    "        # Algerian number with country code\n",
    "        stripped_num = num_str[3:]\n",
    "    elif num_str.startswith('0'):\n",
    "        # Algerian number with leading 0\n",
    "        stripped_num = num_str[1:]\n",
    "    else:\n",
    "        stripped_num = num_str\n",
    "    \n",
    "    # Now determine type based on number format\n",
    "    \n",
    "    # VoIP numbers (format 09x)\n",
    "    if stripped_num.startswith('9') and len(stripped_num) == 8:\n",
    "        return \"VoIP\"\n",
    "    \n",
    "    # Mobile numbers (format 0yxx - 9 digits including leading 0)\n",
    "    if len(stripped_num) == 8:  # 9 digits with the leading 0\n",
    "        prefix = stripped_num[0:2]\n",
    "        if prefix in ['55', '56']:\n",
    "            return \"Mobile-Ooredoo\"\n",
    "        elif prefix in ['66', '67']:\n",
    "            return \"Mobile-Mobilis\" \n",
    "        elif prefix in ['77', '78']:\n",
    "            return \"Mobile-Djezzy\"\n",
    "    \n",
    "    # Fixed lines (format 0yy - 8 digits including leading 0)\n",
    "    if len(stripped_num) == 7:  # 8 digits with the leading 0\n",
    "        prefix = stripped_num[0:2]\n",
    "        if prefix in [p[1:3] for p in adsl_prefixes]:\n",
    "            return \"ADSL\"\n",
    "        elif prefix in [p[1:3] for p in ftth_prefixes]:\n",
    "            return \"FTTH\"\n",
    "    \n",
    "    return \"Other\"\n",
    "\n",
    "# Determine wilaya based on phone number prefix\n",
    "def get_wilaya_from_number(phone_number):\n",
    "    if phone_number is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Convert to string and ensure it's properly formatted\n",
    "    try:\n",
    "        num_str = str(int(phone_number))\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # For fixed lines in Algeria (format 0yy)\n",
    "    if len(num_str) >= 8:\n",
    "        # Standardize format\n",
    "        if num_str.startswith('0'):\n",
    "            prefix = num_str[1:3]  # Get area code after leading 0\n",
    "        elif num_str.startswith('213'):\n",
    "            prefix = num_str[3:5]  # Get area code after 213 country code\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Look up wilaya based on prefix\n",
    "        for wilaya, prefixes in wilaya_prefix_map.items():\n",
    "            for p in prefixes:\n",
    "                if p.endswith(prefix):\n",
    "                    return wilaya\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "# Get operator based on mobile number\n",
    "def get_operator(phone_number):\n",
    "    if phone_number is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Convert to string and ensure it's properly formatted\n",
    "    try:\n",
    "        num_str = str(int(phone_number))\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # For mobile numbers in Algeria (format 0yxx)\n",
    "    if len(num_str) >= 9:  # Mobile numbers have 9 digits\n",
    "        # Standardize format to get the operator prefix\n",
    "        if num_str.startswith('0'):\n",
    "            prefix = num_str[0:3]  # First 3 digits including leading 0\n",
    "        elif num_str.startswith('213'):\n",
    "            prefix = '0' + num_str[3:5]  # Convert to local format\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Check operator prefixes\n",
    "        for operator, prefixes in mobile_operator_prefixes.items():\n",
    "            for p in prefixes:\n",
    "                if prefix.startswith('0' + p):\n",
    "                    return operator\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "# Map call type to descriptive name\n",
    "def map_call_type(call_type_id):\n",
    "    if call_type_id is None:\n",
    "        return \"Unknown\"\n",
    "    return call_types.get(int(call_type_id), \"Other\")\n",
    "\n",
    "# Register UDFs\n",
    "line_type_udf = udf(get_line_type, StringType())\n",
    "wilaya_udf = udf(get_wilaya_from_number, StringType())\n",
    "operator_udf = udf(get_operator, StringType())\n",
    "call_type_udf = udf(map_call_type, StringType())\n",
    "\n",
    "print(\"Transforming data...\")\n",
    "\n",
    "# Select and transform essential columns\n",
    "clean_df = cdr_df.select(\n",
    "    col(\"CDR_ID\").alias(\"record_id\"),\n",
    "    convert_epoch_udf(col(\"START_DATE\")).alias(\"call_start_time\"),\n",
    "    convert_epoch_udf(col(\"END_DATE\")).alias(\"call_end_time\"),\n",
    "    col(\"CallingPartyNumber\").alias(\"source_number\"),\n",
    "    col(\"CalledPartyNumber\").alias(\"destination_number\"),\n",
    "    col(\"ACTUAL_USAGE\").alias(\"duration_seconds\"),\n",
    "    round(col(\"ACTUAL_USAGE\")/60, 2).alias(\"duration_minutes\"),\n",
    "    col(\"CallType\").alias(\"call_type_id\"),\n",
    "    call_type_udf(col(\"CallType\")).alias(\"call_type\"),\n",
    "    col(\"RoamState\").alias(\"roaming_status\"),\n",
    "    col(\"DEBIT_AMOUNT\").alias(\"charge_amount\"),\n",
    "    col(\"CallingCellID\").alias(\"source_cell_id\"),\n",
    "    col(\"TerminationReason\").alias(\"termination_reason\"),\n",
    "    col(\"IMEI\").alias(\"device_id\"),\n",
    "    col(\"ServiceType\").alias(\"service_type\"),\n",
    "    col(\"PayType\").alias(\"payment_type\"),\n",
    "    col(\"CallingHomeCountryCode\").alias(\"source_country_code\"),\n",
    "    col(\"CalledHomeCountryCode\").alias(\"destination_country_code\"),\n",
    "    col(\"CallingHomeAreaNumber\").alias(\"source_area_code\"),\n",
    "    col(\"CalledHomeAreaNumber\").alias(\"destination_area_code\"),\n",
    "    col(\"MSCAddress\").alias(\"switch_address\"),\n",
    "    col(\"BrandID\").alias(\"brand_id\"),\n",
    "    col(\"MainOfferingID\").alias(\"offering_id\")\n",
    ")\n",
    "\n",
    "print(\"Enriching data with date components and call success flag...\")\n",
    "\n",
    "# Extract date components from the formatted timestamp\n",
    "enriched_df = clean_df.withColumn(\n",
    "    \"call_date\", to_date(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
    ").withColumn(\n",
    "    \"call_day\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"d\")\n",
    ").withColumn(\n",
    "    \"call_month\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"M\")\n",
    ").withColumn(\n",
    "    \"call_year\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"yyyy\")\n",
    ").withColumn(\n",
    "    \"call_hour\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"H\")\n",
    ").withColumn(\n",
    "    \"call_day_of_week\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"EEEE\")\n",
    ").withColumn(\n",
    "    \"call_success\", when(col(\"termination_reason\") == 0, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Adding Algerian telecom-specific enrichment...\")\n",
    "\n",
    "# Add enrichment data specific to Algerian telecom\n",
    "final_df = enriched_df.withColumn(\n",
    "    \"source_line_type\", line_type_udf(col(\"source_number\"))\n",
    ").withColumn(\n",
    "    \"destination_line_type\", line_type_udf(col(\"destination_number\"))\n",
    ").withColumn(\n",
    "    \"source_wilaya\", wilaya_udf(col(\"source_number\"))\n",
    ").withColumn(\n",
    "    \"destination_wilaya\", wilaya_udf(col(\"destination_number\"))\n",
    ").withColumn(\n",
    "    \"source_operator\", operator_udf(col(\"source_number\"))\n",
    ").withColumn(\n",
    "    \"destination_operator\", operator_udf(col(\"destination_number\"))\n",
    ")\n",
    "\n",
    "print(\"Removing duplicates and invalid records...\")\n",
    "\n",
    "# Remove duplicates and invalid records\n",
    "valid_df = final_df.dropDuplicates([\"record_id\"]).filter(\n",
    "    (col(\"call_start_time\").isNotNull()) & \n",
    "    (col(\"source_number\").isNotNull()) &\n",
    "    (col(\"duration_seconds\") > 0)\n",
    ")\n",
    "\n",
    "# Print analytics about the data quality\n",
    "print(f\"Initial record count: {cdr_df.count()}\")\n",
    "print(f\"Final valid record count: {valid_df.count()}\")\n",
    "print(f\"Removed {cdr_df.count() - valid_df.count()} invalid or duplicate records\")\n",
    "\n",
    "# Data quality report\n",
    "print(\"\\nData Quality Report:\")\n",
    "total_records = valid_df.count()\n",
    "\n",
    "# Check phone number quality\n",
    "source_line_types = valid_df.groupBy(\"source_line_type\").count().withColumn(\n",
    "    \"percentage\", round(col(\"count\") * 100 / total_records, 2)\n",
    ").orderBy(desc(\"count\"))\n",
    "\n",
    "print(\"\\nSource Line Type Distribution:\")\n",
    "source_line_types.show(truncate=False)\n",
    "\n",
    "# Check wilaya distribution\n",
    "wilaya_dist = valid_df.groupBy(\"source_wilaya\").count().withColumn(\n",
    "    \"percentage\", round(col(\"count\") * 100 / total_records, 2)\n",
    ").orderBy(desc(\"count\"))\n",
    "\n",
    "print(\"\\nSource Wilaya Distribution:\")\n",
    "wilaya_dist.show(truncate=False)\n",
    "\n",
    "# Check operator distribution\n",
    "operator_dist = valid_df.groupBy(\"source_operator\").count().withColumn(\n",
    "    \"percentage\", round(col(\"count\") * 100 / total_records, 2)\n",
    ").orderBy(desc(\"count\"))\n",
    "\n",
    "print(\"\\nSource Operator Distribution:\")\n",
    "operator_dist.show(truncate=False)\n",
    "\n",
    "print(\"\\nSaving the cleaned and enriched data...\")\n",
    "\n",
    "# Save the cleaned and enriched data\n",
    "valid_df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/cdr/cleaned_data\")\n",
    "\n",
    "print(\"\\nData cleaning and transformation complete!\")\n",
    "print(f\"Cleaned data saved to: hdfs://namenode:9000/cdr/cleaned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ba4b9ad-51b6-4078-afb3-f8155412b3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25/04/19 01:46:04 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "Spark session initialized.\n",
      "Spark version: 3.3.4\n",
      "\n",
      "Spark configuration:\n",
      "  spark.executor.memory: 2g\n",
      "  spark.app.submitTime: 1745023947046\n",
      "  spark.driver.host: 9b90dc4a8722\n",
      "  spark.hadoop.fs.defaultFS: hdfs://namenode:9000\n",
      "  spark.executor.id: driver\n",
      "  spark.app.name: CDR-Analysis\n",
      "  spark.app.id: app-20250419005228-0000\n",
      "  spark.app.startTime: 1745023947158\n",
      "  spark.driver.extraJavaOptions: -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "  spark.rdd.compress: True\n",
      "  spark.driver.port: 44123\n",
      "  spark.master: spark://spark-master:7077\n",
      "  spark.driver.memory: 2g\n",
      "  spark.serializer.objectStreamReset: 100\n",
      "  spark.submit.pyFiles: \n",
      "  spark.submit.deployMode: client\n",
      "  spark.executor.extraJavaOptions: -XX:+IgnoreUnrecognizedVMOptions --add-opens=java.base/java.lang=ALL-UNNAMED --add-opens=java.base/java.lang.invoke=ALL-UNNAMED --add-opens=java.base/java.lang.reflect=ALL-UNNAMED --add-opens=java.base/java.io=ALL-UNNAMED --add-opens=java.base/java.net=ALL-UNNAMED --add-opens=java.base/java.nio=ALL-UNNAMED --add-opens=java.base/java.util=ALL-UNNAMED --add-opens=java.base/java.util.concurrent=ALL-UNNAMED --add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED --add-opens=java.base/jdk.internal.ref=ALL-UNNAMED --add-opens=java.base/sun.nio.ch=ALL-UNNAMED --add-opens=java.base/sun.nio.cs=ALL-UNNAMED --add-opens=java.base/sun.security.action=ALL-UNNAMED --add-opens=java.base/sun.util.calendar=ALL-UNNAMED --add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED\n",
      "  spark.sql.warehouse.dir: file:/home/jovyan/work/spark-warehouse\n",
      "  spark.ui.showConsoleProgress: true\n",
      "Loading CDR data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 89911 CDR records.\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o1330.get.\n: java.util.NoSuchElementException: spark.sql.execution.pythonUDF.arrow.enabled\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:1678)\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:4587)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:4587)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:72)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 100\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# Register UDF\u001b[39;00m\n\u001b[0;32m--> 100\u001b[0m convert_epoch_udf \u001b[38;5;241m=\u001b[39m \u001b[43mudf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert_epoch_to_datetime\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mStringType\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;66;03m# Determine the line type based on Algerian phone number format\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_line_type\u001b[39m(phone_number):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/functions.py:15767\u001b[0m, in \u001b[0;36mudf\u001b[0;34m(f, returnType, useArrow)\u001b[0m\n\u001b[1;32m  15761\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m  15762\u001b[0m         _create_py_udf,\n\u001b[1;32m  15763\u001b[0m         returnType\u001b[38;5;241m=\u001b[39mreturn_type,\n\u001b[1;32m  15764\u001b[0m         useArrow\u001b[38;5;241m=\u001b[39museArrow,\n\u001b[1;32m  15765\u001b[0m     )\n\u001b[1;32m  15766\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m> 15767\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_create_py_udf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturnType\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturnType\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43museArrow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43museArrow\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/udf.py:127\u001b[0m, in \u001b[0;36m_create_py_udf\u001b[0;34m(f, returnType, useArrow)\u001b[0m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[1;32m    123\u001b[0m     session \u001b[38;5;241m=\u001b[39m SparkSession\u001b[38;5;241m.\u001b[39m_instantiatedSession\n\u001b[1;32m    124\u001b[0m     is_arrow_enabled \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    125\u001b[0m         \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    126\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 127\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mspark.sql.execution.pythonUDF.arrow.enabled\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrue\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    128\u001b[0m     )\n\u001b[1;32m    129\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    130\u001b[0m     is_arrow_enabled \u001b[38;5;241m=\u001b[39m useArrow\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/sql/conf.py:54\u001b[0m, in \u001b[0;36mRuntimeConfig.get\u001b[0;34m(self, key, default)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkType(key, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m _NoValue:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jconf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m default \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1330.get.\n: java.util.NoSuchElementException: spark.sql.execution.pythonUDF.arrow.enabled\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.noSuchElementExceptionError(QueryExecutionErrors.scala:1678)\n\tat org.apache.spark.sql.internal.SQLConf.$anonfun$getConfString$3(SQLConf.scala:4587)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.SQLConf.getConfString(SQLConf.scala:4587)\n\tat org.apache.spark.sql.RuntimeConfig.get(RuntimeConfig.scala:72)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import datetime\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Create Spark session with HDFS configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CDR-Analysis\") \\\n",
    "    .config(\"spark.master\", \"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session initialized.\")\n",
    "\n",
    "# Print Spark configuration for troubleshooting\n",
    "print(\"Spark version:\", spark.version)\n",
    "print(\"\\nSpark configuration:\")\n",
    "for item in spark.sparkContext.getConf().getAll():\n",
    "    print(f\"  {item[0]}: {item[1]}\")\n",
    "\n",
    "\n",
    "# Define constants for Algerian telecom data\n",
    "# Wilaya prefix mapping with format validation\n",
    "wilaya_prefix_map = {\n",
    "    'Algiers': ['021', '023', '044'],  # First prefix is for ADSL, second is for FTTH\n",
    "    'Oran': ['041', '042'],\n",
    "    'Constantine': ['031', '043'],\n",
    "    'Annaba': ['038', '045'],\n",
    "    'Bejaia': ['034', '046'],\n",
    "    'Tizi Ouzou': ['026', '047'],\n",
    "    'Blida': ['025', '048'],\n",
    "    'Setif': ['036', '049'],\n",
    "    'Batna': ['033', '039'],\n",
    "    'Tlemcen': ['043', '032'],\n",
    "    'Mostaganem': ['045', '037'],\n",
    "    'Boumerdas': ['024', '050'],\n",
    "}\n",
    "\n",
    "# Mobile operator prefixes - ensuring format 0yxx\n",
    "mobile_operator_prefixes = {\n",
    "    'Ooredoo': ['055', '056'],  # 0550, 0560 format\n",
    "    'Mobilis': ['066', '067'],  # 0660, 0661 format\n",
    "    'Djezzy': ['077', '078'],   # 0770, 0771, 0772 format\n",
    "}\n",
    "\n",
    "# ADSL and FTTH prefixes - ensuring landline format 0yy\n",
    "adsl_prefixes = [\"021\", \"023\", \"041\", \"031\", \"038\", \"034\", \"026\", \"025\", \"036\", \"033\", \"043\", \"045\"]\n",
    "ftth_prefixes = [\"044\", \"042\", \"043\", \"045\", \"046\", \"047\", \"048\", \"049\", \"032\", \"037\", \"039\", \"050\"]\n",
    "\n",
    "# VoIP prefixes - format 09x\n",
    "voip_prefixes = [\"09\"]\n",
    "\n",
    "# Emergency numbers\n",
    "emergency_numbers = {\n",
    "    \"Police\": \"1548\",\n",
    "    \"Fire\": \"14\",\n",
    "    \"Gendarme\": \"1055\",\n",
    "    \"Coast Guard\": \"1054\",\n",
    "}\n",
    "\n",
    "# Call types\n",
    "call_types = {\n",
    "    1: \"VOICE\",\n",
    "    2: \"SMS\",\n",
    "    3: \"DATA\",\n",
    "    4: \"MMS\",\n",
    "    5: \"VOICE_ROAMING\",\n",
    "    6: \"SMS_ROAMING\",\n",
    "    7: \"DATA_ROAMING\",\n",
    "    8: \"MMS_ROAMING\"\n",
    "}\n",
    "print(\"Loading CDR data...\")\n",
    "\n",
    "# Load the CDR data\n",
    "cdr_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(f\"hdfs://namenode:9000{hdfs_source_path}/*csv\")\n",
    "\n",
    "print(f\"Loaded {cdr_df.count()} CDR records.\")\n",
    "\n",
    "# Function to convert epoch milliseconds to datetime with proper format\n",
    "def convert_epoch_to_datetime(epoch_time):\n",
    "    try:\n",
    "        if epoch_time is not None and epoch_time > 0:\n",
    "            dt = datetime.datetime.fromtimestamp(epoch_time/1000)\n",
    "            # Format as DD/MM/YY HH:MM:SS\n",
    "            return dt.strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"Error converting epoch time: {e}\")\n",
    "        return None\n",
    "\n",
    "# Register UDF\n",
    "convert_epoch_udf = udf(convert_epoch_to_datetime, StringType())\n",
    "\n",
    "# Determine the line type based on Algerian phone number format\n",
    "def get_line_type(phone_number):\n",
    "    if phone_number is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Convert to string and ensure it's properly formatted\n",
    "    try:\n",
    "        num_str = str(int(phone_number))\n",
    "    except:\n",
    "        return \"Invalid\"\n",
    "    \n",
    "    # Check for emergency numbers\n",
    "    for service, number in emergency_numbers.items():\n",
    "        if num_str.endswith(number):\n",
    "            return f\"Emergency-{service}\"\n",
    "    \n",
    "    # Standardize format for analysis\n",
    "    if num_str.startswith('00'):\n",
    "        # International format with 00 prefix\n",
    "        return \"International\"\n",
    "    elif num_str.startswith('213'):\n",
    "        # Algerian number with country code\n",
    "        stripped_num = num_str[3:]\n",
    "    elif num_str.startswith('0'):\n",
    "        # Algerian number with leading 0\n",
    "        stripped_num = num_str[1:]\n",
    "    else:\n",
    "        stripped_num = num_str\n",
    "    \n",
    "    # Now determine type based on number format\n",
    "    \n",
    "    # VoIP numbers (format 09x)\n",
    "    if stripped_num.startswith('9') and len(stripped_num) == 8:\n",
    "        return \"VoIP\"\n",
    "    \n",
    "    # Mobile numbers (format 0yxx - 9 digits including leading 0)\n",
    "    if len(stripped_num) == 8:  # 9 digits with the leading 0\n",
    "        prefix = stripped_num[0:2]\n",
    "        if prefix in ['55', '56']:\n",
    "            return \"Mobile-Ooredoo\"\n",
    "        elif prefix in ['66', '67']:\n",
    "            return \"Mobile-Mobilis\" \n",
    "        elif prefix in ['77', '78']:\n",
    "            return \"Mobile-Djezzy\"\n",
    "    \n",
    "    # Fixed lines (format 0yy - 8 digits including leading 0)\n",
    "    if len(stripped_num) == 7:  # 8 digits with the leading 0\n",
    "        prefix = stripped_num[0:2]\n",
    "        if prefix in [p[1:3] for p in adsl_prefixes]:\n",
    "            return \"ADSL\"\n",
    "        elif prefix in [p[1:3] for p in ftth_prefixes]:\n",
    "            return \"FTTH\"\n",
    "    \n",
    "    return \"Other\"\n",
    "\n",
    "# Determine wilaya based on phone number prefix\n",
    "def get_wilaya_from_number(phone_number):\n",
    "    if phone_number is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Convert to string and ensure it's properly formatted\n",
    "    try:\n",
    "        num_str = str(int(phone_number))\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # For fixed lines in Algeria (format 0yy)\n",
    "    if len(num_str) >= 8:\n",
    "        # Standardize format\n",
    "        if num_str.startswith('0'):\n",
    "            prefix = num_str[1:3]  # Get area code after leading 0\n",
    "        elif num_str.startswith('213'):\n",
    "            prefix = num_str[3:5]  # Get area code after 213 country code\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Look up wilaya based on prefix\n",
    "        for wilaya, prefixes in wilaya_prefix_map.items():\n",
    "            for p in prefixes:\n",
    "                if p.endswith(prefix):\n",
    "                    return wilaya\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "# Get operator based on mobile number\n",
    "def get_operator(phone_number):\n",
    "    if phone_number is None:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # Convert to string and ensure it's properly formatted\n",
    "    try:\n",
    "        num_str = str(int(phone_number))\n",
    "    except:\n",
    "        return \"Unknown\"\n",
    "    \n",
    "    # For mobile numbers in Algeria (format 0yxx)\n",
    "    if len(num_str) >= 9:  # Mobile numbers have 9 digits\n",
    "        # Standardize format to get the operator prefix\n",
    "        if num_str.startswith('0'):\n",
    "            prefix = num_str[0:3]  # First 3 digits including leading 0\n",
    "        elif num_str.startswith('213'):\n",
    "            prefix = '0' + num_str[3:5]  # Convert to local format\n",
    "        else:\n",
    "            return \"Unknown\"\n",
    "        \n",
    "        # Check operator prefixes\n",
    "        for operator, prefixes in mobile_operator_prefixes.items():\n",
    "            for p in prefixes:\n",
    "                if prefix.startswith('0' + p):\n",
    "                    return operator\n",
    "    \n",
    "    return \"Unknown\"\n",
    "\n",
    "# Map call type to descriptive name\n",
    "def map_call_type(call_type_id):\n",
    "    if call_type_id is None:\n",
    "        return \"Unknown\"\n",
    "    return call_types.get(int(call_type_id), \"Other\")\n",
    "\n",
    "# Register UDFs\n",
    "line_type_udf = udf(get_line_type, StringType())\n",
    "wilaya_udf = udf(get_wilaya_from_number, StringType())\n",
    "operator_udf = udf(get_operator, StringType())\n",
    "call_type_udf = udf(map_call_type, StringType())\n",
    "\n",
    "print(\"Transforming data...\")\n",
    "\n",
    "# Select and transform essential columns\n",
    "clean_df = cdr_df.select(\n",
    "    col(\"CDR_ID\").alias(\"record_id\"),\n",
    "    convert_epoch_udf(col(\"START_DATE\")).alias(\"call_start_time\"),\n",
    "    convert_epoch_udf(col(\"END_DATE\")).alias(\"call_end_time\"),\n",
    "    col(\"CallingPartyNumber\").alias(\"source_number\"),\n",
    "    col(\"CalledPartyNumber\").alias(\"destination_number\"),\n",
    "    col(\"ACTUAL_USAGE\").alias(\"duration_seconds\"),\n",
    "    round(col(\"ACTUAL_USAGE\")/60, 2).alias(\"duration_minutes\"),\n",
    "    col(\"CallType\").alias(\"call_type_id\"),\n",
    "    call_type_udf(col(\"CallType\")).alias(\"call_type\"),\n",
    "    col(\"RoamState\").alias(\"roaming_status\"),\n",
    "    col(\"DEBIT_AMOUNT\").alias(\"charge_amount\"),\n",
    "    col(\"CallingCellID\").alias(\"source_cell_id\"),\n",
    "    col(\"TerminationReason\").alias(\"termination_reason\"),\n",
    "    col(\"IMEI\").alias(\"device_id\"),\n",
    "    col(\"ServiceType\").alias(\"service_type\"),\n",
    "    col(\"PayType\").alias(\"payment_type\"),\n",
    "    col(\"CallingHomeCountryCode\").alias(\"source_country_code\"),\n",
    "    col(\"CalledHomeCountryCode\").alias(\"destination_country_code\"),\n",
    "    col(\"CallingHomeAreaNumber\").alias(\"source_area_code\"),\n",
    "    col(\"CalledHomeAreaNumber\").alias(\"destination_area_code\"),\n",
    "    col(\"MSCAddress\").alias(\"switch_address\"),\n",
    "    col(\"BrandID\").alias(\"brand_id\"),\n",
    "    col(\"MainOfferingID\").alias(\"offering_id\")\n",
    ")\n",
    "\n",
    "print(\"Enriching data with date components and call success flag...\")\n",
    "\n",
    "# Extract date components from the formatted timestamp\n",
    "enriched_df = clean_df.withColumn(\n",
    "    \"call_date\", to_date(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"))\n",
    ").withColumn(\n",
    "    \"call_day\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"d\")\n",
    ").withColumn(\n",
    "    \"call_month\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"M\")\n",
    ").withColumn(\n",
    "    \"call_year\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"yyyy\")\n",
    ").withColumn(\n",
    "    \"call_hour\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"H\")\n",
    ").withColumn(\n",
    "    \"call_day_of_week\", date_format(to_timestamp(col(\"call_start_time\"), \"dd/MM/yyyy HH:mm:ss\"), \"EEEE\")\n",
    ").withColumn(\n",
    "    \"call_success\", when(col(\"termination_reason\") == 0, 1).otherwise(0)\n",
    ")\n",
    "\n",
    "print(\"Adding Algerian telecom-specific enrichment...\")\n",
    "\n",
    "# Add enrichment data specific to Algerian telecom\n",
    "final_df = enriched_df.withColumn(\n",
    "    \"source_line_type\", line_type_udf(col(\"source_number\"))\n",
    ").withColumn(\n",
    "    \"destination_line_type\", line_type_udf(col(\"destination_number\"))\n",
    ").withColumn(\n",
    "    \"source_wilaya\", wilaya_udf(col(\"source_number\"))\n",
    ").withColumn(\n",
    "    \"destination_wilaya\", wilaya_udf(col(\"destination_number\"))\n",
    ").withColumn(\n",
    "    \"source_operator\", operator_udf(col(\"source_number\"))\n",
    ").withColumn(\n",
    "    \"destination_operator\", operator_udf(col(\"destination_number\"))\n",
    ")\n",
    "\n",
    "print(\"Removing duplicates and invalid records...\")\n",
    "\n",
    "# Remove duplicates and invalid records\n",
    "valid_df = final_df.dropDuplicates([\"record_id\"]).filter(\n",
    "    (col(\"call_start_time\").isNotNull()) & \n",
    "    (col(\"source_number\").isNotNull()) &\n",
    "    (col(\"duration_seconds\") > 0)\n",
    ")\n",
    "\n",
    "# Print analytics about the data quality\n",
    "print(f\"Initial record count: {cdr_df.count()}\")\n",
    "print(f\"Final valid record count: {valid_df.count()}\")\n",
    "print(f\"Removed {cdr_df.count() - valid_df.count()} invalid or duplicate records\")\n",
    "\n",
    "# Data quality report\n",
    "print(\"\\nData Quality Report:\")\n",
    "total_records = valid_df.count()\n",
    "\n",
    "# Check phone number quality\n",
    "source_line_types = valid_df.groupBy(\"source_line_type\").count().withColumn(\n",
    "    \"percentage\", round(col(\"count\") * 100 / total_records, 2)\n",
    ").orderBy(desc(\"count\"))\n",
    "\n",
    "print(\"\\nSource Line Type Distribution:\")\n",
    "source_line_types.show(truncate=False)\n",
    "\n",
    "# Check wilaya distribution\n",
    "wilaya_dist = valid_df.groupBy(\"source_wilaya\").count().withColumn(\n",
    "    \"percentage\", round(col(\"count\") * 100 / total_records, 2)\n",
    ").orderBy(desc(\"count\"))\n",
    "\n",
    "print(\"\\nSource Wilaya Distribution:\")\n",
    "wilaya_dist.show(truncate=False)\n",
    "\n",
    "# Check operator distribution\n",
    "operator_dist = valid_df.groupBy(\"source_operator\").count().withColumn(\n",
    "    \"percentage\", round(col(\"count\") * 100 / total_records, 2)\n",
    ").orderBy(desc(\"count\"))\n",
    "\n",
    "print(\"\\nSource Operator Distribution:\")\n",
    "operator_dist.show(truncate=False)\n",
    "\n",
    "print(\"\\nSaving the cleaned and enriched data...\")\n",
    "\n",
    "# Save the cleaned and enriched data\n",
    "valid_df.write.mode(\"overwrite\").parquet(\"hdfs://namenode:9000/cdr/cleaned_data\")\n",
    "\n",
    "print(\"\\nData cleaning and transformation complete!\")\n",
    "print(f\"Cleaned data saved to: hdfs://namenode:9000/cdr/cleaned_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "758121b5-293b-4306-b511-d8f346c99428",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/16 21:39:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ SparkSession initialized (App: CDR Anonymization in Notebook, Spark: 3.5.1)\n"
     ]
    },
    {
     "ename": "IllegalArgumentException",
     "evalue": "Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\n\nJVM stacktrace:\njava.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1322)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:185)\n\tat org.apache.spark.sql.SparkSession.conf(SparkSession.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NumberFormatException: Size must be specified as bytes (b), kibibytes (k), mebibytes (m), gibibytes (g), tebibytes (t), or pebibytes(p). E.g. 50b, 100k, or 250m.\nFailed to parse byte string: 268435456  # 256MB\n\tat org.apache.spark.network.util.JavaUtils.byteStringAs(JavaUtils.java:312)\n\tat org.apache.spark.internal.config.ConfigHelpers$.byteFromString(ConfigBuilder.scala:67)\n\tat org.apache.spark.internal.config.ConfigBuilder.$anonfun$bytesConf$1(ConfigBuilder.scala:261)\n\tat org.apache.spark.internal.config.ConfigBuilder.$anonfun$bytesConf$1$adapted(ConfigBuilder.scala:261)\n\tat org.apache.spark.sql.internal.SQLConf.setConfString(SQLConf.scala:5253)\n\tat org.apache.spark.sql.internal.SQLConf$.$anonfun$mergeSparkConf$1(SQLConf.scala:135)\n\tat org.apache.spark.sql.internal.SQLConf$.$anonfun$mergeSparkConf$1$adapted(SQLConf.scala:134)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.internal.SQLConf$.mergeSparkConf(SQLConf.scala:134)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$conf$2(BaseSessionStateBuilder.scala:94)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.conf$lzycompute(BaseSessionStateBuilder.scala:92)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.conf(BaseSessionStateBuilder.scala:85)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:367)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1319)\n\t... 18 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/work/spark-apps\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspark_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_spark\n\u001b[0;32m----> 6\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43minit_spark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCDR Anonymization in Notebook\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/work/work/spark-apps/spark_init.py:22\u001b[0m, in \u001b[0;36minit_spark\u001b[0;34m(app_name)\u001b[0m\n\u001b[1;32m     20\u001b[0m spark\u001b[38;5;241m.\u001b[39msparkContext\u001b[38;5;241m.\u001b[39msetCheckpointDir(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:9000/user/spark/checkpoints\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ SparkSession initialized (App: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mapp_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Spark: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39mversion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Hive Warehouse: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.sql.warehouse.dir\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Hive Metastore URI: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspark\u001b[38;5;241m.\u001b[39mconf\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspark.hadoop.hive.metastore.uris\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m spark\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:784\u001b[0m, in \u001b[0;36mSparkSession.conf\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    757\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Runtime configuration interface for Spark.\u001b[39;00m\n\u001b[1;32m    758\u001b[0m \n\u001b[1;32m    759\u001b[0m \u001b[38;5;124;03mThis is the interface through which the user can get and set all Spark and Hadoop\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    781\u001b[0m \u001b[38;5;124;03m'value'\u001b[39;00m\n\u001b[1;32m    782\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_conf\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 784\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf \u001b[38;5;241m=\u001b[39m RuntimeConfig(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_conf\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mIllegalArgumentException\u001b[0m: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\n\nJVM stacktrace:\njava.lang.IllegalArgumentException: Error while instantiating 'org.apache.spark.sql.hive.HiveSessionStateBuilder':\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1322)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sessionState$2(SparkSession.scala:162)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession.sessionState$lzycompute(SparkSession.scala:160)\n\tat org.apache.spark.sql.SparkSession.sessionState(SparkSession.scala:157)\n\tat org.apache.spark.sql.SparkSession.conf$lzycompute(SparkSession.scala:185)\n\tat org.apache.spark.sql.SparkSession.conf(SparkSession.scala:185)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.lang.NumberFormatException: Size must be specified as bytes (b), kibibytes (k), mebibytes (m), gibibytes (g), tebibytes (t), or pebibytes(p). E.g. 50b, 100k, or 250m.\nFailed to parse byte string: 268435456  # 256MB\n\tat org.apache.spark.network.util.JavaUtils.byteStringAs(JavaUtils.java:312)\n\tat org.apache.spark.internal.config.ConfigHelpers$.byteFromString(ConfigBuilder.scala:67)\n\tat org.apache.spark.internal.config.ConfigBuilder.$anonfun$bytesConf$1(ConfigBuilder.scala:261)\n\tat org.apache.spark.internal.config.ConfigBuilder.$anonfun$bytesConf$1$adapted(ConfigBuilder.scala:261)\n\tat org.apache.spark.sql.internal.SQLConf.setConfString(SQLConf.scala:5253)\n\tat org.apache.spark.sql.internal.SQLConf$.$anonfun$mergeSparkConf$1(SQLConf.scala:135)\n\tat org.apache.spark.sql.internal.SQLConf$.$anonfun$mergeSparkConf$1$adapted(SQLConf.scala:134)\n\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n\tat org.apache.spark.sql.internal.SQLConf$.mergeSparkConf(SQLConf.scala:134)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.$anonfun$conf$2(BaseSessionStateBuilder.scala:94)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.conf$lzycompute(BaseSessionStateBuilder.scala:92)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.conf(BaseSessionStateBuilder.scala:85)\n\tat org.apache.spark.sql.internal.BaseSessionStateBuilder.build(BaseSessionStateBuilder.scala:367)\n\tat org.apache.spark.sql.SparkSession$.org$apache$spark$sql$SparkSession$$instantiateSessionState(SparkSession.scala:1319)\n\t... 18 more\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work/spark-apps')\n",
    "\n",
    "from spark_init import init_spark\n",
    "\n",
    "spark = init_spark(\"CDR Anonymization in Notebook\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5c9026c-d055-42dd-9af6-651e915e9354",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spark_init'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01manonymize_cdr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m anonymize_cdr\n\u001b[1;32m      3\u001b[0m input_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:9000/user/hive/warehouse/raw/*.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhdfs://namenode:9000/user/hive/warehouse/anonymized/\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/work/work/spark-apps/anonymize_cdr.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Import Spark init from your central script\u001b[39;00m\n\u001b[1;32m      8\u001b[0m sys\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/home/jovyan/work/scripts\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mspark_init\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m init_spark  \u001b[38;5;66;03m# or from spark-init import init_spark if you use underscore or dash\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhash_value\u001b[39m(value, salt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrandom_salt_2025\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'spark_init'"
     ]
    }
   ],
   "source": [
    "from anonymize_cdr import anonymize_cdr\n",
    "\n",
    "input_path = \"hdfs://namenode:9000/user/hive/warehouse/raw/*.csv\"\n",
    "output_path = \"hdfs://namenode:9000/user/hive/warehouse/anonymized/\"\n",
    "salt = \"random_salt_2025\"\n",
    "pii_columns = [\n",
    "    \"MSISDN\", \"IMSI\", \"IMEI\", \"CALLING_NBR\", \"CALLED_NBR\",\n",
    "    \"FIXED_NUMBER\", \"MOBILE_NUMBER\", \"SUBSCRIBER_ID\"\n",
    "]\n",
    "\n",
    "anonymize_cdr(input_path, output_path, salt, pii_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b026b-acf8-4b34-af6a-3605552a7830",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
