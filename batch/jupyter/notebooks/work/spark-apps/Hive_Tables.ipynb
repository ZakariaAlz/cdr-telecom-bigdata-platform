{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71ead42-02c2-46f7-b661-14baa4ad4f2c",
   "metadata": {},
   "source": [
    "# 02. Create Hive Tables ‚Äì CDR Data Management"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe888f8-4aab-4ed3-abf0-d39c8136f286",
   "metadata": {},
   "source": [
    "### Project: CDR Telecom Big Data Engineering Final Year Internship\n",
    "### Objective: Create robust, query-optimized Hive tables and views for anonymized CDR data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb04fdc-ef67-4f3f-908d-ff6b32655c53",
   "metadata": {},
   "source": [
    "#### Table Creation Strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0d3afa-a9e0-4b9b-b237-b9fdb2d2d42e",
   "metadata": {},
   "source": [
    "1- External Tables: Point to Parquet data in HDFS\n",
    "\n",
    "2- Partitioning: By date for scalable analytics\n",
    "\n",
    "3- Schema: Strong types, data cleanliness\n",
    "\n",
    "4- Views: For BI tools and analytics\n",
    "\n",
    "5- Data Quality: Monitoring for row count, nulls, uniqueness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b4385f33-3091-49d1-8ac3-fa11eaf9e250",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/19 06:52:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: CDR Data Engineering - Creating Hive Tables, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "üèóÔ∏è Table Creation Configuration:\n",
      "   Database: algerie_telecom_cdr\n",
      "   Main table: cdr_anonymized\n",
      "   Data location: hdfs://namenode:9000/user/hive/warehouse/cdr_anonymized/\n",
      "   Spark Version: 3.5.1\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 1 ‚Äì Setup, Imports, and Config\n",
    "# ------------------------------------------------------------\n",
    "import sys, os\n",
    "from datetime import datetime\n",
    "from pyspark.sql import functions as F, types as T\n",
    "\n",
    "# --- Spark Project Init ---\n",
    "sys.path.append('/home/jovyan/work/work/scripts')  # Path to your custom scripts\n",
    "from spark_init import init_spark\n",
    "\n",
    "spark = init_spark(\"CDR Data Engineering - Creating Hive Tables\")\n",
    "\n",
    "# HDFS and Hive Paths\n",
    "HDFS_ANON_PATH = \"/user/hive/warehouse/cdr_anonymized/\"\n",
    "DATABASE_NAME = \"algerie_telecom_cdr\"\n",
    "MAIN_TABLE = \"cdr_anonymized\"\n",
    "DAILY_TABLE = \"cdr_daily_summary\"\n",
    "HOURLY_TABLE = \"cdr_hourly_summary\"\n",
    "NETWORK_TABLE = \"cdr_network_metrics\"\n",
    "\n",
    "print(f\"üèóÔ∏è Table Creation Configuration:\")\n",
    "print(f\"   Database: {DATABASE_NAME}\")\n",
    "print(f\"   Main table: {MAIN_TABLE}\")\n",
    "print(f\"   Data location: hdfs://namenode:9000{HDFS_ANON_PATH}\")\n",
    "print(f\"   Spark Version: {spark.version}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35883187-e75d-4784-85e4-f52e01452fb4",
   "metadata": {},
   "source": [
    "#### Load and Inspect Anonymized Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5bf3e9a-b358-4420-823a-8ccc9107a5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/19 06:52:20 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Data Overview:\n",
      "   Total records: 89,911\n",
      "   Total columns: 39\n",
      "\n",
      "üìã Hash columns (anonymized): ['PRI_IDENTITY_HASH', 'CallingPartyNumber_HASH', 'CalledPartyNumber_HASH', 'CallingPartyIMSI_HASH', 'CalledPartyIMSI_HASH', 'IMEI_HASH']\n",
      "   Regular columns: ['CDR_ID', 'CDR_SUB_ID', 'CDR_TYPE', 'CDR_BATCH_ID', 'SRC_CDR_ID', 'START_DATE', 'END_DATE', 'CREATE_DATE', 'CUST_LOCAL_START_DATE', 'CUST_LOCAL_END_DATE', 'OBJ_ID', 'ACTUAL_USAGE', 'RATE_USAGE', 'SERVICE_UNIT_TYPE', 'SERVICE_CATEGORY', 'USAGE_SERVICE_TYPE', 'STD_EVT_TYPE_ID', 'SESSION_ID', 'DEBIT_AMOUNT', 'UN_DEBIT_AMOUNT', 'TOTAL_TAX', 'ServiceFlow', 'CallForwardIndicator', 'ChargingTime', 'CallType', 'RoamState', 'CallingRoamInfo', 'CalledRoamInfo', 'CallingCellID', 'CalledCellID', 'MSCAddress', 'BrandID']\n",
      "root\n",
      " |-- CDR_ID: string (nullable = true)\n",
      " |-- CDR_SUB_ID: string (nullable = true)\n",
      " |-- CDR_TYPE: string (nullable = true)\n",
      " |-- CDR_BATCH_ID: string (nullable = true)\n",
      " |-- SRC_CDR_ID: string (nullable = true)\n",
      " |-- START_DATE: string (nullable = true)\n",
      " |-- END_DATE: string (nullable = true)\n",
      " |-- CREATE_DATE: string (nullable = true)\n",
      " |-- CUST_LOCAL_START_DATE: string (nullable = true)\n",
      " |-- CUST_LOCAL_END_DATE: string (nullable = true)\n",
      " |-- OBJ_ID: string (nullable = true)\n",
      " |-- ACTUAL_USAGE: double (nullable = true)\n",
      " |-- RATE_USAGE: double (nullable = true)\n",
      " |-- SERVICE_UNIT_TYPE: string (nullable = true)\n",
      " |-- SERVICE_CATEGORY: string (nullable = true)\n",
      " |-- USAGE_SERVICE_TYPE: string (nullable = true)\n",
      " |-- STD_EVT_TYPE_ID: string (nullable = true)\n",
      " |-- SESSION_ID: string (nullable = true)\n",
      " |-- DEBIT_AMOUNT: double (nullable = true)\n",
      " |-- UN_DEBIT_AMOUNT: double (nullable = true)\n",
      " |-- TOTAL_TAX: double (nullable = true)\n",
      " |-- ServiceFlow: string (nullable = true)\n",
      " |-- CallForwardIndicator: string (nullable = true)\n",
      " |-- ChargingTime: double (nullable = true)\n",
      " |-- CallType: string (nullable = true)\n",
      " |-- RoamState: string (nullable = true)\n",
      " |-- CallingRoamInfo: string (nullable = true)\n",
      " |-- CalledRoamInfo: string (nullable = true)\n",
      " |-- CallingCellID: string (nullable = true)\n",
      " |-- CalledCellID: string (nullable = true)\n",
      " |-- MSCAddress: string (nullable = true)\n",
      " |-- BrandID: string (nullable = true)\n",
      " |-- PRI_IDENTITY_HASH: string (nullable = true)\n",
      " |-- CallingPartyNumber_HASH: string (nullable = true)\n",
      " |-- CalledPartyNumber_HASH: string (nullable = true)\n",
      " |-- CallingPartyIMSI_HASH: string (nullable = true)\n",
      " |-- CalledPartyIMSI_HASH: string (nullable = true)\n",
      " |-- IMEI_HASH: string (nullable = true)\n",
      " |-- CDR_DAY: date (nullable = true)\n",
      "\n",
      "+------------------+----------+--------+------------------+----------+--------------+--------------+--------------+---------------------+-------------------+------------------+------------+----------+-----------------+----------------+------------------+---------------+--------------------------------------------------------------+------------+---------------+---------+-----------+--------------------+------------------+--------+---------+---------------+--------------+-------------+------------+----------+-------+----------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+---------------------+--------------------+---------+----------+\n",
      "|CDR_ID            |CDR_SUB_ID|CDR_TYPE|CDR_BATCH_ID      |SRC_CDR_ID|START_DATE    |END_DATE      |CREATE_DATE   |CUST_LOCAL_START_DATE|CUST_LOCAL_END_DATE|OBJ_ID            |ACTUAL_USAGE|RATE_USAGE|SERVICE_UNIT_TYPE|SERVICE_CATEGORY|USAGE_SERVICE_TYPE|STD_EVT_TYPE_ID|SESSION_ID                                                    |DEBIT_AMOUNT|UN_DEBIT_AMOUNT|TOTAL_TAX|ServiceFlow|CallForwardIndicator|ChargingTime      |CallType|RoamState|CallingRoamInfo|CalledRoamInfo|CallingCellID|CalledCellID|MSCAddress|BrandID|PRI_IDENTITY_HASH                                               |CallingPartyNumber_HASH                                         |CalledPartyNumber_HASH                                          |CallingPartyIMSI_HASH|CalledPartyIMSI_HASH|IMEI_HASH|CDR_DAY   |\n",
      "+------------------+----------+--------+------------------+----------+--------------+--------------+--------------+---------------------+-------------------+------------------+------------+----------+-----------------+----------------+------------------+---------------+--------------------------------------------------------------+------------+---------------+---------+-----------+--------------------+------------------+--------+---------+---------------+--------------+-------------+------------+----------+-------+----------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+---------------------+--------------------+---------+----------+\n",
      "|184600000351493312|0         |N       |186200000000170208|0         |20250101100952|20250101101202|20250101110054|20250101110952       |20250101111202     |141049000008415808|130.0       |180.0     |1                |1               |10                |11001          |00186200000000170255;00001735729254199124;00184600000004601487|0.0         |0.0            |0.0      |1          |0                   |2.0250101100952E13|0       |0        |NULL           |NULL          |NULL         |NULL        |NULL      |2      |b5cc89696600b5d6f50fb0f9bb85e2ac16eb8507f810dffb91683be85e4eebf6|f2076ca9d2a337fc5824277f290d1f48dec30eb46f7e7328db2ec2a71302a24f|d61b594f283aed4e9d2bb77f39141d60e7380feb11cf7c2ba628ebdb9ac1a521|NULL                 |NULL                |NULL     |2025-01-01|\n",
      "|184600000351493312|0         |N       |186200000000170208|0         |20250101095539|20250101100230|20250101110054|20250101105539       |20250101110230     |132169000005506000|411.0       |420.0     |1                |1               |11                |11001          |00186200000000170255;00001735729254477897;00184600000004601488|6300.0      |0.0            |0.0      |1          |0                   |2.0250101095539E13|1       |0        |NULL           |NULL          |NULL         |NULL        |NULL      |2      |4a579d234c5ce7bd8422ca8475bbc171a996f48ae6ca3fbbd11eec83643eb83e|242783462e3b4d5ead05b34d0114d34327a92930f2c78ca10294269830a58f2a|a9244995d8279af53472526b1beb3e2574b0a318f518ae4a2a52cf8232744af8|NULL                 |NULL                |NULL     |2025-01-01|\n",
      "|184600000351493312|0         |N       |186200000000170208|0         |20250101102227|20250101102347|20250101110054|20250101112227       |20250101112347     |142969000008388096|80.0        |80.0      |1                |1               |11                |11001          |00186200000000170255;00001735729254697805;00184600000004601489|0.0         |0.0            |0.0      |1          |0                   |2.0250101102227E13|1       |0        |NULL           |NULL          |NULL         |NULL        |NULL      |2      |a57791c19de2ccd1b1b0a603c6ed76bb7b3ba9434d9f8d8b6381c986bca4a58b|49fd3818bd73a2ceb2239e286681e5608fa934313f67079ae97e40132ebc54ca|20fd76ee231f76873c02d836080f3f5e4aade7d4b08f479a901499e3681c17a5|NULL                 |NULL                |NULL     |2025-01-01|\n",
      "|184600000351493312|0         |N       |186200000000170208|0         |20250101102731|20250101102811|20250101110054|20250101112731       |20250101112811     |139969000007288992|40.0        |60.0      |1                |1               |10                |11001          |00186200000000170255;00001735729254708605;00184600000004601490|0.0         |0.0            |0.0      |1          |0                   |2.0250101102731E13|0       |0        |NULL           |NULL          |NULL         |NULL        |NULL      |2      |aea1ab942d00de76161786c2d63557adc1689379e546da53bb6fc8e3c647b103|9f1647d679b1d4bbd8dd80abae6b1fa2773552d2e8480c31d68c0641eb478d03|7d9276bff3e3b9016c14ca7350b6d46285087f5ee4a2810d7cb9a34a39331341|NULL                 |NULL                |NULL     |2025-01-01|\n",
      "|184600000351493312|0         |N       |186200000000170208|0         |20250101101232|20250101101553|20250101110054|20250101111232       |20250101111553     |136849000003058896|201.0       |240.0     |1                |1               |10                |11001          |00186200000000170255;00001735729254799291;00184600000004601491|0.0         |0.0            |0.0      |1          |0                   |2.0250101101232E13|0       |0        |NULL           |NULL          |NULL         |NULL        |NULL      |2      |24d6db444774fb9f2ac64bc3d0e2dae05c8898c939aff4d588f8b3d98f9bdbb4|c9002fdc6085281119fb50009b28ceca24b6525fd7213cf7d9d96f53d7a3e4ed|8a84118dc12a52f15251b49f6d497c1ff187931ef4bc34ae8215d06776ef08ca|NULL                 |NULL                |NULL     |2025-01-01|\n",
      "+------------------+----------+--------+------------------+----------+--------------+--------------+--------------+---------------------+-------------------+------------------+------------+----------+-----------------+----------------+------------------+---------------+--------------------------------------------------------------+------------+---------------+---------+-----------+--------------------+------------------+--------+---------+---------------+--------------+-------------+------------+----------+-------+----------------------------------------------------------------+----------------------------------------------------------------+----------------------------------------------------------------+---------------------+--------------------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 2 ‚Äì Load and Inspect Anonymized Data\n",
    "# ------------------------------------------------------------\n",
    "anon_df = spark.read.parquet(f\"hdfs://namenode:9000{HDFS_ANON_PATH}\")\n",
    "anon_df.cache()\n",
    "\n",
    "total_records = anon_df.count()\n",
    "total_columns = len(anon_df.columns)\n",
    "hash_columns = [c for c in anon_df.columns if c.endswith(\"_HASH\")]\n",
    "regular_columns = [c for c in anon_df.columns if not c.endswith(\"_HASH\") and c != \"CDR_DAY\"]\n",
    "\n",
    "print(f\"\\nüìä Data Overview:\")\n",
    "print(f\"   Total records: {total_records:,}\")\n",
    "print(f\"   Total columns: {total_columns}\")\n",
    "print(f\"\\nüìã Hash columns (anonymized): {hash_columns}\")\n",
    "print(f\"   Regular columns: {regular_columns}\")\n",
    "anon_df.printSchema()\n",
    "anon_df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09410b6e-00be-44ee-accb-b74388e3bef1",
   "metadata": {},
   "source": [
    "#### Create Database and Use It"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eb241191-4ccb-4a6c-a7e1-08abc2ea2861",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Database 'algerie_telecom_cdr' created and selected\n",
      "+-------------------+\n",
      "|          namespace|\n",
      "+-------------------+\n",
      "|algerie_telecom_cdr|\n",
      "|            default|\n",
      "+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 3 ‚Äì Create and Use Database\n",
    "# ------------------------------------------------------------\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {DATABASE_NAME}\n",
    "    COMMENT 'Algerie Telecom CDR Data Warehouse - Final Year Project'\n",
    "    LOCATION 'hdfs://namenode:9000/user/hive/warehouse/{DATABASE_NAME}.db'\n",
    "\"\"\")\n",
    "spark.sql(f\"USE {DATABASE_NAME}\")\n",
    "print(f\"‚úÖ Database '{DATABASE_NAME}' created and selected\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b9e5bb0-3167-4485-adb3-8c8af994a025",
   "metadata": {},
   "source": [
    "#### Analyze Columns for Schema Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0bfff57a-6878-4fee-b464-a629bcbdcee8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date columns: ['START_DATE', 'END_DATE', 'CREATE_DATE', 'CUST_LOCAL_START_DATE', 'CUST_LOCAL_END_DATE', 'CDR_DAY']\n",
      "Numeric columns: ['ACTUAL_USAGE', 'RATE_USAGE', 'DEBIT_AMOUNT', 'UN_DEBIT_AMOUNT', 'TOTAL_TAX', 'ChargingTime']\n",
      "String columns: ['CDR_ID', 'CDR_SUB_ID', 'CDR_TYPE', 'CDR_BATCH_ID', 'SRC_CDR_ID', 'OBJ_ID', 'SERVICE_UNIT_TYPE', 'SERVICE_CATEGORY', 'USAGE_SERVICE_TYPE', 'STD_EVT_TYPE_ID', 'SESSION_ID', 'ServiceFlow', 'CallForwardIndicator', 'CallType', 'RoamState', 'CallingRoamInfo', 'CalledRoamInfo', 'CallingCellID', 'CalledCellID', 'MSCAddress', 'BrandID', 'PRI_IDENTITY_HASH', 'CallingPartyNumber_HASH', 'CalledPartyNumber_HASH', 'CallingPartyIMSI_HASH', 'CalledPartyIMSI_HASH', 'IMEI_HASH']\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 4 ‚Äì Analyze Columns for Schema Design\n",
    "# ------------------------------------------------------------\n",
    "date_columns = [c for c in anon_df.columns if \"date\" in c.lower() or \"day\" in c.lower()]\n",
    "numeric_columns = [c for c, t in anon_df.dtypes if t in (\"int\", \"bigint\", \"double\", \"float\")]\n",
    "string_columns = [c for c in anon_df.columns if c not in numeric_columns and c not in date_columns]\n",
    "\n",
    "print(f\"Date columns: {date_columns}\")\n",
    "print(f\"Numeric columns: {numeric_columns}\")\n",
    "print(f\"String columns: {string_columns}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29582e-40cd-4d77-a12e-11fc1642f7ab",
   "metadata": {},
   "source": [
    "#### Create External Main Table (Parquet, Partitioned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0f52775-78b0-4024-9028-0662a2493c9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/19 06:52:37 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Main external table 'cdr_anonymized' created\n",
      "+----------------------------+----------------------------+-------+\n",
      "|col_name                    |data_type                   |comment|\n",
      "+----------------------------+----------------------------+-------+\n",
      "|CDR_ID                      |string                      |NULL   |\n",
      "|CDR_SUB_ID                  |string                      |NULL   |\n",
      "|CDR_TYPE                    |string                      |NULL   |\n",
      "|CDR_BATCH_ID                |string                      |NULL   |\n",
      "|SRC_CDR_ID                  |string                      |NULL   |\n",
      "|START_DATE                  |string                      |NULL   |\n",
      "|END_DATE                    |string                      |NULL   |\n",
      "|CREATE_DATE                 |string                      |NULL   |\n",
      "|CUST_LOCAL_START_DATE       |string                      |NULL   |\n",
      "|CUST_LOCAL_END_DATE         |string                      |NULL   |\n",
      "|OBJ_ID                      |string                      |NULL   |\n",
      "|ACTUAL_USAGE                |double                      |NULL   |\n",
      "|RATE_USAGE                  |double                      |NULL   |\n",
      "|SERVICE_UNIT_TYPE           |string                      |NULL   |\n",
      "|SERVICE_CATEGORY            |string                      |NULL   |\n",
      "|USAGE_SERVICE_TYPE          |string                      |NULL   |\n",
      "|STD_EVT_TYPE_ID             |string                      |NULL   |\n",
      "|SESSION_ID                  |string                      |NULL   |\n",
      "|DEBIT_AMOUNT                |double                      |NULL   |\n",
      "|UN_DEBIT_AMOUNT             |double                      |NULL   |\n",
      "|TOTAL_TAX                   |double                      |NULL   |\n",
      "|ServiceFlow                 |string                      |NULL   |\n",
      "|CallForwardIndicator        |string                      |NULL   |\n",
      "|ChargingTime                |double                      |NULL   |\n",
      "|CallType                    |string                      |NULL   |\n",
      "|RoamState                   |string                      |NULL   |\n",
      "|CallingRoamInfo             |string                      |NULL   |\n",
      "|CalledRoamInfo              |string                      |NULL   |\n",
      "|CallingCellID               |string                      |NULL   |\n",
      "|CalledCellID                |string                      |NULL   |\n",
      "|MSCAddress                  |string                      |NULL   |\n",
      "|BrandID                     |string                      |NULL   |\n",
      "|PRI_IDENTITY_HASH           |string                      |NULL   |\n",
      "|CallingPartyNumber_HASH     |string                      |NULL   |\n",
      "|CalledPartyNumber_HASH      |string                      |NULL   |\n",
      "|CallingPartyIMSI_HASH       |string                      |NULL   |\n",
      "|CalledPartyIMSI_HASH        |string                      |NULL   |\n",
      "|IMEI_HASH                   |string                      |NULL   |\n",
      "|CDR_DAY                     |date                        |NULL   |\n",
      "|# Partition Information     |                            |       |\n",
      "|# col_name                  |data_type                   |comment|\n",
      "|CDR_DAY                     |date                        |NULL   |\n",
      "|                            |                            |       |\n",
      "|# Detailed Table Information|                            |       |\n",
      "|Catalog                     |spark_catalog               |       |\n",
      "|Database                    |algerie_telecom_cdr         |       |\n",
      "|Table                       |cdr_anonymized              |       |\n",
      "|Owner                       |jovyan                      |       |\n",
      "|Created Time                |Thu Jun 19 06:52:37 GMT 2025|       |\n",
      "|Last Access                 |UNKNOWN                     |       |\n",
      "+----------------------------+----------------------------+-------+\n",
      "only showing top 50 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 5 ‚Äì Create Main External Table (Parquet, Partitioned)\n",
    "# ------------------------------------------------------------\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {MAIN_TABLE}\")\n",
    "\n",
    "# Partition if possible\n",
    "partition_by = \"CDR_DAY\" if \"CDR_DAY\" in anon_df.columns else None\n",
    "\n",
    "# Build CREATE TABLE columns with correct Hive types, EXCLUDING the partition column!\n",
    "col_defs = []\n",
    "for c, t in anon_df.dtypes:\n",
    "    if c == partition_by:\n",
    "        continue  # partition columns go ONLY in PARTITIONED BY\n",
    "    if t.startswith(\"int\") or t.startswith(\"bigint\"):\n",
    "        col_defs.append(f\"{c} BIGINT\")\n",
    "    elif t.startswith(\"double\") or t.startswith(\"float\"):\n",
    "        col_defs.append(f\"{c} DOUBLE\")\n",
    "    elif t.startswith(\"date\"):\n",
    "        col_defs.append(f\"{c} DATE\")\n",
    "    else:\n",
    "        col_defs.append(f\"{c} STRING\")\n",
    "\n",
    "# Compose the correct Hive SQL\n",
    "if partition_by:\n",
    "    table_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {MAIN_TABLE} (\n",
    "        {', '.join(col_defs)}\n",
    "    )\n",
    "    PARTITIONED BY ({partition_by} DATE)\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 'hdfs://namenode:9000{HDFS_ANON_PATH}'\n",
    "    TBLPROPERTIES (\n",
    "        'parquet.compression'='SNAPPY',\n",
    "        'created_by'='Algerie_Telecom_Data_Engineering',\n",
    "        'created_date'='{datetime.now().strftime('%Y-%m-%d')}'\n",
    "    )\n",
    "    \"\"\"\n",
    "else:\n",
    "    table_sql = f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS {MAIN_TABLE} (\n",
    "        {', '.join(col_defs)}\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION 'hdfs://namenode:9000{HDFS_ANON_PATH}'\n",
    "    TBLPROPERTIES (\n",
    "        'parquet.compression'='SNAPPY',\n",
    "        'created_by'='Algerie_Telecom_Data_Engineering',\n",
    "        'created_date'='{datetime.now().strftime('%Y-%m-%d')}'\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "# Run the SQL to create the table\n",
    "spark.sql(table_sql)\n",
    "print(f\"‚úÖ Main external table '{MAIN_TABLE}' created\")\n",
    "\n",
    "# Recover partitions if partitioned\n",
    "if partition_by:\n",
    "    spark.sql(f\"MSCK REPAIR TABLE {MAIN_TABLE}\")\n",
    "\n",
    "spark.sql(f\"DESCRIBE EXTENDED {MAIN_TABLE}\").show(50, False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b74abd-9ce5-4d8c-a8e4-b8389700f453",
   "metadata": {},
   "source": [
    "#### Create Aggregated Tables (Daily, Hourly, Network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bfff90ca-c466-48ea-b1f3-6650a0d950c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Daily summary table 'cdr_daily_summary' created\n",
      "+----------+-------------+------------------+------------------+--------------+\n",
      "|   CDR_DAY|total_records|unique_subscribers|      avg_duration|total_duration|\n",
      "+----------+-------------+------------------+------------------+--------------+\n",
      "|2024-12-31|         7181|              3924|105.00612728032307|      754049.0|\n",
      "|2025-01-01|        82730|             38311| 166.0054152060921|   1.3733628E7|\n",
      "+----------+-------------+------------------+------------------+--------------+\n",
      "\n",
      "‚ö†Ô∏è Skipping hourly aggregation: 'CALL_TIME' column not found.\n",
      "‚úÖ Network metrics table 'cdr_network_metrics' created\n",
      "+----------------+-----------+------------+------------------+\n",
      "|   CallingCellID|total_calls|unique_users| avg_call_duration|\n",
      "+----------------+-----------+------------+------------------+\n",
      "|603093E819668914|      18441|        9664|186.92945068054877|\n",
      "|            NULL|      71470|       31179|154.47755701693018|\n",
      "+----------------+-----------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 6 ‚Äì Create Aggregated Tables (Daily, Hourly, Network)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "main_df = spark.table(MAIN_TABLE)\n",
    "\n",
    "# --- CAST numerics for reliable aggregation ---\n",
    "# Make a copy so we don't modify main_df in memory if not needed\n",
    "agg_df = main_df\n",
    "\n",
    "duration_col = \"ACTUAL_USAGE\" if \"ACTUAL_USAGE\" in main_df.columns else None\n",
    "if duration_col and isinstance(agg_df.schema[duration_col].dataType, T.StringType):\n",
    "    agg_df = agg_df.withColumn(duration_col, F.col(duration_col).cast(\"double\"))\n",
    "\n",
    "# Identify date/hash columns again for safety\n",
    "date_col = \"CDR_DAY\" if \"CDR_DAY\" in agg_df.columns else (next((c for c in agg_df.columns if \"date\" in c.lower() or \"day\" in c.lower()), None))\n",
    "hash_col = next((c for c in agg_df.columns if c.endswith(\"_HASH\")), None)\n",
    "\n",
    "# --- Daily Aggregation\n",
    "daily_df = agg_df.groupBy(date_col).agg(\n",
    "    F.count(\"*\").alias(\"total_records\"),\n",
    "    F.countDistinct(hash_col).alias(\"unique_subscribers\") if hash_col else F.lit(0).alias(\"unique_subscribers\"),\n",
    "    F.avg(duration_col).alias(\"avg_duration\") if duration_col else F.lit(0).alias(\"avg_duration\"),\n",
    "    F.sum(duration_col).alias(\"total_duration\") if duration_col else F.lit(0).alias(\"total_duration\")\n",
    ")\n",
    "daily_df.write.mode(\"overwrite\").saveAsTable(DAILY_TABLE)\n",
    "print(f\"‚úÖ Daily summary table '{DAILY_TABLE}' created\")\n",
    "spark.sql(f\"SELECT * FROM {DAILY_TABLE} LIMIT 5\").show()\n",
    "\n",
    "# --- Hourly Aggregation (if CALL_TIME exists)\n",
    "if \"CALL_TIME\" in agg_df.columns and date_col:\n",
    "    hourly_df = agg_df.withColumn(\n",
    "        \"call_hour\",\n",
    "        F.concat_ws(' ', F.col(date_col), F.col(\"CALL_TIME\"))\n",
    "    ).groupBy(\"call_hour\").agg(\n",
    "        F.count(\"*\").alias(\"hourly_records\"),\n",
    "        F.countDistinct(hash_col).alias(\"hourly_unique_users\") if hash_col else F.lit(0).alias(\"hourly_unique_users\"),\n",
    "        F.avg(duration_col).alias(\"avg_hourly_duration\") if duration_col else F.lit(0).alias(\"avg_hourly_duration\")\n",
    "    )\n",
    "    hourly_df.write.mode(\"overwrite\").saveAsTable(HOURLY_TABLE)\n",
    "    print(f\"‚úÖ Hourly summary table '{HOURLY_TABLE}' created\")\n",
    "    spark.sql(f\"SELECT * FROM {HOURLY_TABLE} LIMIT 5\").show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping hourly aggregation: 'CALL_TIME' column not found.\")\n",
    "\n",
    "# --- Network Metrics (CallingCellID)\n",
    "if \"CallingCellID\" in agg_df.columns:\n",
    "    network_df = agg_df.groupBy(\"CallingCellID\").agg(\n",
    "        F.count(\"*\").alias(\"total_calls\"),\n",
    "        F.countDistinct(hash_col).alias(\"unique_users\") if hash_col else F.lit(0).alias(\"unique_users\"),\n",
    "        F.avg(duration_col).alias(\"avg_call_duration\") if duration_col else F.lit(0).alias(\"avg_call_duration\")\n",
    "    )\n",
    "    network_df.write.mode(\"overwrite\").saveAsTable(NETWORK_TABLE)\n",
    "    print(f\"‚úÖ Network metrics table '{NETWORK_TABLE}' created\")\n",
    "    spark.sql(f\"SELECT * FROM {NETWORK_TABLE} LIMIT 5\").show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Skipping network metrics: 'CallingCellID' column not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109238fc-70cc-4465-a3c1-1436d0d35443",
   "metadata": {},
   "source": [
    "#### Create Analytical Views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a96be3a7-44a7-46bc-af09-548fc691fe21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ View v_daily_trends created\n",
      "‚úÖ View v_network_performance created\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 7 ‚Äì Create Analytical Views\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "# View 1: Peak Hours\n",
    "if spark.catalog.tableExists(HOURLY_TABLE):\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW v_peak_hours AS\n",
    "    SELECT HOUR(call_hour) as hour_of_day,\n",
    "           AVG(hourly_records) as avg_calls_per_hour,\n",
    "           MAX(hourly_records) as max_calls_per_hour,\n",
    "           COUNT(*) as total_hours_sampled\n",
    "    FROM {HOURLY_TABLE}\n",
    "    WHERE call_hour IS NOT NULL\n",
    "    GROUP BY HOUR(call_hour)\n",
    "    ORDER BY avg_calls_per_hour DESC\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ View v_peak_hours created\")\n",
    "\n",
    "# View 2: Daily Trends\n",
    "if spark.catalog.tableExists(DAILY_TABLE):\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW v_daily_trends AS\n",
    "    SELECT {date_col} as call_date, total_records, unique_subscribers,\n",
    "           ROUND(total_records * 1.0 / unique_subscribers, 2) as calls_per_subscriber,\n",
    "           ROUND(avg_duration, 2) as avg_duration_formatted\n",
    "    FROM {DAILY_TABLE}\n",
    "    ORDER BY call_date DESC\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ View v_daily_trends created\")\n",
    "\n",
    "# View 3: Network Performance\n",
    "if spark.catalog.tableExists(NETWORK_TABLE):\n",
    "    spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE VIEW v_network_performance AS\n",
    "    SELECT CallingCellID, total_calls, unique_users, avg_call_duration,\n",
    "           ROUND(total_calls * 1.0 / unique_users, 2) as calls_per_user,\n",
    "           CASE\n",
    "               WHEN total_calls > 1000 THEN 'High Traffic'\n",
    "               WHEN total_calls > 100 THEN 'Medium Traffic'\n",
    "               ELSE 'Low Traffic'\n",
    "           END as traffic_category\n",
    "    FROM {NETWORK_TABLE}\n",
    "    ORDER BY total_calls DESC\n",
    "    \"\"\")\n",
    "    print(\"‚úÖ View v_network_performance created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4250e9-723d-4b1b-a183-aa71f165c506",
   "metadata": {},
   "source": [
    "#### Data Quality Table "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4f51e33-c4b2-4a18-95ce-2af019e77281",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data quality checks table created\n",
      "+----------+------------------+---------------+---------------+-------------------+\n",
      "|total_rows|unique_hash_values|null_hash_count|null_date_count|check_timestamp    |\n",
      "+----------+------------------+---------------+---------------+-------------------+\n",
      "|89911     |40843             |0              |0              |2025-06-19 07:02:58|\n",
      "+----------+------------------+---------------+---------------+-------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 8 ‚Äì Data Quality Table (Optional/Recommended)\n",
    "# ------------------------------------------------------------\n",
    "quality_df = main_df.agg(\n",
    "    F.count(\"*\").alias(\"total_rows\"),\n",
    "    F.countDistinct(hash_col).alias(\"unique_hash_values\") if hash_col else F.lit(0).alias(\"unique_hash_values\"),\n",
    "    F.sum(F.when(F.col(hash_col).isNull(), 1).otherwise(0)).alias(\"null_hash_count\") if hash_col else F.lit(0).alias(\"null_hash_count\"),\n",
    "    F.sum(F.when(F.col(date_col).isNull(), 1).otherwise(0)).alias(\"null_date_count\") if date_col else F.lit(0).alias(\"null_date_count\")\n",
    ").withColumn(\"check_timestamp\", F.lit(datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")))\n",
    "\n",
    "quality_df.write.mode(\"overwrite\").saveAsTable(\"data_quality_checks\")\n",
    "print(\"‚úÖ Data quality checks table created\")\n",
    "spark.sql(\"SELECT * FROM data_quality_checks\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b91a0c31-9f4b-466c-a46f-d29c61d4b608",
   "metadata": {},
   "source": [
    "#### Summary, Verification, and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9e4362d-9693-4206-9fee-2440efd03edf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìã HIVE TABLES CREATION SUMMARY REPORT\n",
      "================================================================================\n",
      "Database: algerie_telecom_cdr\n",
      "Tables in DB:\n",
      "+-------------------+---------------------+-----------+\n",
      "|namespace          |tableName            |isTemporary|\n",
      "+-------------------+---------------------+-----------+\n",
      "|algerie_telecom_cdr|cdr_anonymized       |false      |\n",
      "|algerie_telecom_cdr|cdr_daily_summary    |false      |\n",
      "|algerie_telecom_cdr|cdr_network_metrics  |false      |\n",
      "|algerie_telecom_cdr|v_daily_trends       |false      |\n",
      "|algerie_telecom_cdr|v_network_performance|false      |\n",
      "|algerie_telecom_cdr|data_quality_checks  |false      |\n",
      "+-------------------+---------------------+-----------+\n",
      "\n",
      "Source Parquet: /user/hive/warehouse/cdr_anonymized/\n",
      "\n",
      "Sample Queries:\n",
      "+----------+-------------+------------------+--------------------+----------------------+\n",
      "| call_date|total_records|unique_subscribers|calls_per_subscriber|avg_duration_formatted|\n",
      "+----------+-------------+------------------+--------------------+----------------------+\n",
      "|2025-01-01|        82730|             38311|                2.16|                166.01|\n",
      "|2024-12-31|         7181|              3924|                1.83|                105.01|\n",
      "+----------+-------------+------------------+--------------------+----------------------+\n",
      "\n",
      "+----------------+-----------+------------+------------------+--------------+----------------+\n",
      "|   CallingCellID|total_calls|unique_users| avg_call_duration|calls_per_user|traffic_category|\n",
      "+----------------+-----------+------------+------------------+--------------+----------------+\n",
      "|            NULL|      71470|       31179|154.47755701693018|          2.29|    High Traffic|\n",
      "|603093E819668914|      18441|        9664|186.92945068054877|          1.91|    High Traffic|\n",
      "+----------------+-----------+------------+------------------+--------------+----------------+\n",
      "\n",
      "‚úÖ All tables and views ready for BI and further Spark analysis.\n",
      "üöÄ Next: Proceed to Data Engineering Processing & Trend Detection notebook.\n",
      "‚úÖ Spark session stopped.\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# Cell 9 ‚Äì Summary, Verification, and Next Steps\n",
    "# ------------------------------------------------------------\n",
    "print(\"=\"*80)\n",
    "print(\"üìã HIVE TABLES CREATION SUMMARY REPORT\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Database: {DATABASE_NAME}\")\n",
    "print(\"Tables in DB:\")\n",
    "spark.sql(\"SHOW TABLES\").show(truncate=False)\n",
    "print(f\"Source Parquet: {HDFS_ANON_PATH}\")\n",
    "\n",
    "print(\"\\nSample Queries:\")\n",
    "if spark.catalog.tableExists(\"v_daily_trends\"):\n",
    "    spark.sql(\"SELECT * FROM v_daily_trends LIMIT 5\").show()\n",
    "if spark.catalog.tableExists(\"v_peak_hours\"):\n",
    "    spark.sql(\"SELECT * FROM v_peak_hours LIMIT 5\").show()\n",
    "if spark.catalog.tableExists(\"v_network_performance\"):\n",
    "    spark.sql(\"SELECT * FROM v_network_performance LIMIT 5\").show()\n",
    "\n",
    "print(\"‚úÖ All tables and views ready for BI and further Spark analysis.\")\n",
    "print(\"üöÄ Next: Proceed to Data Engineering Processing & Trend Detection notebook.\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"‚úÖ Spark session stopped.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562a5eed-d650-4969-9a96-c78074ac1610",
   "metadata": {},
   "outputs": [],
   "source": [
    "Genrated Mobile data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f46197d0-640c-45b7-858e-0c7ac64fb71e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:04:23 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n",
      "25/06/22 15:04:23 WARN SetCommand: 'SET hive.exec.dynamic.partition=true' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.dynamic.partition) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 15:04:23 WARN SetCommand: 'SET hive.exec.dynamic.partition.mode=nonstrict' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.dynamic.partition.mode) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 15:04:23 WARN SetCommand: 'SET hive.exec.max.dynamic.partitions=10000' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.max.dynamic.partitions) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 15:04:23 WARN SetCommand: 'SET hive.exec.max.dynamic.partitions.pernode=1000' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.max.dynamic.partitions.pernode) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: Fix Hive Table Types, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "================================================================================\n",
      "üîß FIXING HIVE TABLE TYPE MISMATCH\n",
      "================================================================================\n",
      "\n",
      "üöÄ Applying Quick Fix: Recreating table with correct types...\n",
      "‚úÖ Dropped old table\n",
      "‚úÖ Created new table with correct types\n",
      "\n",
      "üì• Reloading data into corrected table...\n"
     ]
    },
    {
     "ename": "AnalysisException",
     "evalue": "[INCOMPATIBLE_DATA_FOR_TABLE.CANNOT_SAFELY_CAST] Cannot write incompatible data for the table `spark_catalog`.`algerie_telecom_gen`.`cdr_partitioned`: Cannot safely cast `download_mb` \"STRING\" to \"DOUBLE\".",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 112\u001b[0m\n\u001b[1;32m    109\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSET hive.exec.max.dynamic.partitions.pernode = 1000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;66;03m# Insert data - no casting needed since types now match\u001b[39;00m\n\u001b[0;32m--> 112\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;124;43m    INSERT OVERWRITE TABLE cdr_partitioned PARTITION(year, month, day)\u001b[39;49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;124;43m    SELECT \u001b[39;49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;124;43m        cdr_id,\u001b[39;49m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;124;43m        subscriber_id,\u001b[39;49m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;124;43m        msisdn,\u001b[39;49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;124;43m        imsi,\u001b[39;49m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;124;43m        imei,\u001b[39;49m\n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;43m        service_type,\u001b[39;49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;124;43m        service_subtype,\u001b[39;49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;124;43m        session_id,\u001b[39;49m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;124;43m        calling_party,\u001b[39;49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;124;43m        called_party,\u001b[39;49m\n\u001b[1;32m    125\u001b[0m \u001b[38;5;124;43m        CAST(start_time AS TIMESTAMP) as start_time,\u001b[39;49m\n\u001b[1;32m    126\u001b[0m \u001b[38;5;124;43m        CAST(end_time AS TIMESTAMP) as end_time,\u001b[39;49m\n\u001b[1;32m    127\u001b[0m \u001b[38;5;124;43m        duration,  -- No cast needed, already BIGINT\u001b[39;49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;124;43m        data_volume_mb,\u001b[39;49m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;124;43m        upload_mb,\u001b[39;49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;124;43m        download_mb,\u001b[39;49m\n\u001b[1;32m    131\u001b[0m \u001b[38;5;124;43m        cell_id,\u001b[39;49m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;124;43m        lac,\u001b[39;49m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;124;43m        location_area,\u001b[39;49m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;43m        serving_cell_tower,\u001b[39;49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;43m        network_type,\u001b[39;49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;124;43m        charging_amount,\u001b[39;49m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;43m        currency,\u001b[39;49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;43m        payment_type,\u001b[39;49m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;43m        tax_amount,\u001b[39;49m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;43m        call_result,\u001b[39;49m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;43m        quality_score,\u001b[39;49m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;43m        signal_strength,  -- No cast needed, already BIGINT\u001b[39;49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;124;43m        dropped_call_flag,\u001b[39;49m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;124;43m        network_congestion_level,\u001b[39;49m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;124;43m        customer_segment,\u001b[39;49m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;124;43m        tariff_plan,\u001b[39;49m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;124;43m        operator,\u001b[39;49m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;124;43m        age_group,\u001b[39;49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;124;43m        gender,\u001b[39;49m\n\u001b[1;32m    150\u001b[0m \u001b[38;5;124;43m        fraud_indicator,\u001b[39;49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;124;43m        unusual_pattern_flag,\u001b[39;49m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;124;43m        time_of_day_category,\u001b[39;49m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;124;43m        day_of_week,\u001b[39;49m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;124;43m        is_weekend,\u001b[39;49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;124;43m        is_holiday,\u001b[39;49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;43m        roaming_flag,\u001b[39;49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;124;43m        roaming_country,\u001b[39;49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;124;43m        roaming_type,\u001b[39;49m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;43m        special_offer_applied,\u001b[39;49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;43m        promotional_discount,\u001b[39;49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;124;43m        application_used,\u001b[39;49m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;124;43m        content_category,\u001b[39;49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;124;43m        revenue_per_mb,\u001b[39;49m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;43m        customer_lifetime_value_category,\u001b[39;49m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;43m        YEAR(CAST(start_time AS TIMESTAMP)) as year,\u001b[39;49m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;124;43m        MONTH(CAST(start_time AS TIMESTAMP)) as month,\u001b[39;49m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;43m        DAY(CAST(start_time AS TIMESTAMP)) as day\u001b[39;49m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;43m    FROM cdr_raw\u001b[39;49m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Data loaded successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# Verify the load\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [INCOMPATIBLE_DATA_FOR_TABLE.CANNOT_SAFELY_CAST] Cannot write incompatible data for the table `spark_catalog`.`algerie_telecom_gen`.`cdr_partitioned`: Cannot safely cast `download_mb` \"STRING\" to \"DOUBLE\"."
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# QUICK FIX FOR HIVE TABLE TYPE MISMATCH\n",
    "# =====================================================\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/scripts')\n",
    "from spark_init import init_spark\n",
    "\n",
    "spark = init_spark(\"Fix Hive Table Types\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîß FIXING HIVE TABLE TYPE MISMATCH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Use database\n",
    "spark.sql(\"USE algerie_telecom_gen\")\n",
    "\n",
    "# =====================================================\n",
    "# OPTION 1: DROP AND RECREATE WITH CORRECT TYPES (FASTEST)\n",
    "# =====================================================\n",
    "print(\"\\nüöÄ Applying Quick Fix: Recreating table with correct types...\")\n",
    "\n",
    "# Drop the problematic partitioned table\n",
    "spark.sql(\"DROP TABLE IF EXISTS cdr_partitioned\")\n",
    "print(\"‚úÖ Dropped old table\")\n",
    "\n",
    "# Recreate with correct types matching your Parquet schema\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS cdr_partitioned (\n",
    "        -- String fields\n",
    "        cdr_id STRING,\n",
    "        subscriber_id STRING,\n",
    "        msisdn STRING,\n",
    "        imsi STRING,\n",
    "        imei STRING,\n",
    "        service_type STRING,\n",
    "        service_subtype STRING,\n",
    "        session_id STRING,\n",
    "        calling_party STRING,\n",
    "        called_party STRING,\n",
    "        start_time TIMESTAMP,\n",
    "        end_time TIMESTAMP,\n",
    "        \n",
    "        -- Fix: Use BIGINT instead of INT for these fields\n",
    "        duration BIGINT,  -- Changed from INT\n",
    "        signal_strength BIGINT,  -- Changed from INT\n",
    "        \n",
    "        -- Double fields\n",
    "        data_volume_mb DOUBLE,\n",
    "        upload_mb DOUBLE,\n",
    "        download_mb DOUBLE,\n",
    "        charging_amount DOUBLE,\n",
    "        tax_amount DOUBLE,\n",
    "        revenue_per_mb DOUBLE,\n",
    "        quality_score DOUBLE,\n",
    "        promotional_discount DOUBLE,\n",
    "        \n",
    "        -- String fields continued\n",
    "        cell_id STRING,\n",
    "        lac STRING,\n",
    "        location_area STRING,\n",
    "        serving_cell_tower STRING,\n",
    "        network_type STRING,\n",
    "        currency STRING,\n",
    "        payment_type STRING,\n",
    "        call_result STRING,\n",
    "        customer_segment STRING,\n",
    "        tariff_plan STRING,\n",
    "        operator STRING,\n",
    "        age_group STRING,\n",
    "        gender STRING,\n",
    "        roaming_country STRING,\n",
    "        roaming_type STRING,\n",
    "        special_offer_applied STRING,\n",
    "        network_congestion_level STRING,\n",
    "        time_of_day_category STRING,\n",
    "        day_of_week STRING,\n",
    "        application_used STRING,\n",
    "        content_category STRING,\n",
    "        customer_lifetime_value_category STRING,\n",
    "        \n",
    "        -- Boolean fields\n",
    "        dropped_call_flag BOOLEAN,\n",
    "        roaming_flag BOOLEAN,\n",
    "        fraud_indicator BOOLEAN,\n",
    "        unusual_pattern_flag BOOLEAN,\n",
    "        is_weekend BOOLEAN,\n",
    "        is_holiday BOOLEAN\n",
    "    )\n",
    "    PARTITIONED BY (year INT, month INT, day INT)\n",
    "    STORED AS PARQUET\n",
    "    TBLPROPERTIES (\n",
    "        'compression' = 'snappy',\n",
    "        'transactional' = 'false'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created new table with correct types\")\n",
    "\n",
    "# =====================================================\n",
    "# RELOAD DATA WITH DYNAMIC PARTITIONING\n",
    "# =====================================================\n",
    "print(\"\\nüì• Reloading data into corrected table...\")\n",
    "\n",
    "# Configure for dynamic partitioning\n",
    "spark.sql(\"SET hive.exec.dynamic.partition = true\")\n",
    "spark.sql(\"SET hive.exec.dynamic.partition.mode = nonstrict\")\n",
    "spark.sql(\"SET hive.exec.max.dynamic.partitions = 10000\")\n",
    "spark.sql(\"SET hive.exec.max.dynamic.partitions.pernode = 1000\")\n",
    "\n",
    "# Insert data - no casting needed since types now match\n",
    "spark.sql(\"\"\"\n",
    "    INSERT OVERWRITE TABLE cdr_partitioned PARTITION(year, month, day)\n",
    "    SELECT \n",
    "        cdr_id,\n",
    "        subscriber_id,\n",
    "        msisdn,\n",
    "        imsi,\n",
    "        imei,\n",
    "        service_type,\n",
    "        service_subtype,\n",
    "        session_id,\n",
    "        calling_party,\n",
    "        called_party,\n",
    "        CAST(start_time AS TIMESTAMP) as start_time,\n",
    "        CAST(end_time AS TIMESTAMP) as end_time,\n",
    "        duration,  -- No cast needed, already BIGINT\n",
    "        data_volume_mb,\n",
    "        upload_mb,\n",
    "        download_mb,\n",
    "        cell_id,\n",
    "        lac,\n",
    "        location_area,\n",
    "        serving_cell_tower,\n",
    "        network_type,\n",
    "        charging_amount,\n",
    "        currency,\n",
    "        payment_type,\n",
    "        tax_amount,\n",
    "        call_result,\n",
    "        quality_score,\n",
    "        signal_strength,  -- No cast needed, already BIGINT\n",
    "        dropped_call_flag,\n",
    "        network_congestion_level,\n",
    "        customer_segment,\n",
    "        tariff_plan,\n",
    "        operator,\n",
    "        age_group,\n",
    "        gender,\n",
    "        fraud_indicator,\n",
    "        unusual_pattern_flag,\n",
    "        time_of_day_category,\n",
    "        day_of_week,\n",
    "        is_weekend,\n",
    "        is_holiday,\n",
    "        roaming_flag,\n",
    "        roaming_country,\n",
    "        roaming_type,\n",
    "        special_offer_applied,\n",
    "        promotional_discount,\n",
    "        application_used,\n",
    "        content_category,\n",
    "        revenue_per_mb,\n",
    "        customer_lifetime_value_category,\n",
    "        YEAR(CAST(start_time AS TIMESTAMP)) as year,\n",
    "        MONTH(CAST(start_time AS TIMESTAMP)) as month,\n",
    "        DAY(CAST(start_time AS TIMESTAMP)) as day\n",
    "    FROM cdr_raw\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Data loaded successfully!\")\n",
    "\n",
    "# Verify the load\n",
    "count = spark.sql(\"SELECT COUNT(*) FROM cdr_partitioned\").collect()[0][0]\n",
    "print(f\"\\nüìä Verification: {count:,} records loaded into partitioned table\")\n",
    "\n",
    "# Show partition statistics\n",
    "print(\"\\nüìä Partition Statistics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        year, \n",
    "        month,\n",
    "        COUNT(DISTINCT day) as days,\n",
    "        COUNT(*) as records\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month\n",
    "    ORDER BY year, month\n",
    "\"\"\").show()\n",
    "\n",
    "print(\"\\n‚úÖ Table fixed and data reloaded successfully!\")\n",
    "print(\"You can now continue with the rest of Notebook 02\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65c2de76-bcea-41fb-bd42-0a97eb8c7e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:04:50 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: Hive Tables - Continuation, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "================================================================================\n",
      "üìä CONTINUING HIVE TABLE SETUP\n",
      "================================================================================\n",
      "\n",
      "üëÅÔ∏è  CREATING ADVANCED ANALYTICAL VIEWS...\n",
      "‚úÖ Created voice_calls view\n",
      "‚úÖ Created data_sessions view\n",
      "‚úÖ Created sms_records view\n",
      "‚úÖ Created fraud_cases view\n",
      "‚úÖ Created network_issues view\n",
      "‚úÖ Created high_value_customers view\n",
      "‚úÖ Created special_offers_usage view\n",
      "‚úÖ Created roaming_records view\n",
      "‚úÖ Created app_usage_data view\n",
      "\n",
      "‚úÖ Total views created: 9\n",
      "\n",
      "üìä CREATING PRE-AGGREGATED ANALYTICAL TABLES...\n",
      "\n",
      "‚è≥ Creating daily_kpis table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:04:50 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n",
      "25/06/22 15:04:51 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created daily_kpis table\n",
      "\n",
      "‚è≥ Creating hourly_patterns table...\n",
      "‚úÖ Created hourly_patterns table\n",
      "\n",
      "‚è≥ Creating location_network_metrics table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:04:52 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created location_network_metrics table\n",
      "\n",
      "‚è≥ Creating customer_demographics_summary table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:04:52 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created customer_demographics_summary table\n",
      "\n",
      "‚è≥ Creating app_usage_analytics table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:04:53 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created app_usage_analytics table\n",
      "\n",
      "‚úÖ All pre-aggregated tables created successfully!\n",
      "\n",
      "üìà CREATING MATERIALIZED VIEWS FOR REAL-TIME ANALYTICS...\n",
      "\n",
      "‚è≥ Creating fraud_monitoring table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:04:53 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created fraud_monitoring table\n",
      "\n",
      "‚è≥ Creating revenue_tracking table...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:04:53 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created revenue_tracking table\n",
      "\n",
      "üìà COMPUTING TABLE STATISTICS FOR OPTIMIZATION...\n",
      "\n",
      "‚è≥ Analyzing cdr_partitioned...\n",
      "   ‚úÖ Analyzed cdr_partitioned\n",
      "\n",
      "‚è≥ Analyzing daily_kpis...\n",
      "   ‚úÖ Analyzed daily_kpis\n",
      "\n",
      "‚è≥ Analyzing hourly_patterns...\n",
      "   ‚úÖ Analyzed hourly_patterns\n",
      "\n",
      "‚è≥ Analyzing location_network_metrics...\n",
      "   ‚úÖ Analyzed location_network_metrics\n",
      "\n",
      "‚è≥ Analyzing customer_demographics_summary...\n",
      "   ‚úÖ Analyzed customer_demographics_summary\n",
      "\n",
      "‚è≥ Analyzing app_usage_analytics...\n",
      "   ‚úÖ Analyzed app_usage_analytics\n",
      "\n",
      "‚è≥ Analyzing fraud_monitoring...\n",
      "   ‚úÖ Analyzed fraud_monitoring\n",
      "\n",
      "‚è≥ Analyzing revenue_tracking...\n",
      "   ‚úÖ Analyzed revenue_tracking\n",
      "\n",
      "================================================================================\n",
      "üìä HIVE INFRASTRUCTURE SETUP COMPLETE!\n",
      "================================================================================\n",
      "\n",
      "üìã Database Objects Created:\n",
      "\n",
      "üì¶ Tables:\n",
      "   - voice_calls: 0 records\n",
      "   - data_sessions: 0 records\n",
      "   - sms_records: 0 records\n",
      "   - fraud_cases: 0 records\n",
      "   - network_issues: 0 records\n",
      "   - high_value_customers: 0 records\n",
      "   - special_offers_usage: 0 records\n",
      "   - roaming_records: 0 records\n",
      "   - app_usage_data: 0 records\n",
      "   - daily_kpis: 0 records\n",
      "   - hourly_patterns: 0 records\n",
      "   - location_network_metrics: 0 records\n",
      "   - customer_demographics_summary: 0 records\n",
      "   - app_usage_analytics: 0 records\n",
      "   - fraud_monitoring: 0 records\n",
      "   - revenue_tracking: 0 records\n",
      "\n",
      "üëÅÔ∏è  Views created: 9\n",
      "\n",
      "‚ö° Testing query performance...\n",
      "\n",
      "‚úÖ Query executed in 0.14 seconds\n",
      "\n",
      "Sample results:\n",
      "\n",
      "================================================================================\n",
      "üéØ SETUP COMPLETE! Next Steps:\n",
      "   ‚Üí Run Notebook 03 for Advanced Data Engineering\n",
      "   ‚Üí Run Notebook 04 for Anomaly Detection & Trend Analysis\n",
      "   ‚Üí Run Notebook 05 for Business Intelligence Dashboards\n",
      "================================================================================\n",
      "\n",
      "üîö Spark session closed successfully.\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# NOTEBOOK 02: CONTINUATION - CREATE VIEWS AND AGGREGATIONS\n",
    "# Continue from Section 5 after fixing table types\n",
    "# =====================================================\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F\n",
    "import time\n",
    "\n",
    "# Initialize Spark\n",
    "spark = init_spark(\"Hive Tables - Continuation\")\n",
    "\n",
    "# Use database\n",
    "spark.sql(\"USE algerie_telecom_gen\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä CONTINUING HIVE TABLE SETUP\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =====================================================\n",
    "# 5. CREATE OPTIMIZED ANALYTICAL VIEWS\n",
    "# =====================================================\n",
    "print(\"\\nüëÅÔ∏è  CREATING ADVANCED ANALYTICAL VIEWS...\")\n",
    "\n",
    "views_created = 0\n",
    "\n",
    "# Service-based views\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW voice_calls AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'VOICE'\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created voice_calls view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW data_sessions AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'DATA'\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created data_sessions view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW sms_records AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'SMS'\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created sms_records view\")\n",
    "\n",
    "# Advanced analytical views\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW fraud_cases AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE fraud_indicator = true OR unusual_pattern_flag = true\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created fraud_cases view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW network_issues AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE dropped_call_flag = true \n",
    "       OR call_result = 'FAILED'\n",
    "       OR network_congestion_level IN ('MEDIUM', 'HIGH')\n",
    "       OR quality_score < 0.5\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created network_issues view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW high_value_customers AS\n",
    "    SELECT DISTINCT\n",
    "        subscriber_id,\n",
    "        customer_segment,\n",
    "        customer_lifetime_value_category,\n",
    "        tariff_plan,\n",
    "        payment_type,\n",
    "        age_group,\n",
    "        gender,\n",
    "        operator\n",
    "    FROM cdr_partitioned\n",
    "    WHERE customer_segment IN ('Premium')\n",
    "       OR customer_lifetime_value_category IN ('High', 'Very High')\n",
    "       OR payment_type = 'POSTPAID'\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created high_value_customers view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW special_offers_usage AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE special_offer_applied != 'None' \n",
    "      AND promotional_discount > 0\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created special_offers_usage view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW roaming_records AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE roaming_flag = true\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created roaming_records view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW app_usage_data AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'DATA' \n",
    "      AND application_used IS NOT NULL \n",
    "      AND application_used != ''\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "print(\"‚úÖ Created app_usage_data view\")\n",
    "\n",
    "print(f\"\\n‚úÖ Total views created: {views_created}\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. CREATE PRE-AGGREGATED TABLES FOR PERFORMANCE\n",
    "# =====================================================\n",
    "print(\"\\nüìä CREATING PRE-AGGREGATED ANALYTICAL TABLES...\")\n",
    "\n",
    "# Daily KPIs by Service and Operator\n",
    "print(\"\\n‚è≥ Creating daily_kpis table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS daily_kpis AS\n",
    "    SELECT \n",
    "        year, month, day,\n",
    "        service_type,\n",
    "        operator,\n",
    "        customer_segment,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_subscribers,\n",
    "        COUNT(*) as total_transactions,\n",
    "        SUM(duration) as total_duration_seconds,\n",
    "        SUM(data_volume_mb) as total_data_mb,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        SUM(tax_amount) as total_tax,\n",
    "        AVG(quality_score) as avg_quality_score,\n",
    "        SUM(CASE WHEN fraud_indicator THEN 1 ELSE 0 END) as fraud_cases,\n",
    "        SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_calls,\n",
    "        SUM(CASE WHEN special_offer_applied != 'None' THEN 1 ELSE 0 END) as special_offer_usage,\n",
    "        AVG(promotional_discount) as avg_discount_applied\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month, day, service_type, operator, customer_segment\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created daily_kpis table\")\n",
    "\n",
    "# Hourly Usage Patterns\n",
    "print(\"\\n‚è≥ Creating hourly_patterns table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hourly_patterns AS\n",
    "    SELECT \n",
    "        time_of_day_category,\n",
    "        day_of_week,\n",
    "        is_weekend,\n",
    "        service_type,\n",
    "        operator,\n",
    "        COUNT(*) as transaction_count,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_users,\n",
    "        AVG(duration) as avg_duration,\n",
    "        AVG(data_volume_mb) as avg_data_mb,\n",
    "        AVG(charging_amount) as avg_revenue,\n",
    "        AVG(quality_score) as avg_quality,\n",
    "        SUM(CASE WHEN network_congestion_level = 'HIGH' THEN 1 ELSE 0 END) as high_congestion_count\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY time_of_day_category, day_of_week, is_weekend, service_type, operator\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created hourly_patterns table\")\n",
    "\n",
    "# Location Performance Metrics\n",
    "print(\"\\n‚è≥ Creating location_network_metrics table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS location_network_metrics AS\n",
    "    SELECT \n",
    "        location_area,\n",
    "        network_type,\n",
    "        operator,\n",
    "        service_type,\n",
    "        COUNT(*) as total_transactions,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_subscribers,\n",
    "        AVG(quality_score) as avg_quality_score,\n",
    "        AVG(signal_strength) as avg_signal_strength,\n",
    "        SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_count,\n",
    "        SUM(CASE WHEN call_result = 'FAILED' THEN 1 ELSE 0 END) as failed_count,\n",
    "        SUM(CASE WHEN network_congestion_level = 'HIGH' THEN 1 ELSE 0 END) as high_congestion_count,\n",
    "        AVG(data_volume_mb) as avg_data_usage,\n",
    "        SUM(charging_amount) as total_revenue\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY location_area, network_type, operator, service_type\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created location_network_metrics table\")\n",
    "\n",
    "# Customer Demographics Analysis\n",
    "print(\"\\n‚è≥ Creating customer_demographics_summary table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customer_demographics_summary AS\n",
    "    SELECT \n",
    "        customer_segment,\n",
    "        age_group,\n",
    "        gender,\n",
    "        payment_type,\n",
    "        operator,\n",
    "        COUNT(DISTINCT subscriber_id) as subscriber_count,\n",
    "        COUNT(*) as total_activities,\n",
    "        AVG(charging_amount) as avg_transaction_value,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        AVG(data_volume_mb) as avg_data_usage,\n",
    "        SUM(CASE WHEN fraud_indicator THEN 1 ELSE 0 END) as fraud_incidents,\n",
    "        AVG(promotional_discount) as avg_discount_received\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY customer_segment, age_group, gender, payment_type, operator\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created customer_demographics_summary table\")\n",
    "\n",
    "# App Usage Analytics\n",
    "print(\"\\n‚è≥ Creating app_usage_analytics table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS app_usage_analytics AS\n",
    "    SELECT \n",
    "        application_used,\n",
    "        content_category,\n",
    "        customer_segment,\n",
    "        age_group,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_users,\n",
    "        COUNT(*) as total_sessions,\n",
    "        SUM(data_volume_mb) as total_data_mb,\n",
    "        AVG(data_volume_mb) as avg_data_per_session,\n",
    "        SUM(duration) as total_duration,\n",
    "        AVG(duration) as avg_session_duration,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        AVG(revenue_per_mb) as avg_revenue_per_mb\n",
    "    FROM cdr_partitioned\n",
    "    WHERE service_type = 'DATA' AND application_used IS NOT NULL\n",
    "    GROUP BY application_used, content_category, customer_segment, age_group\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created app_usage_analytics table\")\n",
    "\n",
    "print(\"\\n‚úÖ All pre-aggregated tables created successfully!\")\n",
    "\n",
    "# =====================================================\n",
    "# 7. CREATE MATERIALIZED VIEWS FOR DASHBOARDS\n",
    "# =====================================================\n",
    "print(\"\\nüìà CREATING MATERIALIZED VIEWS FOR REAL-TIME ANALYTICS...\")\n",
    "\n",
    "# Real-time fraud monitoring\n",
    "print(\"\\n‚è≥ Creating fraud_monitoring table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fraud_monitoring AS\n",
    "    SELECT \n",
    "        DATE(start_time) as fraud_date,\n",
    "        operator,\n",
    "        location_area,\n",
    "        COUNT(*) as total_incidents,\n",
    "        COUNT(DISTINCT subscriber_id) as affected_subscribers,\n",
    "        SUM(charging_amount) as potential_loss,\n",
    "        COLLECT_SET(service_type) as affected_services\n",
    "    FROM cdr_partitioned\n",
    "    WHERE fraud_indicator = true OR unusual_pattern_flag = true\n",
    "    GROUP BY DATE(start_time), operator, location_area\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created fraud_monitoring table\")\n",
    "\n",
    "# Revenue tracking\n",
    "print(\"\\n‚è≥ Creating revenue_tracking table...\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS revenue_tracking AS\n",
    "    SELECT \n",
    "        year, month, day,\n",
    "        operator,\n",
    "        service_type,\n",
    "        customer_segment,\n",
    "        payment_type,\n",
    "        SUM(charging_amount) as gross_revenue,\n",
    "        SUM(tax_amount) as tax_collected,\n",
    "        SUM(charging_amount - tax_amount) as net_revenue,\n",
    "        COUNT(DISTINCT subscriber_id) as active_customers,\n",
    "        COUNT(*) as total_transactions,\n",
    "        AVG(charging_amount) as arpu_daily\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month, day, operator, service_type, customer_segment, payment_type\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created revenue_tracking table\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. COMPUTE COMPREHENSIVE STATISTICS\n",
    "# =====================================================\n",
    "print(\"\\nüìà COMPUTING TABLE STATISTICS FOR OPTIMIZATION...\")\n",
    "\n",
    "tables_to_analyze = [\n",
    "    'cdr_partitioned', 'daily_kpis', 'hourly_patterns', \n",
    "    'location_network_metrics', 'customer_demographics_summary',\n",
    "    'app_usage_analytics', 'fraud_monitoring', 'revenue_tracking'\n",
    "]\n",
    "\n",
    "for table in tables_to_analyze:\n",
    "    print(f\"\\n‚è≥ Analyzing {table}...\")\n",
    "    try:\n",
    "        spark.sql(f\"ANALYZE TABLE {table} COMPUTE STATISTICS\")\n",
    "        # Note: COMPUTE STATISTICS FOR ALL COLUMNS might be too intensive\n",
    "        # Only do it for smaller tables\n",
    "        if table in ['hourly_patterns', 'customer_demographics_summary']:\n",
    "            spark.sql(f\"ANALYZE TABLE {table} COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
    "        print(f\"   ‚úÖ Analyzed {table}\")\n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ö†Ô∏è  Could not analyze {table}: {str(e)}\")\n",
    "\n",
    "# =====================================================\n",
    "# 9. FINAL VERIFICATION AND SUMMARY\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä HIVE INFRASTRUCTURE SETUP COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary of created objects\n",
    "print(\"\\nüìã Database Objects Created:\")\n",
    "\n",
    "# Tables\n",
    "print(\"\\nüì¶ Tables:\")\n",
    "tables = spark.sql(f\"SHOW TABLES IN algerie_telecom_gen\").filter(\"isTemporary = false\").collect()\n",
    "for table in tables:\n",
    "    if table.tableName not in ['cdr_raw', 'cdr_partitioned']:  # Skip these as we know their counts\n",
    "        try:\n",
    "            count = spark.sql(f\"SELECT COUNT(*) FROM {table.tableName}\").collect()[0][0]\n",
    "            print(f\"   - {table.tableName}: {count:,} records\")\n",
    "        except:\n",
    "            print(f\"   - {table.tableName}: (aggregated table)\")\n",
    "\n",
    "# Views\n",
    "print(\"\\nüëÅÔ∏è  Views created: {}\".format(views_created))\n",
    "\n",
    "# Performance test\n",
    "print(\"\\n‚ö° Testing query performance...\")\n",
    "test_start = time.time()\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        operator,\n",
    "        service_type,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(SUM(charging_amount), 2) as revenue\n",
    "    FROM cdr_partitioned\n",
    "    WHERE year = 2025 AND month = 1\n",
    "    GROUP BY operator, service_type\n",
    "    ORDER BY operator, service_type\n",
    "\"\"\").collect()\n",
    "\n",
    "test_time = time.time() - test_start\n",
    "print(f\"\\n‚úÖ Query executed in {test_time:.2f} seconds\")\n",
    "print(\"\\nSample results:\")\n",
    "for row in result[:6]:  # Show first 6 rows\n",
    "    print(f\"   {row['operator']} - {row['service_type']}: {row['count']:,} calls, {row['revenue']:,.2f} DZD\")\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üéØ SETUP COMPLETE! Next Steps:\")\n",
    "print(\"   ‚Üí Run Notebook 03 for Advanced Data Engineering\")\n",
    "print(\"   ‚Üí Run Notebook 04 for Anomaly Detection & Trend Analysis\")\n",
    "print(\"   ‚Üí Run Notebook 05 for Business Intelligence Dashboards\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nüîö Spark session closed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e10ddb06-eb7e-400e-9d3a-5df2bc06120a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:02:53 WARN SparkSession: Using an existing Spark session; only runtime SQL configurations will take effect.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: Hive Tables - Generated CDR Advanced, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "================================================================================\n",
      "üìä ALGERIE TELECOM - ADVANCED CDR DATA PIPELINE\n",
      "üìÖ Creating Comprehensive Hive Infrastructure\n",
      "================================================================================\n",
      "\n",
      "üèóÔ∏è  CREATING ADVANCED DATABASE STRUCTURE...\n",
      "‚úÖ Database 'algerie_telecom_gen' created successfully\n",
      "\n",
      "üîç ANALYZING GENERATED DATA STRUCTURE...\n",
      "\n",
      "üìã Data Schema Analysis:\n",
      "Total columns: 50\n",
      "\n",
      "üìä Data Quality Metrics:\n",
      "Total records: 146,876,149\n",
      "Anonymization status: ‚úÖ Enabled\n",
      "\n",
      "üì¶ CREATING MAIN EXTERNAL TABLE...\n",
      "‚úÖ External table 'cdr_raw' created\n",
      "   Records loaded: 146,876,149\n",
      "\n",
      "üóÇÔ∏è  CREATING PARTITIONED TABLE FOR PERFORMANCE...\n",
      "‚úÖ Partitioned table structure created\n",
      "\n",
      "üì• Loading data with dynamic partitioning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:02:55 WARN SetCommand: 'SET hive.exec.dynamic.partition=true' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.dynamic.partition) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 15:02:55 WARN SetCommand: 'SET hive.exec.dynamic.partition.mode=nonstrict' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.dynamic.partition.mode) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 15:02:55 WARN SetCommand: 'SET hive.exec.max.dynamic.partitions=10000' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.max.dynamic.partitions) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 15:02:55 WARN SetCommand: 'SET hive.exec.max.dynamic.partitions.pernode=1000' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.max.dynamic.partitions.pernode) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 15:02:56 WARN TaskSetManager: Lost task 2.0 in stage 39.0 (TID 1177) (172.30.0.34 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "25/06/22 15:02:56 WARN TaskSetManager: Lost task 6.0 in stage 39.0 (TID 1181) (172.30.0.34 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250116_to_20250120.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "25/06/22 15:02:56 WARN TaskSetManager: Lost task 0.0 in stage 39.0 (TID 1175) (172.30.0.34 executor 0): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to hdfs://namenode:9000/user/hive/warehouse/algerie_telecom_gen.db/cdr_partitioned/.spark-staging-d11a53dc-551c-4ce2-a010-ed101aeb8578.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:774)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250101_to_20250105.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\t... 17 more\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 28 more\n",
      "\n",
      "25/06/22 15:02:57 WARN TaskSetManager: Lost task 4.0 in stage 39.0 (TID 1179) (172.30.0.34 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250111_to_20250115.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "25/06/22 15:02:57 WARN TaskSetManager: Lost task 1.0 in stage 39.0 (TID 1176) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250101_to_20250105.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "25/06/22 15:02:57 WARN TaskSetManager: Lost task 8.0 in stage 39.0 (TID 1183) (172.30.0.34 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250121_to_20250125.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "25/06/22 15:02:58 ERROR TaskSetManager: Task 2 in stage 39.0 failed 4 times; aborting job\n",
      "25/06/22 15:02:58 ERROR FileFormatWriter: Aborting job d30d5112-58ff-433b-b815-6bc84977e728.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n",
      "\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n",
      "\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n",
      "\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n",
      "\tat jdk.internal.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:829)\n",
      "Caused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "25/06/22 15:02:58 WARN TaskSetManager: Lost task 8.2 in stage 39.0 (TID 1208) (172.30.0.35 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/22 15:02:58 WARN TaskSetManager: Lost task 1.2 in stage 39.0 (TID 1202) (172.30.0.34 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/22 15:02:58 WARN TaskSetManager: Lost task 0.3 in stage 39.0 (TID 1201) (172.30.0.35 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/22 15:02:58 WARN TaskSetManager: Lost task 3.3 in stage 39.0 (TID 1203) (172.30.0.34 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/22 15:02:58 WARN TaskSetManager: Lost task 4.3 in stage 39.0 (TID 1204) (172.30.0.35 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/22 15:02:58 WARN TaskSetManager: Lost task 7.3 in stage 39.0 (TID 1207) (172.30.0.34 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/06/22 15:02:58 WARN TaskSetManager: Lost task 6.3 in stage 39.0 (TID 1206) (172.30.0.35 executor 1): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o73.sql.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat jdk.internal.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 25 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 261\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[38;5;66;03m# Insert with proper timestamp conversion and partitioning\u001b[39;00m\n\u001b[1;32m    260\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 261\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\"\"\u001b[39;49m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;124;43m    INSERT OVERWRITE TABLE cdr_partitioned PARTITION(year, month, day)\u001b[39;49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;124;43m    SELECT \u001b[39;49m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;124;43m        cdr_id,\u001b[39;49m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;43m        subscriber_id,\u001b[39;49m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;43m        msisdn,\u001b[39;49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;124;43m        imsi,\u001b[39;49m\n\u001b[1;32m    268\u001b[0m \u001b[38;5;124;43m        imei,\u001b[39;49m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;124;43m        service_type,\u001b[39;49m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;124;43m        service_subtype,\u001b[39;49m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;124;43m        session_id,\u001b[39;49m\n\u001b[1;32m    272\u001b[0m \u001b[38;5;124;43m        calling_party,\u001b[39;49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;124;43m        called_party,\u001b[39;49m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;43m        CAST(start_time AS TIMESTAMP) as start_time,\u001b[39;49m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;43m        CAST(end_time AS TIMESTAMP) as end_time,\u001b[39;49m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;43m        CAST(duration AS INT) as duration,\u001b[39;49m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;124;43m        data_volume_mb,\u001b[39;49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;124;43m        upload_mb,\u001b[39;49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;124;43m        download_mb,\u001b[39;49m\n\u001b[1;32m    280\u001b[0m \u001b[38;5;124;43m        cell_id,\u001b[39;49m\n\u001b[1;32m    281\u001b[0m \u001b[38;5;124;43m        lac,\u001b[39;49m\n\u001b[1;32m    282\u001b[0m \u001b[38;5;124;43m        location_area,\u001b[39;49m\n\u001b[1;32m    283\u001b[0m \u001b[38;5;124;43m        serving_cell_tower,\u001b[39;49m\n\u001b[1;32m    284\u001b[0m \u001b[38;5;124;43m        network_type,\u001b[39;49m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;124;43m        operator,\u001b[39;49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;124;43m        charging_amount,\u001b[39;49m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;124;43m        currency,\u001b[39;49m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;124;43m        payment_type,\u001b[39;49m\n\u001b[1;32m    289\u001b[0m \u001b[38;5;124;43m        tax_amount,\u001b[39;49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;124;43m        revenue_per_mb,\u001b[39;49m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;124;43m        call_result,\u001b[39;49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;124;43m        quality_score,\u001b[39;49m\n\u001b[1;32m    293\u001b[0m \u001b[38;5;124;43m        signal_strength,\u001b[39;49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;124;43m        dropped_call_flag,\u001b[39;49m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;124;43m        network_congestion_level,\u001b[39;49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;124;43m        customer_segment,\u001b[39;49m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;124;43m        tariff_plan,\u001b[39;49m\n\u001b[1;32m    298\u001b[0m \u001b[38;5;124;43m        age_group,\u001b[39;49m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;124;43m        gender,\u001b[39;49m\n\u001b[1;32m    300\u001b[0m \u001b[38;5;124;43m        customer_lifetime_value_category,\u001b[39;49m\n\u001b[1;32m    301\u001b[0m \u001b[38;5;124;43m        time_of_day_category,\u001b[39;49m\n\u001b[1;32m    302\u001b[0m \u001b[38;5;124;43m        day_of_week,\u001b[39;49m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;124;43m        is_weekend,\u001b[39;49m\n\u001b[1;32m    304\u001b[0m \u001b[38;5;124;43m        is_holiday,\u001b[39;49m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;124;43m        special_offer_applied,\u001b[39;49m\n\u001b[1;32m    306\u001b[0m \u001b[38;5;124;43m        promotional_discount,\u001b[39;49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;124;43m        application_used,\u001b[39;49m\n\u001b[1;32m    308\u001b[0m \u001b[38;5;124;43m        content_category,\u001b[39;49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;124;43m        roaming_flag,\u001b[39;49m\n\u001b[1;32m    310\u001b[0m \u001b[38;5;124;43m        roaming_country,\u001b[39;49m\n\u001b[1;32m    311\u001b[0m \u001b[38;5;124;43m        roaming_type,\u001b[39;49m\n\u001b[1;32m    312\u001b[0m \u001b[38;5;124;43m        fraud_indicator,\u001b[39;49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;124;43m        unusual_pattern_flag,\u001b[39;49m\n\u001b[1;32m    314\u001b[0m \u001b[38;5;124;43m        YEAR(CAST(start_time AS TIMESTAMP)) as year,\u001b[39;49m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;124;43m        MONTH(CAST(start_time AS TIMESTAMP)) as month,\u001b[39;49m\n\u001b[1;32m    316\u001b[0m \u001b[38;5;124;43m        DAY(CAST(start_time AS TIMESTAMP)) as day\u001b[39;49m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;43m    FROM cdr_raw\u001b[39;49m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;43m\"\"\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m load_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m‚úÖ Data loaded in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mload_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/session.py:1631\u001b[0m, in \u001b[0;36mSparkSession.sql\u001b[0;34m(self, sqlQuery, args, **kwargs)\u001b[0m\n\u001b[1;32m   1627\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1628\u001b[0m         litArgs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jvm\u001b[38;5;241m.\u001b[39mPythonUtils\u001b[38;5;241m.\u001b[39mtoArray(\n\u001b[1;32m   1629\u001b[0m             [_to_java_column(lit(v)) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m (args \u001b[38;5;129;01mor\u001b[39;00m [])]\n\u001b[1;32m   1630\u001b[0m         )\n\u001b[0;32m-> 1631\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jsparkSession\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msqlQuery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlitArgs\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   1632\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m   1633\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(kwargs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o73.sql.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 25 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n\tat org.apache.spark.sql.Dataset.<init>(Dataset.scala:220)\n\tat org.apache.spark.sql.Dataset$.$anonfun$ofRows$2(Dataset.scala:100)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:97)\n\tat org.apache.spark.sql.SparkSession.$anonfun$sql$1(SparkSession.scala:638)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:629)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:659)\n\tat jdk.internal.reflect.GeneratedMethodAccessor49.invoke(Unknown Source)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 25 more\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:02:58 WARN TaskSetManager: Lost task 5.3 in stage 39.0 (TID 1205) (172.30.0.34 executor 0): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 1197) (172.30.0.35 executor 1): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250106_to_20250110.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.sort_addToSorter_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 25 more\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "# =====================================================\n",
    "# NOTEBOOK 02: CREATE HIVE TABLES FOR GENERATED CDR DATA\n",
    "# Algerie Telecom Big Data Project - Advanced Analytics\n",
    "# =====================================================\n",
    "\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Spark with Hive support\n",
    "spark = init_spark(\"Hive Tables - Generated CDR Advanced\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üìä ALGERIE TELECOM - ADVANCED CDR DATA PIPELINE\")\n",
    "print(\"üìÖ Creating Comprehensive Hive Infrastructure\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# =====================================================\n",
    "# 1. CREATE ADVANCED DATABASE\n",
    "# =====================================================\n",
    "print(\"\\nüèóÔ∏è  CREATING ADVANCED DATABASE STRUCTURE...\")\n",
    "\n",
    "db_name = \"algerie_telecom_gen\"\n",
    "spark.sql(f\"DROP DATABASE IF EXISTS {db_name} CASCADE\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE DATABASE IF NOT EXISTS {db_name}\n",
    "    COMMENT 'Algerie Telecom Generated CDR - Advanced Analytics Platform'\n",
    "    LOCATION '/user/hive/warehouse/{db_name}.db'\n",
    "    WITH DBPROPERTIES (\n",
    "        'creator' = 'CDR Analytics Pipeline',\n",
    "        'version' = '2.0',\n",
    "        'date' = '{datetime.now().strftime(\"%Y-%m-%d\")}'\n",
    "    )\n",
    "\"\"\")\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "print(f\"‚úÖ Database '{db_name}' created successfully\")\n",
    "\n",
    "# =====================================================\n",
    "# 2. ANALYZE DATA STRUCTURE & QUALITY\n",
    "# =====================================================\n",
    "print(\"\\nüîç ANALYZING GENERATED DATA STRUCTURE...\")\n",
    "\n",
    "raw_path = \"/user/hive/warehouse/generated_raw_cdr/*.parquet\"\n",
    "df_sample = spark.read.parquet(raw_path)\n",
    "\n",
    "# Get schema information\n",
    "print(\"\\nüìã Data Schema Analysis:\")\n",
    "total_columns = len(df_sample.columns)\n",
    "print(f\"Total columns: {total_columns}\")\n",
    "\n",
    "# Categorize columns\n",
    "id_cols = ['cdr_id', 'subscriber_id', 'session_id']\n",
    "pii_cols = ['msisdn', 'imsi', 'imei', 'calling_party', 'called_party']\n",
    "service_cols = ['service_type', 'service_subtype']\n",
    "time_cols = ['start_time', 'end_time', 'duration']\n",
    "data_cols = ['data_volume_mb', 'upload_mb', 'download_mb']\n",
    "location_cols = ['cell_id', 'lac', 'location_area', 'serving_cell_tower']\n",
    "network_cols = ['network_type', 'operator', 'network_congestion_level']\n",
    "financial_cols = ['charging_amount', 'tax_amount', 'revenue_per_mb']\n",
    "customer_cols = ['customer_segment', 'tariff_plan', 'payment_type', 'age_group', 'gender', 'customer_lifetime_value_category']\n",
    "quality_cols = ['call_result', 'quality_score', 'signal_strength', 'dropped_call_flag']\n",
    "pattern_cols = ['time_of_day_category', 'day_of_week', 'is_weekend', 'is_holiday']\n",
    "anomaly_cols = ['fraud_indicator', 'unusual_pattern_flag']\n",
    "special_cols = ['special_offer_applied', 'promotional_discount']\n",
    "app_cols = ['application_used', 'content_category']\n",
    "roaming_cols = ['roaming_flag', 'roaming_country', 'roaming_type']\n",
    "\n",
    "# Check data quality\n",
    "print(\"\\nüìä Data Quality Metrics:\")\n",
    "total_records = df_sample.count()\n",
    "print(f\"Total records: {total_records:,}\")\n",
    "\n",
    "# Sample anonymization check\n",
    "sample_msisdn = df_sample.select(\"msisdn\").first()[0]\n",
    "is_anonymized = len(str(sample_msisdn)) == 16 and all(c in '0123456789abcdef' for c in str(sample_msisdn))\n",
    "print(f\"Anonymization status: {'‚úÖ Enabled' if is_anonymized else '‚ùå Disabled'}\")\n",
    "\n",
    "# =====================================================\n",
    "# 3. CREATE MAIN EXTERNAL TABLE\n",
    "# =====================================================\n",
    "print(\"\\nüì¶ CREATING MAIN EXTERNAL TABLE...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS cdr_raw (\n",
    "        -- Identifiers\n",
    "        cdr_id STRING COMMENT 'Unique CDR identifier',\n",
    "        subscriber_id STRING COMMENT 'Subscriber identifier',\n",
    "        msisdn STRING COMMENT 'Mobile number (anonymized)',\n",
    "        imsi STRING COMMENT 'IMSI (anonymized)',\n",
    "        imei STRING COMMENT 'Device IMEI (anonymized)',\n",
    "        \n",
    "        -- Service Information\n",
    "        service_type STRING COMMENT 'VOICE/DATA/SMS',\n",
    "        service_subtype STRING COMMENT 'Detailed service type',\n",
    "        session_id STRING COMMENT 'Session identifier',\n",
    "        \n",
    "        -- Call Details\n",
    "        calling_party STRING COMMENT 'Caller number (anonymized)',\n",
    "        called_party STRING COMMENT 'Called number (anonymized)',\n",
    "        start_time STRING COMMENT 'Call/session start time',\n",
    "        end_time STRING COMMENT 'Call/session end time',\n",
    "        duration INT COMMENT 'Duration in seconds',\n",
    "        \n",
    "        -- Data Usage\n",
    "        data_volume_mb DOUBLE COMMENT 'Total data volume in MB',\n",
    "        upload_mb DOUBLE COMMENT 'Upload volume in MB',\n",
    "        download_mb DOUBLE COMMENT 'Download volume in MB',\n",
    "        \n",
    "        -- Location & Network\n",
    "        cell_id STRING COMMENT 'Cell tower ID',\n",
    "        lac STRING COMMENT 'Location area code',\n",
    "        location_area STRING COMMENT 'Geographic location',\n",
    "        serving_cell_tower STRING COMMENT 'Serving tower ID',\n",
    "        network_type STRING COMMENT '2G/3G/4G/5G',\n",
    "        operator STRING COMMENT 'Mobilis/Djezzy/Ooredoo',\n",
    "        \n",
    "        -- Financial\n",
    "        charging_amount DOUBLE COMMENT 'Total charge in DZD',\n",
    "        currency STRING COMMENT 'Currency code',\n",
    "        payment_type STRING COMMENT 'PREPAID/POSTPAID',\n",
    "        tax_amount DOUBLE COMMENT 'Tax amount in DZD',\n",
    "        revenue_per_mb DOUBLE COMMENT 'Revenue per MB for data',\n",
    "        \n",
    "        -- Quality & Status\n",
    "        call_result STRING COMMENT 'SUCCESS/FAILED',\n",
    "        quality_score DOUBLE COMMENT 'Quality score 0-1',\n",
    "        signal_strength INT COMMENT 'Signal strength',\n",
    "        dropped_call_flag BOOLEAN COMMENT 'Call dropped flag',\n",
    "        network_congestion_level STRING COMMENT 'LOW/MEDIUM/HIGH',\n",
    "        \n",
    "        -- Customer Profile\n",
    "        customer_segment STRING COMMENT 'Premium/Standard/Basic',\n",
    "        tariff_plan STRING COMMENT 'Customer tariff plan',\n",
    "        age_group STRING COMMENT 'Age group category',\n",
    "        gender STRING COMMENT 'M/F',\n",
    "        customer_lifetime_value_category STRING COMMENT 'CLV category',\n",
    "        \n",
    "        -- Patterns & Analytics\n",
    "        time_of_day_category STRING COMMENT 'MORNING/AFTERNOON/EVENING/NIGHT',\n",
    "        day_of_week STRING COMMENT 'Day name',\n",
    "        is_weekend BOOLEAN COMMENT 'Weekend flag',\n",
    "        is_holiday BOOLEAN COMMENT 'Holiday flag',\n",
    "        \n",
    "        -- Special Features\n",
    "        special_offer_applied STRING COMMENT 'Applied offer name',\n",
    "        promotional_discount DOUBLE COMMENT 'Discount percentage',\n",
    "        application_used STRING COMMENT 'App name for data',\n",
    "        content_category STRING COMMENT 'Content type',\n",
    "        \n",
    "        -- Roaming\n",
    "        roaming_flag BOOLEAN COMMENT 'Roaming indicator',\n",
    "        roaming_country STRING COMMENT 'Roaming country',\n",
    "        roaming_type STRING COMMENT 'NONE/NATIONAL/INTERNATIONAL',\n",
    "        \n",
    "        -- Anomalies\n",
    "        fraud_indicator BOOLEAN COMMENT 'Fraud detection flag',\n",
    "        unusual_pattern_flag BOOLEAN COMMENT 'Unusual pattern detected'\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '/user/hive/warehouse/generated_raw_cdr'\n",
    "    TBLPROPERTIES (\n",
    "        'compression' = 'snappy',\n",
    "        'creator' = 'Advanced CDR Pipeline',\n",
    "        'external.table.purge' = 'false'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ External table 'cdr_raw' created\")\n",
    "\n",
    "# Verify and compute statistics\n",
    "spark.sql(\"ANALYZE TABLE cdr_raw COMPUTE STATISTICS\")\n",
    "row_count = spark.sql(\"SELECT COUNT(*) FROM cdr_raw\").collect()[0][0]\n",
    "print(f\"   Records loaded: {row_count:,}\")\n",
    "\n",
    "# =====================================================\n",
    "# 4. CREATE OPTIMIZED PARTITIONED TABLE\n",
    "# =====================================================\n",
    "print(\"\\nüóÇÔ∏è  CREATING PARTITIONED TABLE FOR PERFORMANCE...\")\n",
    "\n",
    "# Drop if exists and create new\n",
    "spark.sql(\"DROP TABLE IF EXISTS cdr_partitioned\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS cdr_partitioned (\n",
    "        -- All columns from raw table\n",
    "        cdr_id STRING,\n",
    "        subscriber_id STRING,\n",
    "        msisdn STRING,\n",
    "        imsi STRING,\n",
    "        imei STRING,\n",
    "        service_type STRING,\n",
    "        service_subtype STRING,\n",
    "        session_id STRING,\n",
    "        calling_party STRING,\n",
    "        called_party STRING,\n",
    "        start_time TIMESTAMP,  -- Note: converted to TIMESTAMP\n",
    "        end_time TIMESTAMP,\n",
    "        duration INT,\n",
    "        data_volume_mb DOUBLE,\n",
    "        upload_mb DOUBLE,\n",
    "        download_mb DOUBLE,\n",
    "        cell_id STRING,\n",
    "        lac STRING,\n",
    "        location_area STRING,\n",
    "        serving_cell_tower STRING,\n",
    "        network_type STRING,\n",
    "        operator STRING,\n",
    "        charging_amount DOUBLE,\n",
    "        currency STRING,\n",
    "        payment_type STRING,\n",
    "        tax_amount DOUBLE,\n",
    "        revenue_per_mb DOUBLE,\n",
    "        call_result STRING,\n",
    "        quality_score DOUBLE,\n",
    "        signal_strength INT,\n",
    "        dropped_call_flag BOOLEAN,\n",
    "        network_congestion_level STRING,\n",
    "        customer_segment STRING,\n",
    "        tariff_plan STRING,\n",
    "        age_group STRING,\n",
    "        gender STRING,\n",
    "        customer_lifetime_value_category STRING,\n",
    "        time_of_day_category STRING,\n",
    "        day_of_week STRING,\n",
    "        is_weekend BOOLEAN,\n",
    "        is_holiday BOOLEAN,\n",
    "        special_offer_applied STRING,\n",
    "        promotional_discount DOUBLE,\n",
    "        application_used STRING,\n",
    "        content_category STRING,\n",
    "        roaming_flag BOOLEAN,\n",
    "        roaming_country STRING,\n",
    "        roaming_type STRING,\n",
    "        fraud_indicator BOOLEAN,\n",
    "        unusual_pattern_flag BOOLEAN\n",
    "    )\n",
    "    PARTITIONED BY (year INT, month INT, day INT)\n",
    "    STORED AS PARQUET\n",
    "    TBLPROPERTIES (\n",
    "        'compression' = 'snappy',\n",
    "        'transactional' = 'false',\n",
    "        'auto.purge' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Partitioned table structure created\")\n",
    "\n",
    "# Configure for dynamic partitioning\n",
    "print(\"\\nüì• Loading data with dynamic partitioning...\")\n",
    "spark.sql(\"SET hive.exec.dynamic.partition = true\")\n",
    "spark.sql(\"SET hive.exec.dynamic.partition.mode = nonstrict\")\n",
    "spark.sql(\"SET hive.exec.max.dynamic.partitions = 10000\")\n",
    "spark.sql(\"SET hive.exec.max.dynamic.partitions.pernode = 1000\")\n",
    "\n",
    "# Insert with proper timestamp conversion and partitioning\n",
    "start_time = time.time()\n",
    "spark.sql(\"\"\"\n",
    "    INSERT OVERWRITE TABLE cdr_partitioned PARTITION(year, month, day)\n",
    "    SELECT \n",
    "        cdr_id,\n",
    "        subscriber_id,\n",
    "        msisdn,\n",
    "        imsi,\n",
    "        imei,\n",
    "        service_type,\n",
    "        service_subtype,\n",
    "        session_id,\n",
    "        calling_party,\n",
    "        called_party,\n",
    "        CAST(start_time AS TIMESTAMP) as start_time,\n",
    "        CAST(end_time AS TIMESTAMP) as end_time,\n",
    "        CAST(duration AS INT) as duration,\n",
    "        data_volume_mb,\n",
    "        upload_mb,\n",
    "        download_mb,\n",
    "        cell_id,\n",
    "        lac,\n",
    "        location_area,\n",
    "        serving_cell_tower,\n",
    "        network_type,\n",
    "        operator,\n",
    "        charging_amount,\n",
    "        currency,\n",
    "        payment_type,\n",
    "        tax_amount,\n",
    "        revenue_per_mb,\n",
    "        call_result,\n",
    "        quality_score,\n",
    "        signal_strength,\n",
    "        dropped_call_flag,\n",
    "        network_congestion_level,\n",
    "        customer_segment,\n",
    "        tariff_plan,\n",
    "        age_group,\n",
    "        gender,\n",
    "        customer_lifetime_value_category,\n",
    "        time_of_day_category,\n",
    "        day_of_week,\n",
    "        is_weekend,\n",
    "        is_holiday,\n",
    "        special_offer_applied,\n",
    "        promotional_discount,\n",
    "        application_used,\n",
    "        content_category,\n",
    "        roaming_flag,\n",
    "        roaming_country,\n",
    "        roaming_type,\n",
    "        fraud_indicator,\n",
    "        unusual_pattern_flag,\n",
    "        YEAR(CAST(start_time AS TIMESTAMP)) as year,\n",
    "        MONTH(CAST(start_time AS TIMESTAMP)) as month,\n",
    "        DAY(CAST(start_time AS TIMESTAMP)) as day\n",
    "    FROM cdr_raw\n",
    "\"\"\")\n",
    "\n",
    "load_time = time.time() - start_time\n",
    "print(f\"‚úÖ Data loaded in {load_time:.1f} seconds\")\n",
    "\n",
    "# Show partition statistics\n",
    "print(\"\\nüìä Partition Statistics:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        year, \n",
    "        month,\n",
    "        COUNT(DISTINCT day) as days,\n",
    "        COUNT(*) as records,\n",
    "        ROUND(SUM(charging_amount), 2) as revenue\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month\n",
    "    ORDER BY year, month\n",
    "\"\"\").show(12)\n",
    "\n",
    "# =====================================================\n",
    "# 5. CREATE OPTIMIZED ANALYTICAL VIEWS\n",
    "# =====================================================\n",
    "print(\"\\nüëÅÔ∏è  CREATING ADVANCED ANALYTICAL VIEWS...\")\n",
    "\n",
    "# Service-based views\n",
    "views_created = 0\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW voice_calls AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'VOICE'\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW data_sessions AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'DATA'\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW sms_records AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'SMS'\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "# Advanced analytical views\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW fraud_cases AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE fraud_indicator = true OR unusual_pattern_flag = true\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW network_issues AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE dropped_call_flag = true \n",
    "       OR call_result = 'FAILED'\n",
    "       OR network_congestion_level IN ('MEDIUM', 'HIGH')\n",
    "       OR quality_score < 0.5\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW high_value_customers AS\n",
    "    SELECT DISTINCT\n",
    "        subscriber_id,\n",
    "        customer_segment,\n",
    "        customer_lifetime_value_category,\n",
    "        tariff_plan,\n",
    "        payment_type,\n",
    "        age_group,\n",
    "        gender,\n",
    "        operator\n",
    "    FROM cdr_partitioned\n",
    "    WHERE customer_segment IN ('Premium')\n",
    "       OR customer_lifetime_value_category IN ('High', 'Very High')\n",
    "       OR payment_type = 'POSTPAID'\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW special_offers_usage AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE special_offer_applied != 'None' \n",
    "      AND promotional_discount > 0\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW roaming_records AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE roaming_flag = true\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW app_usage_data AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'DATA' \n",
    "      AND application_used IS NOT NULL \n",
    "      AND application_used != ''\n",
    "\"\"\")\n",
    "views_created += 1\n",
    "\n",
    "print(f\"‚úÖ Created {views_created} analytical views\")\n",
    "\n",
    "# =====================================================\n",
    "# 6. CREATE PRE-AGGREGATED TABLES\n",
    "# =====================================================\n",
    "print(\"\\nüìä CREATING PRE-AGGREGATED ANALYTICAL TABLES...\")\n",
    "\n",
    "# Daily KPIs by Service and Operator\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS daily_kpis AS\n",
    "    SELECT \n",
    "        year, month, day,\n",
    "        service_type,\n",
    "        operator,\n",
    "        customer_segment,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_subscribers,\n",
    "        COUNT(*) as total_transactions,\n",
    "        SUM(duration) as total_duration_seconds,\n",
    "        SUM(data_volume_mb) as total_data_mb,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        SUM(tax_amount) as total_tax,\n",
    "        AVG(quality_score) as avg_quality_score,\n",
    "        SUM(CASE WHEN fraud_indicator THEN 1 ELSE 0 END) as fraud_cases,\n",
    "        SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_calls,\n",
    "        SUM(CASE WHEN special_offer_applied != 'None' THEN 1 ELSE 0 END) as special_offer_usage,\n",
    "        AVG(promotional_discount) as avg_discount_applied\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month, day, service_type, operator, customer_segment\n",
    "\"\"\")\n",
    "\n",
    "# Hourly Usage Patterns\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hourly_patterns AS\n",
    "    SELECT \n",
    "        time_of_day_category,\n",
    "        day_of_week,\n",
    "        is_weekend,\n",
    "        service_type,\n",
    "        operator,\n",
    "        COUNT(*) as transaction_count,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_users,\n",
    "        AVG(duration) as avg_duration,\n",
    "        AVG(data_volume_mb) as avg_data_mb,\n",
    "        AVG(charging_amount) as avg_revenue,\n",
    "        AVG(quality_score) as avg_quality,\n",
    "        SUM(CASE WHEN network_congestion_level = 'HIGH' THEN 1 ELSE 0 END) as high_congestion_count\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY time_of_day_category, day_of_week, is_weekend, service_type, operator\n",
    "\"\"\")\n",
    "\n",
    "# Location Performance Metrics\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS location_network_metrics AS\n",
    "    SELECT \n",
    "        location_area,\n",
    "        network_type,\n",
    "        operator,\n",
    "        service_type,\n",
    "        COUNT(*) as total_transactions,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_subscribers,\n",
    "        AVG(quality_score) as avg_quality_score,\n",
    "        AVG(signal_strength) as avg_signal_strength,\n",
    "        SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_count,\n",
    "        SUM(CASE WHEN call_result = 'FAILED' THEN 1 ELSE 0 END) as failed_count,\n",
    "        SUM(CASE WHEN network_congestion_level = 'HIGH' THEN 1 ELSE 0 END) as high_congestion_count,\n",
    "        AVG(data_volume_mb) as avg_data_usage,\n",
    "        SUM(charging_amount) as total_revenue\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY location_area, network_type, operator, service_type\n",
    "\"\"\")\n",
    "\n",
    "# Customer Demographics Analysis\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customer_demographics_summary AS\n",
    "    SELECT \n",
    "        customer_segment,\n",
    "        age_group,\n",
    "        gender,\n",
    "        payment_type,\n",
    "        operator,\n",
    "        COUNT(DISTINCT subscriber_id) as subscriber_count,\n",
    "        COUNT(*) as total_activities,\n",
    "        AVG(charging_amount) as avg_transaction_value,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        AVG(data_volume_mb) as avg_data_usage,\n",
    "        SUM(CASE WHEN fraud_indicator THEN 1 ELSE 0 END) as fraud_incidents,\n",
    "        AVG(promotional_discount) as avg_discount_received\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY customer_segment, age_group, gender, payment_type, operator\n",
    "\"\"\")\n",
    "\n",
    "# App Usage Analytics\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS app_usage_analytics AS\n",
    "    SELECT \n",
    "        application_used,\n",
    "        content_category,\n",
    "        customer_segment,\n",
    "        age_group,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_users,\n",
    "        COUNT(*) as total_sessions,\n",
    "        SUM(data_volume_mb) as total_data_mb,\n",
    "        AVG(data_volume_mb) as avg_data_per_session,\n",
    "        SUM(duration) as total_duration,\n",
    "        AVG(duration) as avg_session_duration,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        AVG(revenue_per_mb) as avg_revenue_per_mb\n",
    "    FROM cdr_partitioned\n",
    "    WHERE service_type = 'DATA' AND application_used IS NOT NULL\n",
    "    GROUP BY application_used, content_category, customer_segment, age_group\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created 5 pre-aggregated analytical tables\")\n",
    "\n",
    "# =====================================================\n",
    "# 7. CREATE MATERIALIZED VIEWS FOR DASHBOARDS\n",
    "# =====================================================\n",
    "print(\"\\nüìà CREATING MATERIALIZED VIEWS FOR REAL-TIME ANALYTICS...\")\n",
    "\n",
    "# Real-time fraud monitoring\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fraud_monitoring AS\n",
    "    SELECT \n",
    "        DATE(start_time) as fraud_date,\n",
    "        operator,\n",
    "        location_area,\n",
    "        COUNT(*) as total_incidents,\n",
    "        COUNT(DISTINCT subscriber_id) as affected_subscribers,\n",
    "        SUM(charging_amount) as potential_loss,\n",
    "        COLLECT_SET(service_type) as affected_services\n",
    "    FROM cdr_partitioned\n",
    "    WHERE fraud_indicator = true OR unusual_pattern_flag = true\n",
    "    GROUP BY DATE(start_time), operator, location_area\n",
    "\"\"\")\n",
    "\n",
    "# Revenue tracking\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS revenue_tracking AS\n",
    "    SELECT \n",
    "        year, month, day,\n",
    "        operator,\n",
    "        service_type,\n",
    "        customer_segment,\n",
    "        payment_type,\n",
    "        SUM(charging_amount) as gross_revenue,\n",
    "        SUM(tax_amount) as tax_collected,\n",
    "        SUM(charging_amount - tax_amount) as net_revenue,\n",
    "        COUNT(DISTINCT subscriber_id) as active_customers,\n",
    "        COUNT(*) as total_transactions,\n",
    "        AVG(charging_amount) as arpu_daily\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month, day, operator, service_type, customer_segment, payment_type\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Created materialized views for dashboards\")\n",
    "\n",
    "# =====================================================\n",
    "# 8. COMPUTE COMPREHENSIVE STATISTICS\n",
    "# =====================================================\n",
    "print(\"\\nüìà COMPUTING TABLE STATISTICS FOR OPTIMIZATION...\")\n",
    "\n",
    "tables_to_analyze = [\n",
    "    'cdr_partitioned', 'daily_kpis', 'hourly_patterns', \n",
    "    'location_network_metrics', 'customer_demographics_summary',\n",
    "    'app_usage_analytics', 'fraud_monitoring', 'revenue_tracking'\n",
    "]\n",
    "\n",
    "for table in tables_to_analyze:\n",
    "    spark.sql(f\"ANALYZE TABLE {table} COMPUTE STATISTICS\")\n",
    "    spark.sql(f\"ANALYZE TABLE {table} COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
    "    print(f\"   ‚úÖ Analyzed {table}\")\n",
    "\n",
    "# =====================================================\n",
    "# 9. FINAL VERIFICATION AND SUMMARY\n",
    "# =====================================================\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"üìä HIVE INFRASTRUCTURE SETUP COMPLETE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Summary of created objects\n",
    "print(\"\\nüìã Database Objects Created:\")\n",
    "\n",
    "# Tables\n",
    "print(\"\\nüì¶ Tables:\")\n",
    "tables = spark.sql(f\"SHOW TABLES IN {db_name}\").filter(\"isTemporary = false\").collect()\n",
    "for table in tables:\n",
    "    count = spark.sql(f\"SELECT COUNT(*) FROM {table.tableName}\").collect()[0][0]\n",
    "    print(f\"   - {table.tableName}: {count:,} records\")\n",
    "\n",
    "# Views\n",
    "print(\"\\nüëÅÔ∏è  Views:\")\n",
    "views = spark.sql(\"SHOW VIEWS\").collect()\n",
    "for view in views:\n",
    "    print(f\"   - {view.viewName}\")\n",
    "\n",
    "# Performance test\n",
    "print(\"\\n‚ö° Performance Test Query:\")\n",
    "test_start = time.time()\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        operator,\n",
    "        service_type,\n",
    "        COUNT(*) as count,\n",
    "        ROUND(SUM(charging_amount), 2) as revenue\n",
    "    FROM cdr_partitioned\n",
    "    WHERE year = 2025 AND month = 1\n",
    "    GROUP BY operator, service_type\n",
    "    ORDER BY operator, service_type\n",
    "\"\"\").show()\n",
    "test_time = time.time() - test_start\n",
    "print(f\"Query executed in {test_time:.2f} seconds\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   1. Run Notebook 03 for Advanced Data Engineering\")\n",
    "print(\"   2. Run Notebook 04 for Anomaly Detection & Trend Analysis\")\n",
    "print(\"   3. Run Notebook 05 for Business Intelligence Dashboards\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    \"setup_date\": datetime.now().isoformat(),\n",
    "    \"database\": db_name,\n",
    "    \"total_records\": row_count,\n",
    "    \"tables_created\": len(tables),\n",
    "    \"views_created\": views_created,\n",
    "    \"load_time_seconds\": load_time\n",
    "}\n",
    "\n",
    "print(\"\\nüìä Setup Metadata:\")\n",
    "for key, value in metadata.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nüîö Spark session closed successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b3a52e5-722a-47ec-b11e-ac6f1a745fe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: Check - Generated CDR Advanced, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "+-------------------+-----------------------------+-----------+\n",
      "|namespace          |tableName                    |isTemporary|\n",
      "+-------------------+-----------------------------+-----------+\n",
      "|algerie_telecom_gen|cdr_raw                      |false      |\n",
      "|algerie_telecom_gen|cdr_partitioned              |false      |\n",
      "|algerie_telecom_gen|voice_calls                  |false      |\n",
      "|algerie_telecom_gen|data_sessions                |false      |\n",
      "|algerie_telecom_gen|sms_records                  |false      |\n",
      "|algerie_telecom_gen|fraud_cases                  |false      |\n",
      "|algerie_telecom_gen|network_issues               |false      |\n",
      "|algerie_telecom_gen|high_value_customers         |false      |\n",
      "|algerie_telecom_gen|special_offers_usage         |false      |\n",
      "|algerie_telecom_gen|roaming_records              |false      |\n",
      "|algerie_telecom_gen|app_usage_data               |false      |\n",
      "|algerie_telecom_gen|daily_kpis                   |false      |\n",
      "|algerie_telecom_gen|hourly_patterns              |false      |\n",
      "|algerie_telecom_gen|location_network_metrics     |false      |\n",
      "|algerie_telecom_gen|customer_demographics_summary|false      |\n",
      "|algerie_telecom_gen|app_usage_analytics          |false      |\n",
      "|algerie_telecom_gen|fraud_monitoring             |false      |\n",
      "|algerie_telecom_gen|revenue_tracking             |false      |\n",
      "+-------------------+-----------------------------+-----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|count(1)|\n",
      "+--------+\n",
      "|       0|\n",
      "+--------+\n",
      "\n",
      "+------+-------------+------+----+----+------------+---------------+----------+-------------+------------+----------+--------+--------+---------------+--------------+---------+-----------+---------------+----------+--------------+-------------+--------------------+-------+---+-------------+------------------+------------+--------+------------+-----------+----------------+-----------+--------+---------+------+---------------+------------+---------------------+------------------------+--------------------+-----------+----------------+----------------+--------------------------------+-----------------+------------+---------------+--------------------+----------+----------+----+-----+---+\n",
      "|cdr_id|subscriber_id|msisdn|imsi|imei|service_type|service_subtype|session_id|calling_party|called_party|start_time|end_time|duration|signal_strength|data_volume_mb|upload_mb|download_mb|charging_amount|tax_amount|revenue_per_mb|quality_score|promotional_discount|cell_id|lac|location_area|serving_cell_tower|network_type|currency|payment_type|call_result|customer_segment|tariff_plan|operator|age_group|gender|roaming_country|roaming_type|special_offer_applied|network_congestion_level|time_of_day_category|day_of_week|application_used|content_category|customer_lifetime_value_category|dropped_call_flag|roaming_flag|fraud_indicator|unusual_pattern_flag|is_weekend|is_holiday|year|month|day|\n",
      "+------+-------------+------+----+----+------------+---------------+----------+-------------+------------+----------+--------+--------+---------------+--------------+---------+-----------+---------------+----------+--------------+-------------+--------------------+-------+---+-------------+------------------+------------+--------+------------+-----------+----------------+-----------+--------+---------+------+---------------+------------+---------------------+------------------------+--------------------+-----------+----------------+----------------+--------------------------------+-----------------+------------+---------------+--------------------+----------+----------+----+-----+---+\n",
      "+------+-------------+------+----+----+------------+---------------+----------+-------------+------------+----------+--------+--------+---------------+--------------+---------+-----------+---------------+----------+--------------+-------------+--------------------+-------+---+-------------+------------------+------------+--------+------------+-----------+----------------+-----------+--------+---------+------+---------------+------------+---------------------+------------------------+--------------------+-----------+----------------+----------------+--------------------------------+-----------------+------------+---------------+--------------------+----------+----------+----+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Spark with Hive support\n",
    "spark = init_spark(\"Check - Generated CDR Advanced\")\n",
    "spark.sql(\"USE algerie_telecom_gen\")\n",
    "spark.sql(\"SHOW TABLES IN algerie_telecom_gen\").show(100, truncate=False)\n",
    "spark.sql(\"SELECT COUNT(*) FROM cdr_partitioned\").show()\n",
    "spark.sql(\"SELECT * FROM cdr_partitioned LIMIT 10\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9dda5d2f-87e3-44b7-993f-caf65b3226d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- cdr_id: string (nullable = true)\n",
      " |-- subscriber_id: string (nullable = true)\n",
      " |-- msisdn: string (nullable = true)\n",
      " |-- imsi: string (nullable = true)\n",
      " |-- imei: string (nullable = true)\n",
      " |-- service_type: string (nullable = true)\n",
      " |-- service_subtype: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- calling_party: string (nullable = true)\n",
      " |-- called_party: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- data_volume_mb: double (nullable = true)\n",
      " |-- upload_mb: double (nullable = true)\n",
      " |-- download_mb: double (nullable = true)\n",
      " |-- cell_id: string (nullable = true)\n",
      " |-- lac: string (nullable = true)\n",
      " |-- location_area: string (nullable = true)\n",
      " |-- serving_cell_tower: string (nullable = true)\n",
      " |-- network_type: string (nullable = true)\n",
      " |-- charging_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- tax_amount: double (nullable = true)\n",
      " |-- call_result: string (nullable = true)\n",
      " |-- quality_score: double (nullable = true)\n",
      " |-- signal_strength: long (nullable = true)\n",
      " |-- dropped_call_flag: boolean (nullable = true)\n",
      " |-- customer_segment: string (nullable = true)\n",
      " |-- tariff_plan: string (nullable = true)\n",
      " |-- operator: string (nullable = true)\n",
      " |-- age_group: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- roaming_flag: boolean (nullable = true)\n",
      " |-- roaming_country: string (nullable = true)\n",
      " |-- roaming_type: string (nullable = true)\n",
      " |-- special_offer_applied: string (nullable = true)\n",
      " |-- promotional_discount: double (nullable = true)\n",
      " |-- network_congestion_level: string (nullable = true)\n",
      " |-- fraud_indicator: boolean (nullable = true)\n",
      " |-- unusual_pattern_flag: boolean (nullable = true)\n",
      " |-- time_of_day_category: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_holiday: boolean (nullable = true)\n",
      " |-- application_used: string (nullable = true)\n",
      " |-- content_category: string (nullable = true)\n",
      " |-- revenue_per_mb: double (nullable = true)\n",
      " |-- customer_lifetime_value_category: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250111_to_20250115.parquet\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bdd7d35e-21cc-4aa4-8f09-53848377e4ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdr_raw:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|146876149|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 15:59:41 WARN TaskSetManager: Lost task 0.0 in stage 5.0 (TID 111) (172.30.0.34 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250101_to_20250105.parquet. Column: [duration], Expected: int, Found: INT64.\n",
      "\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n",
      "\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n",
      "\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n",
      "\t... 23 more\n",
      "\n",
      "25/06/22 15:59:42 ERROR TaskSetManager: Task 0 in stage 5.0 failed 4 times; aborting job\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o573.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 114) (172.30.0.34 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250101_to_20250105.parquet. Column: [duration], Expected: int, Found: INT64.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250101_to_20250105.parquet. Column: [duration], Expected: int, Found: INT64.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 23 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcdr_raw:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT COUNT(*) FROM cdr_raw\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m----> 3\u001b[0m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM cdr_raw LIMIT 5\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshow\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtruncate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcdr_partitioned:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSELECT COUNT(*) FROM cdr_partitioned\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mshow()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:945\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mshow\u001b[39m(\u001b[38;5;28mself\u001b[39m, n: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20\u001b[39m, truncate: Union[\u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, vertical: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    886\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Prints the first ``n`` rows to the console.\u001b[39;00m\n\u001b[1;32m    887\u001b[0m \n\u001b[1;32m    888\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    943\u001b[0m \u001b[38;5;124;03m    name | Bob\u001b[39;00m\n\u001b[1;32m    944\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 945\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_show_string\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtruncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/sql/dataframe.py:976\u001b[0m, in \u001b[0;36mDataFrame._show_string\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    967\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[1;32m    968\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m PySparkTypeError(\n\u001b[1;32m    969\u001b[0m         error_class\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNOT_BOOL\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    970\u001b[0m         message_parameters\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    973\u001b[0m         },\n\u001b[1;32m    974\u001b[0m     )\n\u001b[0;32m--> 976\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshowString\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint_truncate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvertical\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o573.showString.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 5.0 failed 4 times, most recent failure: Lost task 0.3 in stage 5.0 (TID 114) (172.30.0.34 executor 0): org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250101_to_20250105.parquet. Column: [duration], Expected: int, Found: INT64.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 23 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n\tat scala.Option.foreach(Option.scala:407)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2419)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2438)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:530)\n\tat org.apache.spark.sql.execution.SparkPlan.executeTake(SparkPlan.scala:483)\n\tat org.apache.spark.sql.execution.CollectLimitExec.executeCollect(limit.scala:61)\n\tat org.apache.spark.sql.Dataset.collectFromPlan(Dataset.scala:4332)\n\tat org.apache.spark.sql.Dataset.$anonfun$head$1(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$2(Dataset.scala:4322)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:546)\n\tat org.apache.spark.sql.Dataset.$anonfun$withAction$1(Dataset.scala:4320)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n\tat org.apache.spark.sql.Dataset.withAction(Dataset.scala:4320)\n\tat org.apache.spark.sql.Dataset.head(Dataset.scala:3314)\n\tat org.apache.spark.sql.Dataset.take(Dataset.scala:3537)\n\tat org.apache.spark.sql.Dataset.getRows(Dataset.scala:280)\n\tat org.apache.spark.sql.Dataset.showString(Dataset.scala:315)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: org.apache.spark.SparkException: Parquet column cannot be converted in file hdfs://namenode:9000/user/hive/warehouse/generated_raw_cdr/cdr_20250101_to_20250105.parquet. Column: [duration], Expected: int, Found: INT64.\n\tat org.apache.spark.sql.errors.QueryExecutionErrors$.unsupportedSchemaColumnConvertError(QueryExecutionErrors.scala:854)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:287)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.FileSourceScanExec$$anon$1.hasNext(DataSourceScanExec.scala:593)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.columnartorow_nextBatch_0$(Unknown Source)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n\tat org.apache.spark.sql.execution.SparkPlan.$anonfun$getByteArrayRdd$1(SparkPlan.scala:388)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n\tat java.base/java.lang.Thread.run(Thread.java:840)\nCaused by: org.apache.spark.sql.execution.datasources.SchemaColumnConvertNotSupportedException: column: [duration], physicalType: INT64, logicalType: int\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.constructConvertNotSupportedException(ParquetVectorUpdaterFactory.java:1136)\n\tat org.apache.spark.sql.execution.datasources.parquet.ParquetVectorUpdaterFactory.getUpdater(ParquetVectorUpdaterFactory.java:199)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedColumnReader.readBatch(VectorizedColumnReader.java:175)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextBatch(VectorizedParquetRecordReader.java:342)\n\tat org.apache.spark.sql.execution.datasources.parquet.VectorizedParquetRecordReader.nextKeyValue(VectorizedParquetRecordReader.java:233)\n\tat org.apache.spark.sql.execution.datasources.RecordReaderIterator.hasNext(RecordReaderIterator.scala:39)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.hasNext(FileScanRDD.scala:129)\n\tat org.apache.spark.sql.execution.datasources.FileScanRDD$$anon$1.nextIterator(FileScanRDD.scala:283)\n\t... 23 more\n"
     ]
    }
   ],
   "source": [
    "print(\"cdr_raw:\")\n",
    "spark.sql(\"SELECT COUNT(*) FROM cdr_raw\").show()\n",
    "spark.sql(\"SELECT * FROM cdr_raw LIMIT 5\").show(truncate=False)\n",
    "\n",
    "print(\"cdr_partitioned:\")\n",
    "spark.sql(\"SELECT COUNT(*) FROM cdr_partitioned\").show()\n",
    "spark.sql(\"SELECT * FROM cdr_partitioned LIMIT 5\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579d1d3f-9527-48c3-890e-50b7d07b182c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146876149\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+---------------+--------+------------+----------+-----------+-------------+---------------+-----------------+----------------+-------------+--------+---------+------+------------+---------------+------------+---------------------+--------------------+------------------------+---------------+--------------------+--------------------+-----------+----------+----------+----------------+----------------+--------------+--------------------------------+\n",
      "|cdr_id                              |subscriber_id|msisdn          |imsi            |imei            |service_type|service_subtype |session_id    |calling_party   |called_party    |start_time         |end_time           |duration|data_volume_mb|upload_mb|download_mb|cell_id    |lac    |location_area |serving_cell_tower|network_type|charging_amount|currency|payment_type|tax_amount|call_result|quality_score|signal_strength|dropped_call_flag|customer_segment|tariff_plan  |operator|age_group|gender|roaming_flag|roaming_country|roaming_type|special_offer_applied|promotional_discount|network_congestion_level|fraud_indicator|unusual_pattern_flag|time_of_day_category|day_of_week|is_weekend|is_holiday|application_used|content_category|revenue_per_mb|customer_lifetime_value_category|\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+---------------+--------+------------+----------+-----------+-------------+---------------+-----------------+----------------+-------------+--------+---------+------+------------+---------------+------------+---------------------+--------------------+------------------------+---------------+--------------------+--------------------+-----------+----------+----------+----------------+----------------+--------------+--------------------------------+\n",
      "|1ed3fa8c-02fa-4ac4-8522-044ea493e948|AT00019248   |89d7b44045d23836|02ba595c4881d2fe|912205004a7a302f|VOICE       |OUTGOING_CALL   |VOICE_8e71ccd8|89d7b44045d23836|e2c78d4fcc154bfd|2025-01-04 09:49:17|2025-01-04 09:50:32|75      |0.0           |0.0      |0.0        |60302954013|LAC5401|Khenchela     |Tower_954013      |4G          |5.95           |DZD     |PREPAID     |0.95      |SUCCESS    |0.95         |80             |false            |youth           |Mobilis Smart|Djezzy  |26-35    |F     |false       |               |NONE        |Weekend_Family       |20.0                |LOW                     |false          |false               |MORNING             |Saturday   |true      |false     |                |                |0.0           |High                            |\n",
      "|c1325ba8-a8c1-40b9-a3e7-2b0f3048ae5a|AT00019678   |907aa3f59e83148e|f736555d07010481|d73fadb95eaaf7a1|DATA        |INTERNET_SESSION|DATA_48b59838 |907aa3f59e83148e|                |2025-01-04 08:50:13|2025-01-04 13:05:49|15336   |128.12        |12.81    |115.31     |60302268065|LAC6806|Chlef         |Tower_268065      |4G          |304.93         |DZD     |PREPAID     |48.69     |SUCCESS    |0.95         |71             |false            |business        |Jeune Connect|Djezzy  |18-25    |F     |false       |               |NONE        |Weekend_Family       |20.0                |LOW                     |false          |false               |MORNING             |Saturday   |true      |false     |Spotify         |music           |2.38          |Very High                       |\n",
      "|2c8d0480-35f1-4ac1-b5ce-b0cdda731746|AT00094835   |ad763ea4e289697f|930135f977112d90|c3cd9637b7e6d26b|DATA        |INTERNET_SESSION|DATA_266f7f67 |ad763ea4e289697f|                |2025-01-03 01:34:48|2025-01-03 06:38:48|18240   |287.98        |28.8     |259.18     |60301887051|LAC8705|Tiaret        |Tower_887051      |4G          |642.56         |DZD     |POSTPAID    |102.59    |SUCCESS    |0.95         |63             |false            |premium         |Jeune Connect|Mobilis |50+      |M     |false       |               |NONE        |Nouvel_An_2025       |25.0                |LOW                     |false          |false               |NIGHT               |Friday     |false     |false     |TikTok          |video           |2.23          |Very High                       |\n",
      "|f1c7c0ce-faa3-4920-b3b3-e3b8810214dc|AT00008349   |0029aca7fb1bc6e3|ad6efbabe9beb432|50c7f6f42dcb9b0e|DATA        |INTERNET_SESSION|DATA_2c6915b3 |0029aca7fb1bc6e3|                |2025-01-02 16:17:51|2025-01-03 01:02:51|31500   |160.44        |16.04    |144.4      |60301934022|LAC3402|Oum El Bouaghi|Tower_934022      |3G          |357.98         |DZD     |PREPAID     |57.16     |SUCCESS    |0.95         |99             |false            |business        |Mobilis Smart|Mobilis |26-35    |M     |false       |               |NONE        |Nouvel_An_2025       |25.0                |LOW                     |false          |false               |AFTERNOON           |Thursday   |false     |false     |Spotify         |music           |2.23          |Medium                          |\n",
      "|2333279d-5ef3-4bb0-8327-ab7b044949b4|AT00020140   |cceaa5199f3038a8|8d252bcea9f5dd6f|1e64848b154abb3f|DATA        |INTERNET_SESSION|DATA_ea6b8cf9 |cceaa5199f3038a8|                |2025-01-02 21:37:32|2025-01-03 02:25:02|17250   |414.45        |41.45    |373.0      |60301346015|LAC4601|Mostaganem    |Tower_346015      |3G          |924.74         |DZD     |PREPAID     |147.65    |SUCCESS    |0.95         |90             |false            |business        |Mobilis Smart|Mobilis |26-35    |M     |false       |               |NONE        |Nouvel_An_2025       |25.0                |LOW                     |false          |false               |EVENING             |Thursday   |false     |false     |TikTok          |video           |2.23          |Very High                       |\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+---------------+--------+------------+----------+-----------+-------------+---------------+-----------------+----------------+-------------+--------+---------+------+------------+---------------+------------+---------------------+--------------------+------------------------+---------------+--------------------+--------------------+-----------+----------+----------+----------------+----------------+--------------+--------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.parquet(\"/user/hive/warehouse/generated_raw_cdr/*.parquet\")\n",
    "print(df.count())\n",
    "df.show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "599a731f-dacd-4b07-a286-2e47e3856c39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 16:07:51 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS cdr_raw\")\n",
    "spark.sql(\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS cdr_raw (\n",
    "        -- Identifiers\n",
    "        cdr_id STRING,\n",
    "        subscriber_id STRING,\n",
    "        msisdn STRING,\n",
    "        imsi STRING,\n",
    "        imei STRING,\n",
    "        \n",
    "        -- Service Information\n",
    "        service_type STRING,\n",
    "        service_subtype STRING,\n",
    "        session_id STRING,\n",
    "        \n",
    "        -- Call Details\n",
    "        calling_party STRING,\n",
    "        called_party STRING,\n",
    "        start_time STRING,\n",
    "        end_time STRING,\n",
    "        duration BIGINT,  -- ‚Üê FIXED! Was INT, now BIGINT\n",
    "\n",
    "        -- Data Usage\n",
    "        data_volume_mb DOUBLE,\n",
    "        upload_mb DOUBLE,\n",
    "        download_mb DOUBLE,\n",
    "\n",
    "        -- Location & Network\n",
    "        cell_id STRING,\n",
    "        lac STRING,\n",
    "        location_area STRING,\n",
    "        serving_cell_tower STRING,\n",
    "        network_type STRING,\n",
    "        operator STRING,\n",
    "\n",
    "        -- Financial\n",
    "        charging_amount DOUBLE,\n",
    "        currency STRING,\n",
    "        payment_type STRING,\n",
    "        tax_amount DOUBLE,\n",
    "        revenue_per_mb DOUBLE,\n",
    "\n",
    "        -- Quality & Status\n",
    "        call_result STRING,\n",
    "        quality_score DOUBLE,\n",
    "        signal_strength BIGINT, -- (if Parquet is INT64, else leave INT)\n",
    "        dropped_call_flag BOOLEAN,\n",
    "        network_congestion_level STRING,\n",
    "\n",
    "        -- Customer Profile\n",
    "        customer_segment STRING,\n",
    "        tariff_plan STRING,\n",
    "        age_group STRING,\n",
    "        gender STRING,\n",
    "        customer_lifetime_value_category STRING,\n",
    "\n",
    "        -- Patterns & Analytics\n",
    "        time_of_day_category STRING,\n",
    "        day_of_week STRING,\n",
    "        is_weekend BOOLEAN,\n",
    "        is_holiday BOOLEAN,\n",
    "\n",
    "        -- Special Features\n",
    "        special_offer_applied STRING,\n",
    "        promotional_discount DOUBLE,\n",
    "        application_used STRING,\n",
    "        content_category STRING,\n",
    "\n",
    "        -- Roaming\n",
    "        roaming_flag BOOLEAN,\n",
    "        roaming_country STRING,\n",
    "        roaming_type STRING,\n",
    "\n",
    "        -- Anomalies\n",
    "        fraud_indicator BOOLEAN,\n",
    "        unusual_pattern_flag BOOLEAN\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '/user/hive/warehouse/generated_raw_cdr'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d437aae-5d18-43f6-a3f9-496e32edb506",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"ANALYZE TABLE cdr_raw COMPUTE STATISTICS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cf4384c2-9f35-46fc-a55c-2e09ae6e85e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|146876149|\n",
      "+---------+\n",
      "\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+--------+---------------+--------+------------+----------+--------------+-----------+-------------+---------------+-----------------+------------------------+----------------+-------------+---------+------+--------------------------------+--------------------+-----------+----------+----------+---------------------+--------------------+----------------+----------------+------------+---------------+------------+---------------+--------------------+\n",
      "|cdr_id                              |subscriber_id|msisdn          |imsi            |imei            |service_type|service_subtype |session_id    |calling_party   |called_party    |start_time         |end_time           |duration|data_volume_mb|upload_mb|download_mb|cell_id    |lac    |location_area |serving_cell_tower|network_type|operator|charging_amount|currency|payment_type|tax_amount|revenue_per_mb|call_result|quality_score|signal_strength|dropped_call_flag|network_congestion_level|customer_segment|tariff_plan  |age_group|gender|customer_lifetime_value_category|time_of_day_category|day_of_week|is_weekend|is_holiday|special_offer_applied|promotional_discount|application_used|content_category|roaming_flag|roaming_country|roaming_type|fraud_indicator|unusual_pattern_flag|\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+--------+---------------+--------+------------+----------+--------------+-----------+-------------+---------------+-----------------+------------------------+----------------+-------------+---------+------+--------------------------------+--------------------+-----------+----------+----------+---------------------+--------------------+----------------+----------------+------------+---------------+------------+---------------+--------------------+\n",
      "|1ed3fa8c-02fa-4ac4-8522-044ea493e948|AT00019248   |89d7b44045d23836|02ba595c4881d2fe|912205004a7a302f|VOICE       |OUTGOING_CALL   |VOICE_8e71ccd8|89d7b44045d23836|e2c78d4fcc154bfd|2025-01-04 09:49:17|2025-01-04 09:50:32|75      |0.0           |0.0      |0.0        |60302954013|LAC5401|Khenchela     |Tower_954013      |4G          |Djezzy  |5.95           |DZD     |PREPAID     |0.95      |0.0           |SUCCESS    |0.95         |80             |false            |LOW                     |youth           |Mobilis Smart|26-35    |F     |High                            |MORNING             |Saturday   |true      |false     |Weekend_Family       |20.0                |                |                |false       |               |NONE        |false          |false               |\n",
      "|c1325ba8-a8c1-40b9-a3e7-2b0f3048ae5a|AT00019678   |907aa3f59e83148e|f736555d07010481|d73fadb95eaaf7a1|DATA        |INTERNET_SESSION|DATA_48b59838 |907aa3f59e83148e|                |2025-01-04 08:50:13|2025-01-04 13:05:49|15336   |128.12        |12.81    |115.31     |60302268065|LAC6806|Chlef         |Tower_268065      |4G          |Djezzy  |304.93         |DZD     |PREPAID     |48.69     |2.38          |SUCCESS    |0.95         |71             |false            |LOW                     |business        |Jeune Connect|18-25    |F     |Very High                       |MORNING             |Saturday   |true      |false     |Weekend_Family       |20.0                |Spotify         |music           |false       |               |NONE        |false          |false               |\n",
      "|2c8d0480-35f1-4ac1-b5ce-b0cdda731746|AT00094835   |ad763ea4e289697f|930135f977112d90|c3cd9637b7e6d26b|DATA        |INTERNET_SESSION|DATA_266f7f67 |ad763ea4e289697f|                |2025-01-03 01:34:48|2025-01-03 06:38:48|18240   |287.98        |28.8     |259.18     |60301887051|LAC8705|Tiaret        |Tower_887051      |4G          |Mobilis |642.56         |DZD     |POSTPAID    |102.59    |2.23          |SUCCESS    |0.95         |63             |false            |LOW                     |premium         |Jeune Connect|50+      |M     |Very High                       |NIGHT               |Friday     |false     |false     |Nouvel_An_2025       |25.0                |TikTok          |video           |false       |               |NONE        |false          |false               |\n",
      "|f1c7c0ce-faa3-4920-b3b3-e3b8810214dc|AT00008349   |0029aca7fb1bc6e3|ad6efbabe9beb432|50c7f6f42dcb9b0e|DATA        |INTERNET_SESSION|DATA_2c6915b3 |0029aca7fb1bc6e3|                |2025-01-02 16:17:51|2025-01-03 01:02:51|31500   |160.44        |16.04    |144.4      |60301934022|LAC3402|Oum El Bouaghi|Tower_934022      |3G          |Mobilis |357.98         |DZD     |PREPAID     |57.16     |2.23          |SUCCESS    |0.95         |99             |false            |LOW                     |business        |Mobilis Smart|26-35    |M     |Medium                          |AFTERNOON           |Thursday   |false     |false     |Nouvel_An_2025       |25.0                |Spotify         |music           |false       |               |NONE        |false          |false               |\n",
      "|2333279d-5ef3-4bb0-8327-ab7b044949b4|AT00020140   |cceaa5199f3038a8|8d252bcea9f5dd6f|1e64848b154abb3f|DATA        |INTERNET_SESSION|DATA_ea6b8cf9 |cceaa5199f3038a8|                |2025-01-02 21:37:32|2025-01-03 02:25:02|17250   |414.45        |41.45    |373.0      |60301346015|LAC4601|Mostaganem    |Tower_346015      |3G          |Mobilis |924.74         |DZD     |PREPAID     |147.65    |2.23          |SUCCESS    |0.95         |90             |false            |LOW                     |business        |Mobilis Smart|26-35    |M     |Very High                       |EVENING             |Thursday   |false     |false     |Nouvel_An_2025       |25.0                |TikTok          |video           |false       |               |NONE        |false          |false               |\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+--------+---------------+--------+------------+----------+--------------+-----------+-------------+---------------+-----------------+------------------------+----------------+-------------+---------+------+--------------------------------+--------------------+-----------+----------+----------+---------------------+--------------------+----------------+----------------+------------+---------------+------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM cdr_raw\").show()\n",
    "spark.sql(\"SELECT * FROM cdr_raw LIMIT 5\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fb33086-f9f3-45ad-9fc1-e45a79b37a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/06/22 16:55:51 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ SparkSession initialized (App: Notebook 02: Hive Advanced Setup, Spark: 3.5.1)\n",
      "‚úÖ Hive Warehouse: hdfs://namenode:9000/user/hive/warehouse\n",
      "‚úÖ Hive Metastore URI: thrift://hive-metastore:9083\n",
      "‚úÖ SparkSession initialized (App: Notebook 02: Hive Advanced Setup, Spark: 3.5.1)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/work/work/scripts')\n",
    "from spark_init import init_spark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Spark with Hive support\n",
    "spark = init_spark(\"Notebook 02: Hive Advanced Setup\")\n",
    "print(f\"‚úÖ SparkSession initialized (App: {spark.sparkContext.appName}, Spark: {spark.version})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1deceb3-4b23-4b6d-ae19-af4656e3e693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 16:56:02 WARN HiveConf: HiveConf of name hive.metastore.event.db.notification.api.auth does not exist\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Using database: algerie_telecom_gen\n"
     ]
    }
   ],
   "source": [
    "# (Re)create database for a clean start (only if you want to overwrite old stuff)\n",
    "db_name = \"algerie_telecom_gen\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name} LOCATION '/user/hive/warehouse/{db_name}.db'\")\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "print(f\"‚úÖ Using database: {db_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c5c7e7b-32ae-4fa9-8cb5-6ebe6fd4a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìã Data Schema from Parquet:\n",
      "root\n",
      " |-- cdr_id: string (nullable = true)\n",
      " |-- subscriber_id: string (nullable = true)\n",
      " |-- msisdn: string (nullable = true)\n",
      " |-- imsi: string (nullable = true)\n",
      " |-- imei: string (nullable = true)\n",
      " |-- service_type: string (nullable = true)\n",
      " |-- service_subtype: string (nullable = true)\n",
      " |-- session_id: string (nullable = true)\n",
      " |-- calling_party: string (nullable = true)\n",
      " |-- called_party: string (nullable = true)\n",
      " |-- start_time: string (nullable = true)\n",
      " |-- end_time: string (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- data_volume_mb: double (nullable = true)\n",
      " |-- upload_mb: double (nullable = true)\n",
      " |-- download_mb: double (nullable = true)\n",
      " |-- cell_id: string (nullable = true)\n",
      " |-- lac: string (nullable = true)\n",
      " |-- location_area: string (nullable = true)\n",
      " |-- serving_cell_tower: string (nullable = true)\n",
      " |-- network_type: string (nullable = true)\n",
      " |-- charging_amount: double (nullable = true)\n",
      " |-- currency: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- tax_amount: double (nullable = true)\n",
      " |-- call_result: string (nullable = true)\n",
      " |-- quality_score: double (nullable = true)\n",
      " |-- signal_strength: long (nullable = true)\n",
      " |-- dropped_call_flag: boolean (nullable = true)\n",
      " |-- customer_segment: string (nullable = true)\n",
      " |-- tariff_plan: string (nullable = true)\n",
      " |-- operator: string (nullable = true)\n",
      " |-- age_group: string (nullable = true)\n",
      " |-- gender: string (nullable = true)\n",
      " |-- roaming_flag: boolean (nullable = true)\n",
      " |-- roaming_country: string (nullable = true)\n",
      " |-- roaming_type: string (nullable = true)\n",
      " |-- special_offer_applied: string (nullable = true)\n",
      " |-- promotional_discount: double (nullable = true)\n",
      " |-- network_congestion_level: string (nullable = true)\n",
      " |-- fraud_indicator: boolean (nullable = true)\n",
      " |-- unusual_pattern_flag: boolean (nullable = true)\n",
      " |-- time_of_day_category: string (nullable = true)\n",
      " |-- day_of_week: string (nullable = true)\n",
      " |-- is_weekend: boolean (nullable = true)\n",
      " |-- is_holiday: boolean (nullable = true)\n",
      " |-- application_used: string (nullable = true)\n",
      " |-- content_category: string (nullable = true)\n",
      " |-- revenue_per_mb: double (nullable = true)\n",
      " |-- customer_lifetime_value_category: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 16:56:18 WARN SparkStringUtils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total records in Parquet: 146,876,149\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+-------------+------------------+------------+---------------+--------+------------+----------+-----------+-------------+---------------+-----------------+----------------+-------------+--------+---------+------+------------+---------------+------------+---------------------+--------------------+------------------------+---------------+--------------------+--------------------+-----------+----------+----------+----------------+----------------+--------------+--------------------------------+\n",
      "|cdr_id                              |subscriber_id|msisdn          |imsi            |imei            |service_type|service_subtype |session_id    |calling_party   |called_party    |start_time         |end_time           |duration|data_volume_mb|upload_mb|download_mb|cell_id    |lac    |location_area|serving_cell_tower|network_type|charging_amount|currency|payment_type|tax_amount|call_result|quality_score|signal_strength|dropped_call_flag|customer_segment|tariff_plan  |operator|age_group|gender|roaming_flag|roaming_country|roaming_type|special_offer_applied|promotional_discount|network_congestion_level|fraud_indicator|unusual_pattern_flag|time_of_day_category|day_of_week|is_weekend|is_holiday|application_used|content_category|revenue_per_mb|customer_lifetime_value_category|\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+-------------+------------------+------------+---------------+--------+------------+----------+-----------+-------------+---------------+-----------------+----------------+-------------+--------+---------+------+------------+---------------+------------+---------------------+--------------------+------------------------+---------------+--------------------+--------------------+-----------+----------+----------+----------------+----------------+--------------+--------------------------------+\n",
      "|1ed3fa8c-02fa-4ac4-8522-044ea493e948|AT00019248   |89d7b44045d23836|02ba595c4881d2fe|912205004a7a302f|VOICE       |OUTGOING_CALL   |VOICE_8e71ccd8|89d7b44045d23836|e2c78d4fcc154bfd|2025-01-04 09:49:17|2025-01-04 09:50:32|75      |0.0           |0.0      |0.0        |60302954013|LAC5401|Khenchela    |Tower_954013      |4G          |5.95           |DZD     |PREPAID     |0.95      |SUCCESS    |0.95         |80             |false            |youth           |Mobilis Smart|Djezzy  |26-35    |F     |false       |               |NONE        |Weekend_Family       |20.0                |LOW                     |false          |false               |MORNING             |Saturday   |true      |false     |                |                |0.0           |High                            |\n",
      "|c1325ba8-a8c1-40b9-a3e7-2b0f3048ae5a|AT00019678   |907aa3f59e83148e|f736555d07010481|d73fadb95eaaf7a1|DATA        |INTERNET_SESSION|DATA_48b59838 |907aa3f59e83148e|                |2025-01-04 08:50:13|2025-01-04 13:05:49|15336   |128.12        |12.81    |115.31     |60302268065|LAC6806|Chlef        |Tower_268065      |4G          |304.93         |DZD     |PREPAID     |48.69     |SUCCESS    |0.95         |71             |false            |business        |Jeune Connect|Djezzy  |18-25    |F     |false       |               |NONE        |Weekend_Family       |20.0                |LOW                     |false          |false               |MORNING             |Saturday   |true      |false     |Spotify         |music           |2.38          |Very High                       |\n",
      "|2c8d0480-35f1-4ac1-b5ce-b0cdda731746|AT00094835   |ad763ea4e289697f|930135f977112d90|c3cd9637b7e6d26b|DATA        |INTERNET_SESSION|DATA_266f7f67 |ad763ea4e289697f|                |2025-01-03 01:34:48|2025-01-03 06:38:48|18240   |287.98        |28.8     |259.18     |60301887051|LAC8705|Tiaret       |Tower_887051      |4G          |642.56         |DZD     |POSTPAID    |102.59    |SUCCESS    |0.95         |63             |false            |premium         |Jeune Connect|Mobilis |50+      |M     |false       |               |NONE        |Nouvel_An_2025       |25.0                |LOW                     |false          |false               |NIGHT               |Friday     |false     |false     |TikTok          |video           |2.23          |Very High                       |\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+-------------+------------------+------------+---------------+--------+------------+----------+-----------+-------------+---------------+-----------------+----------------+-------------+--------+---------+------+------------+---------------+------------+---------------------+--------------------+------------------------+---------------+--------------------+--------------------+-----------+----------+----------+----------------+----------------+--------------+--------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_parquet_path = \"/user/hive/warehouse/generated_raw_cdr/*.parquet\"\n",
    "df = spark.read.parquet(raw_parquet_path)\n",
    "print(\"üìã Data Schema from Parquet:\")\n",
    "df.printSchema()\n",
    "print(f\"Total records in Parquet: {df.count():,}\")\n",
    "df.show(3, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55ae0fd1-dac2-4435-a433-846274bf8583",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 16:56:24 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ External table 'cdr_raw' created with correct schema.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS cdr_raw\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE EXTERNAL TABLE IF NOT EXISTS cdr_raw (\n",
    "        cdr_id STRING,\n",
    "        subscriber_id STRING,\n",
    "        msisdn STRING,\n",
    "        imsi STRING,\n",
    "        imei STRING,\n",
    "        service_type STRING,\n",
    "        service_subtype STRING,\n",
    "        session_id STRING,\n",
    "        calling_party STRING,\n",
    "        called_party STRING,\n",
    "        start_time STRING,\n",
    "        end_time STRING,\n",
    "        duration BIGINT,  -- Matches Parquet INT64\n",
    "        data_volume_mb DOUBLE,\n",
    "        upload_mb DOUBLE,\n",
    "        download_mb DOUBLE,\n",
    "        cell_id STRING,\n",
    "        lac STRING,\n",
    "        location_area STRING,\n",
    "        serving_cell_tower STRING,\n",
    "        network_type STRING,\n",
    "        operator STRING,\n",
    "        charging_amount DOUBLE,\n",
    "        currency STRING,\n",
    "        payment_type STRING,\n",
    "        tax_amount DOUBLE,\n",
    "        revenue_per_mb DOUBLE,\n",
    "        call_result STRING,\n",
    "        quality_score DOUBLE,\n",
    "        signal_strength BIGINT,  -- Matches Parquet INT64\n",
    "        dropped_call_flag BOOLEAN,\n",
    "        network_congestion_level STRING,\n",
    "        customer_segment STRING,\n",
    "        tariff_plan STRING,\n",
    "        age_group STRING,\n",
    "        gender STRING,\n",
    "        customer_lifetime_value_category STRING,\n",
    "        time_of_day_category STRING,\n",
    "        day_of_week STRING,\n",
    "        is_weekend BOOLEAN,\n",
    "        is_holiday BOOLEAN,\n",
    "        special_offer_applied STRING,\n",
    "        promotional_discount DOUBLE,\n",
    "        application_used STRING,\n",
    "        content_category STRING,\n",
    "        roaming_flag BOOLEAN,\n",
    "        roaming_country STRING,\n",
    "        roaming_type STRING,\n",
    "        fraud_indicator BOOLEAN,\n",
    "        unusual_pattern_flag BOOLEAN\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "    LOCATION '/user/hive/warehouse/generated_raw_cdr'\n",
    "\"\"\")\n",
    "print(\"‚úÖ External table 'cdr_raw' created with correct schema.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b83a488a-a974-4cf8-80e0-9379bd05b7f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cdr_raw Table Count:\n",
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|146876149|\n",
      "+---------+\n",
      "\n",
      "cdr_raw Table Preview:\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+--------+---------------+--------+------------+----------+--------------+-----------+-------------+---------------+-----------------+------------------------+----------------+-------------+---------+------+--------------------------------+--------------------+-----------+----------+----------+---------------------+--------------------+----------------+----------------+------------+---------------+------------+---------------+--------------------+\n",
      "|cdr_id                              |subscriber_id|msisdn          |imsi            |imei            |service_type|service_subtype |session_id    |calling_party   |called_party    |start_time         |end_time           |duration|data_volume_mb|upload_mb|download_mb|cell_id    |lac    |location_area |serving_cell_tower|network_type|operator|charging_amount|currency|payment_type|tax_amount|revenue_per_mb|call_result|quality_score|signal_strength|dropped_call_flag|network_congestion_level|customer_segment|tariff_plan  |age_group|gender|customer_lifetime_value_category|time_of_day_category|day_of_week|is_weekend|is_holiday|special_offer_applied|promotional_discount|application_used|content_category|roaming_flag|roaming_country|roaming_type|fraud_indicator|unusual_pattern_flag|\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+--------+---------------+--------+------------+----------+--------------+-----------+-------------+---------------+-----------------+------------------------+----------------+-------------+---------+------+--------------------------------+--------------------+-----------+----------+----------+---------------------+--------------------+----------------+----------------+------------+---------------+------------+---------------+--------------------+\n",
      "|1ed3fa8c-02fa-4ac4-8522-044ea493e948|AT00019248   |89d7b44045d23836|02ba595c4881d2fe|912205004a7a302f|VOICE       |OUTGOING_CALL   |VOICE_8e71ccd8|89d7b44045d23836|e2c78d4fcc154bfd|2025-01-04 09:49:17|2025-01-04 09:50:32|75      |0.0           |0.0      |0.0        |60302954013|LAC5401|Khenchela     |Tower_954013      |4G          |Djezzy  |5.95           |DZD     |PREPAID     |0.95      |0.0           |SUCCESS    |0.95         |80             |false            |LOW                     |youth           |Mobilis Smart|26-35    |F     |High                            |MORNING             |Saturday   |true      |false     |Weekend_Family       |20.0                |                |                |false       |               |NONE        |false          |false               |\n",
      "|c1325ba8-a8c1-40b9-a3e7-2b0f3048ae5a|AT00019678   |907aa3f59e83148e|f736555d07010481|d73fadb95eaaf7a1|DATA        |INTERNET_SESSION|DATA_48b59838 |907aa3f59e83148e|                |2025-01-04 08:50:13|2025-01-04 13:05:49|15336   |128.12        |12.81    |115.31     |60302268065|LAC6806|Chlef         |Tower_268065      |4G          |Djezzy  |304.93         |DZD     |PREPAID     |48.69     |2.38          |SUCCESS    |0.95         |71             |false            |LOW                     |business        |Jeune Connect|18-25    |F     |Very High                       |MORNING             |Saturday   |true      |false     |Weekend_Family       |20.0                |Spotify         |music           |false       |               |NONE        |false          |false               |\n",
      "|2c8d0480-35f1-4ac1-b5ce-b0cdda731746|AT00094835   |ad763ea4e289697f|930135f977112d90|c3cd9637b7e6d26b|DATA        |INTERNET_SESSION|DATA_266f7f67 |ad763ea4e289697f|                |2025-01-03 01:34:48|2025-01-03 06:38:48|18240   |287.98        |28.8     |259.18     |60301887051|LAC8705|Tiaret        |Tower_887051      |4G          |Mobilis |642.56         |DZD     |POSTPAID    |102.59    |2.23          |SUCCESS    |0.95         |63             |false            |LOW                     |premium         |Jeune Connect|50+      |M     |Very High                       |NIGHT               |Friday     |false     |false     |Nouvel_An_2025       |25.0                |TikTok          |video           |false       |               |NONE        |false          |false               |\n",
      "|f1c7c0ce-faa3-4920-b3b3-e3b8810214dc|AT00008349   |0029aca7fb1bc6e3|ad6efbabe9beb432|50c7f6f42dcb9b0e|DATA        |INTERNET_SESSION|DATA_2c6915b3 |0029aca7fb1bc6e3|                |2025-01-02 16:17:51|2025-01-03 01:02:51|31500   |160.44        |16.04    |144.4      |60301934022|LAC3402|Oum El Bouaghi|Tower_934022      |3G          |Mobilis |357.98         |DZD     |PREPAID     |57.16     |2.23          |SUCCESS    |0.95         |99             |false            |LOW                     |business        |Mobilis Smart|26-35    |M     |Medium                          |AFTERNOON           |Thursday   |false     |false     |Nouvel_An_2025       |25.0                |Spotify         |music           |false       |               |NONE        |false          |false               |\n",
      "|2333279d-5ef3-4bb0-8327-ab7b044949b4|AT00020140   |cceaa5199f3038a8|8d252bcea9f5dd6f|1e64848b154abb3f|DATA        |INTERNET_SESSION|DATA_ea6b8cf9 |cceaa5199f3038a8|                |2025-01-02 21:37:32|2025-01-03 02:25:02|17250   |414.45        |41.45    |373.0      |60301346015|LAC4601|Mostaganem    |Tower_346015      |3G          |Mobilis |924.74         |DZD     |PREPAID     |147.65    |2.23          |SUCCESS    |0.95         |90             |false            |LOW                     |business        |Mobilis Smart|26-35    |M     |Very High                       |EVENING             |Thursday   |false     |false     |Nouvel_An_2025       |25.0                |TikTok          |video           |false       |               |NONE        |false          |false               |\n",
      "+------------------------------------+-------------+----------------+----------------+----------------+------------+----------------+--------------+----------------+----------------+-------------------+-------------------+--------+--------------+---------+-----------+-----------+-------+--------------+------------------+------------+--------+---------------+--------+------------+----------+--------------+-----------+-------------+---------------+-----------------+------------------------+----------------+-------------+---------+------+--------------------------------+--------------------+-----------+----------+----------+---------------------+--------------------+----------------+----------------+------------+---------------+------------+---------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"cdr_raw Table Count:\")\n",
    "spark.sql(\"SELECT COUNT(*) FROM cdr_raw\").show()\n",
    "\n",
    "print(\"cdr_raw Table Preview:\")\n",
    "spark.sql(\"SELECT * FROM cdr_raw LIMIT 5\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d487886e-a16c-4fba-a01d-7483cd61dfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Partitioned table 'cdr_partitioned' created.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"DROP TABLE IF EXISTS cdr_partitioned\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS cdr_partitioned (\n",
    "        cdr_id STRING,\n",
    "        subscriber_id STRING,\n",
    "        msisdn STRING,\n",
    "        imsi STRING,\n",
    "        imei STRING,\n",
    "        service_type STRING,\n",
    "        service_subtype STRING,\n",
    "        session_id STRING,\n",
    "        calling_party STRING,\n",
    "        called_party STRING,\n",
    "        start_time TIMESTAMP,\n",
    "        end_time TIMESTAMP,\n",
    "        duration BIGINT,\n",
    "        data_volume_mb DOUBLE,\n",
    "        upload_mb DOUBLE,\n",
    "        download_mb DOUBLE,\n",
    "        cell_id STRING,\n",
    "        lac STRING,\n",
    "        location_area STRING,\n",
    "        serving_cell_tower STRING,\n",
    "        network_type STRING,\n",
    "        operator STRING,\n",
    "        charging_amount DOUBLE,\n",
    "        currency STRING,\n",
    "        payment_type STRING,\n",
    "        tax_amount DOUBLE,\n",
    "        revenue_per_mb DOUBLE,\n",
    "        call_result STRING,\n",
    "        quality_score DOUBLE,\n",
    "        signal_strength BIGINT,\n",
    "        dropped_call_flag BOOLEAN,\n",
    "        network_congestion_level STRING,\n",
    "        customer_segment STRING,\n",
    "        tariff_plan STRING,\n",
    "        age_group STRING,\n",
    "        gender STRING,\n",
    "        customer_lifetime_value_category STRING,\n",
    "        time_of_day_category STRING,\n",
    "        day_of_week STRING,\n",
    "        is_weekend BOOLEAN,\n",
    "        is_holiday BOOLEAN,\n",
    "        special_offer_applied STRING,\n",
    "        promotional_discount DOUBLE,\n",
    "        application_used STRING,\n",
    "        content_category STRING,\n",
    "        roaming_flag BOOLEAN,\n",
    "        roaming_country STRING,\n",
    "        roaming_type STRING,\n",
    "        fraud_indicator BOOLEAN,\n",
    "        unusual_pattern_flag BOOLEAN\n",
    "    )\n",
    "    PARTITIONED BY (year INT, month INT, day INT)\n",
    "    STORED AS PARQUET\n",
    "    TBLPROPERTIES (\n",
    "        'compression' = 'snappy',\n",
    "        'transactional' = 'false'\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"‚úÖ Partitioned table 'cdr_partitioned' created.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6a258dcd-d32c-4c26-aa91-5c4b7d5756a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 16:57:30 WARN SetCommand: 'SET hive.exec.dynamic.partition=true' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.dynamic.partition) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 16:57:30 WARN SetCommand: 'SET hive.exec.dynamic.partition.mode=nonstrict' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.dynamic.partition.mode) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 16:57:30 WARN SetCommand: 'SET hive.exec.max.dynamic.partitions=10000' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.max.dynamic.partitions) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n",
      "25/06/22 16:57:30 WARN SetCommand: 'SET hive.exec.max.dynamic.partitions.pernode=1000' might not work, since Spark doesn't support changing the Hive config dynamically. Please pass the Hive-specific config by adding the prefix spark.hadoop (e.g. spark.hadoop.hive.exec.max.dynamic.partitions.pernode) when starting a Spark application. For details, see the link: https://spark.apache.org/docs/latest/configuration.html#dynamically-loading-spark-properties.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üóÇÔ∏è Loading data into cdr_partitioned...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data inserted into cdr_partitioned.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SET hive.exec.dynamic.partition = true\")\n",
    "spark.sql(\"SET hive.exec.dynamic.partition.mode = nonstrict\")\n",
    "spark.sql(\"SET hive.exec.max.dynamic.partitions = 10000\")\n",
    "spark.sql(\"SET hive.exec.max.dynamic.partitions.pernode = 1000\")\n",
    "\n",
    "print(\"üóÇÔ∏è Loading data into cdr_partitioned...\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT OVERWRITE TABLE cdr_partitioned PARTITION(year, month, day)\n",
    "    SELECT\n",
    "        cdr_id, subscriber_id, msisdn, imsi, imei,\n",
    "        service_type, service_subtype, session_id,\n",
    "        calling_party, called_party,\n",
    "        CAST(start_time AS TIMESTAMP) as start_time,\n",
    "        CAST(end_time AS TIMESTAMP) as end_time,\n",
    "        duration, data_volume_mb, upload_mb, download_mb,\n",
    "        cell_id, lac, location_area, serving_cell_tower,\n",
    "        network_type, operator, charging_amount, currency, payment_type,\n",
    "        tax_amount, revenue_per_mb, call_result, quality_score,\n",
    "        signal_strength, dropped_call_flag, network_congestion_level,\n",
    "        customer_segment, tariff_plan, age_group, gender,\n",
    "        customer_lifetime_value_category, time_of_day_category,\n",
    "        day_of_week, is_weekend, is_holiday,\n",
    "        special_offer_applied, promotional_discount, application_used, content_category,\n",
    "        roaming_flag, roaming_country, roaming_type,\n",
    "        fraud_indicator, unusual_pattern_flag,\n",
    "        YEAR(CAST(start_time AS TIMESTAMP)) as year,\n",
    "        MONTH(CAST(start_time AS TIMESTAMP)) as month,\n",
    "        DAY(CAST(start_time AS TIMESTAMP)) as day\n",
    "    FROM cdr_raw\n",
    "\"\"\")\n",
    "print(\"‚úÖ Data inserted into cdr_partitioned.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "caf6f6e9-42a4-41e8-a2c9-ae36118844b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "| count(1)|\n",
      "+---------+\n",
      "|146876149|\n",
      "+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:================================================>      (97 + 8) / 109]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+----+--------+\n",
      "|year|month|days| records|\n",
      "+----+-----+----+--------+\n",
      "|2025|    1|  31|25157166|\n",
      "|2025|    2|  28|22717051|\n",
      "|2025|    3|  31|25160438|\n",
      "|2025|    4|  30|24347011|\n",
      "|2025|    5|  31|25156520|\n",
      "|2025|    6|  30|24337963|\n",
      "+----+-----+----+--------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT COUNT(*) FROM cdr_partitioned\").show()\n",
    "spark.sql(\"\"\"\n",
    "    SELECT year, month, COUNT(DISTINCT day) AS days, COUNT(*) AS records\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month\n",
    "    ORDER BY year, month\n",
    "\"\"\").show(24)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b0b0a77-0122-47ca-84ff-a771c008ff4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 9 analytical views.\n"
     ]
    }
   ],
   "source": [
    "views_created = 0\n",
    "\n",
    "# Service-based\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW voice_calls AS\n",
    "    SELECT * FROM cdr_partitioned WHERE service_type = 'VOICE'\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW data_sessions AS\n",
    "    SELECT * FROM cdr_partitioned WHERE service_type = 'DATA'\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW sms_records AS\n",
    "    SELECT * FROM cdr_partitioned WHERE service_type = 'SMS'\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "# Fraud/Anomalies\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW fraud_cases AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE fraud_indicator = true OR unusual_pattern_flag = true\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "# Network Issues\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW network_issues AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE dropped_call_flag = true OR call_result = 'FAILED'\n",
    "       OR network_congestion_level IN ('MEDIUM', 'HIGH') OR quality_score < 0.5\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "# High Value\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW high_value_customers AS\n",
    "    SELECT DISTINCT\n",
    "        subscriber_id, customer_segment, customer_lifetime_value_category,\n",
    "        tariff_plan, payment_type, age_group, gender, operator\n",
    "    FROM cdr_partitioned\n",
    "    WHERE customer_segment IN ('Premium')\n",
    "       OR customer_lifetime_value_category IN ('High', 'Very High')\n",
    "       OR payment_type = 'POSTPAID'\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "# Special offers\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW special_offers_usage AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE special_offer_applied != 'None' AND promotional_discount > 0\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "# Roaming\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW roaming_records AS\n",
    "    SELECT * FROM cdr_partitioned WHERE roaming_flag = true\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "# App usage\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE VIEW app_usage_data AS\n",
    "    SELECT * FROM cdr_partitioned\n",
    "    WHERE service_type = 'DATA' AND application_used IS NOT NULL AND application_used != ''\n",
    "\"\"\"); views_created += 1\n",
    "\n",
    "print(f\"‚úÖ Created {views_created} analytical views.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "06b6c977-207b-4d77-b870-bd053f90d886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created daily_kpis table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 17:02:47 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS daily_kpis AS\n",
    "    SELECT\n",
    "        year, month, day, service_type, operator, customer_segment,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_subscribers,\n",
    "        COUNT(*) as total_transactions,\n",
    "        SUM(duration) as total_duration_seconds,\n",
    "        SUM(data_volume_mb) as total_data_mb,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        SUM(tax_amount) as total_tax,\n",
    "        AVG(quality_score) as avg_quality_score,\n",
    "        SUM(CASE WHEN fraud_indicator THEN 1 ELSE 0 END) as fraud_cases,\n",
    "        SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_calls,\n",
    "        SUM(CASE WHEN special_offer_applied != 'None' THEN 1 ELSE 0 END) as special_offer_usage,\n",
    "        AVG(promotional_discount) as avg_discount_applied\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month, day, service_type, operator, customer_segment\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created daily_kpis table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8b93e204-6be5-48f1-971c-09fdd0e31b28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created hourly_patterns table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 17:02:50 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS hourly_patterns AS\n",
    "    SELECT\n",
    "        time_of_day_category, day_of_week, is_weekend, service_type, operator,\n",
    "        COUNT(*) as transaction_count,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_users,\n",
    "        AVG(duration) as avg_duration,\n",
    "        AVG(data_volume_mb) as avg_data_mb,\n",
    "        AVG(charging_amount) as avg_revenue,\n",
    "        AVG(quality_score) as avg_quality,\n",
    "        SUM(CASE WHEN network_congestion_level = 'HIGH' THEN 1 ELSE 0 END) as high_congestion_count\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY time_of_day_category, day_of_week, is_weekend, service_type, operator\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created hourly_patterns table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "57ca93e8-cd15-43ad-be2f-84043777f5da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created location_network_metrics table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 17:02:52 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS location_network_metrics AS\n",
    "    SELECT\n",
    "        location_area, network_type, operator, service_type,\n",
    "        COUNT(*) as total_transactions,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_subscribers,\n",
    "        AVG(quality_score) as avg_quality_score,\n",
    "        AVG(signal_strength) as avg_signal_strength,\n",
    "        SUM(CASE WHEN dropped_call_flag THEN 1 ELSE 0 END) as dropped_count,\n",
    "        SUM(CASE WHEN call_result = 'FAILED' THEN 1 ELSE 0 END) as failed_count,\n",
    "        SUM(CASE WHEN network_congestion_level = 'HIGH' THEN 1 ELSE 0 END) as high_congestion_count,\n",
    "        AVG(data_volume_mb) as avg_data_usage,\n",
    "        SUM(charging_amount) as total_revenue\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY location_area, network_type, operator, service_type\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created location_network_metrics table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4bc6c5da-5c14-4389-9771-159e16456227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created customer_demographics_summary table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 17:02:55 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS customer_demographics_summary AS\n",
    "    SELECT\n",
    "        customer_segment, age_group, gender, payment_type, operator,\n",
    "        COUNT(DISTINCT subscriber_id) as subscriber_count,\n",
    "        COUNT(*) as total_activities,\n",
    "        AVG(charging_amount) as avg_transaction_value,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        AVG(data_volume_mb) as avg_data_usage,\n",
    "        SUM(CASE WHEN fraud_indicator THEN 1 ELSE 0 END) as fraud_incidents,\n",
    "        AVG(promotional_discount) as avg_discount_received\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY customer_segment, age_group, gender, payment_type, operator\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created customer_demographics_summary table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f49df704-c92c-4f0d-b015-a5efbcfb6a50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created app_usage_analytics table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 17:04:57 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS app_usage_analytics AS\n",
    "    SELECT\n",
    "        application_used, content_category, customer_segment, age_group,\n",
    "        COUNT(DISTINCT subscriber_id) as unique_users,\n",
    "        COUNT(*) as total_sessions,\n",
    "        SUM(data_volume_mb) as total_data_mb,\n",
    "        AVG(data_volume_mb) as avg_data_per_session,\n",
    "        SUM(duration) as total_duration,\n",
    "        AVG(duration) as avg_session_duration,\n",
    "        SUM(charging_amount) as total_revenue,\n",
    "        AVG(revenue_per_mb) as avg_revenue_per_mb\n",
    "    FROM cdr_partitioned\n",
    "    WHERE service_type = 'DATA' AND application_used IS NOT NULL\n",
    "    GROUP BY application_used, content_category, customer_segment, age_group\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created app_usage_analytics table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bb497735-bfc6-4ee5-abd1-5769d6eff161",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created fraud_monitoring table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 17:05:00 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS fraud_monitoring AS\n",
    "    SELECT\n",
    "        DATE(start_time) as fraud_date,\n",
    "        operator, location_area,\n",
    "        COUNT(*) as total_incidents,\n",
    "        COUNT(DISTINCT subscriber_id) as affected_subscribers,\n",
    "        SUM(charging_amount) as potential_loss,\n",
    "        COLLECT_SET(service_type) as affected_services\n",
    "    FROM cdr_partitioned\n",
    "    WHERE fraud_indicator = true OR unusual_pattern_flag = true\n",
    "    GROUP BY DATE(start_time), operator, location_area\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created fraud_monitoring table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71e8aba2-5bc2-4968-bd47-f9a14a3e92b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created revenue_tracking table.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/06/22 17:05:02 WARN ResolveSessionCatalog: A Hive serde table will be created as there is no table provider specified. You can set spark.sql.legacy.createHiveTableByDefault to false so that native data source table will be created instead.\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS revenue_tracking AS\n",
    "    SELECT\n",
    "        year, month, day, operator, service_type, customer_segment, payment_type,\n",
    "        SUM(charging_amount) as gross_revenue,\n",
    "        SUM(tax_amount) as tax_collected,\n",
    "        SUM(charging_amount - tax_amount) as net_revenue,\n",
    "        COUNT(DISTINCT subscriber_id) as active_customers,\n",
    "        COUNT(*) as total_transactions,\n",
    "        AVG(charging_amount) as arpu_daily\n",
    "    FROM cdr_partitioned\n",
    "    GROUP BY year, month, day, operator, service_type, customer_segment, payment_type\n",
    "\"\"\")\n",
    "print(\"‚úÖ Created revenue_tracking table.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adc7aaba-3cf2-4790-b0c9-c32aaf343456",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Analyzed cdr_partitioned\n",
      "‚úÖ Analyzed daily_kpis\n",
      "‚úÖ Analyzed hourly_patterns\n",
      "‚úÖ Analyzed location_network_metrics\n",
      "‚úÖ Analyzed customer_demographics_summary\n",
      "‚úÖ Analyzed app_usage_analytics\n",
      "‚úÖ Analyzed fraud_monitoring\n",
      "‚úÖ Analyzed revenue_tracking\n"
     ]
    }
   ],
   "source": [
    "tables_to_analyze = [\n",
    "    'cdr_partitioned', 'daily_kpis', 'hourly_patterns',\n",
    "    'location_network_metrics', 'customer_demographics_summary',\n",
    "    'app_usage_analytics', 'fraud_monitoring', 'revenue_tracking'\n",
    "]\n",
    "for table in tables_to_analyze:\n",
    "    spark.sql(f\"ANALYZE TABLE {table} COMPUTE STATISTICS\")\n",
    "    if table in ['hourly_patterns', 'customer_demographics_summary']:\n",
    "        spark.sql(f\"ANALYZE TABLE {table} COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
    "    print(f\"‚úÖ Analyzed {table}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3ce78a99-1319-4308-936d-1b8132e10702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì¶ Tables in database:\n",
      "- daily_kpis\n",
      "- hourly_patterns\n",
      "- location_network_metrics\n",
      "- customer_demographics_summary\n",
      "- app_usage_analytics\n",
      "- fraud_monitoring\n",
      "- revenue_tracking\n",
      "- cdr_raw\n",
      "- cdr_partitioned\n",
      "- voice_calls\n",
      "- data_sessions\n",
      "- sms_records\n",
      "- fraud_cases\n",
      "- network_issues\n",
      "- high_value_customers\n",
      "- special_offers_usage\n",
      "- roaming_records\n",
      "- app_usage_data\n",
      "\n",
      "üëÅÔ∏è Views created: 9\n",
      "\n",
      "‚ö° Performance Test:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 43:================================>                       (11 + 8) / 19]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------------+-------+---------------+\n",
      "|operator|service_type|  count|        revenue|\n",
      "+--------+------------+-------+---------------+\n",
      "|  Djezzy|        DATA|4673700|5.88676314612E9|\n",
      "|  Djezzy|         SMS|2337228|  1.455933981E7|\n",
      "|  Djezzy|       VOICE|1868374| 1.4483556425E8|\n",
      "| Mobilis|        DATA|5922757|7.45828300811E9|\n",
      "| Mobilis|         SMS|2961333|  1.843686113E7|\n",
      "| Mobilis|       VOICE|2372992| 1.8381413374E8|\n",
      "| Ooredoo|        DATA|2642823|3.32654639053E9|\n",
      "| Ooredoo|         SMS|1320866|     8225269.28|\n",
      "| Ooredoo|       VOICE|1057093|  8.202056435E7|\n",
      "+--------+------------+-------+---------------+\n",
      "\n",
      "Test query executed in 1.19 seconds\n",
      "\n",
      "üéØ Next Steps:\n",
      "   ‚Üí Run Notebook 03 for Advanced Data Engineering\n",
      "   ‚Üí Run Notebook 04 for Anomaly Detection & Trend Analysis\n",
      "   ‚Üí Run Notebook 05 for Business Intelligence Dashboards\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "print(\"\\nüì¶ Tables in database:\")\n",
    "for row in spark.sql(f\"SHOW TABLES IN {db_name}\").collect():\n",
    "    print(f\"- {row.tableName}\")\n",
    "\n",
    "print(\"\\nüëÅÔ∏è Views created:\", views_created)\n",
    "\n",
    "print(\"\\n‚ö° Performance Test:\")\n",
    "start = time.time()\n",
    "spark.sql(\"\"\"\n",
    "    SELECT operator, service_type, COUNT(*) as count, ROUND(SUM(charging_amount),2) as revenue\n",
    "    FROM cdr_partitioned\n",
    "    WHERE year = 2025 AND month = 1\n",
    "    GROUP BY operator, service_type\n",
    "    ORDER BY operator, service_type\n",
    "\"\"\").show()\n",
    "print(f\"Test query executed in {time.time()-start:.2f} seconds\")\n",
    "\n",
    "print(\"\\nüéØ Next Steps:\")\n",
    "print(\"   ‚Üí Run Notebook 03 for Advanced Data Engineering\")\n",
    "print(\"   ‚Üí Run Notebook 04 for Anomaly Detection & Trend Analysis\")\n",
    "print(\"   ‚Üí Run Notebook 05 for Business Intelligence Dashboards\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218761d-100c-4d9a-85cb-81c457ad0a61",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
